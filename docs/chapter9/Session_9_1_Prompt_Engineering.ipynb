{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Prompt Engineering for Toxic Comment Classification\n",
    "\n",
    "In this notebook, we explore advanced **prompt engineering** techniques for **text classification** using **LiteLLM** (with OpenAI backends). We‚Äôll work on the **toxic comments** dataset, building prompt-based classifiers that **don‚Äôt require model fine-tuning**.\n",
    "\n",
    "## üìö Dataset\n",
    "\n",
    "- **[Toxic Comments Dataset](https://huggingface.co/datasets/AiresPucrs/toxic-comments)**  \n",
    "  This dataset consists of **user-generated comments** labeled as either **toxic** or **non-toxic**.  \n",
    "  In this session, we‚Äôll sample:\n",
    "  - **300 examples** for demonstration (150 positive, 150 negative),\n",
    "  - **100 examples** as a dev set to help us tune the prompts.\n",
    "\n",
    "## üéØ Prompt Engineering Techniques\n",
    "\n",
    "We‚Äôll experiment with several **prompt engineering** strategies:\n",
    "\n",
    "| Technique                          | Description |\n",
    "|------------------------------------|-------------|\n",
    "| **1Ô∏è‚É£ Prompt Only**               | Use a single, well-crafted prompt to classify the text. |\n",
    "| **2Ô∏è‚É£ Few-Shot Learning**         | Add a few labeled examples in the prompt to guide the model. |\n",
    "| **3Ô∏è‚É£ Chain-of-Thoughts**         | Encourage step-by-step reasoning in the prompt for better classification. |\n",
    "| **4Ô∏è‚É£ Automatic Prompt Engineering** | Iteratively refine prompts based on output failures, inspired by the latest research ([Zhou et al., 2022](https://arxiv.org/pdf/2211.01910)). |\n",
    "\n",
    "## üß™ Evaluation Metrics\n",
    "\n",
    "For each technique, we‚Äôll measure:\n",
    "- ‚úÖ **Accuracy** (dev set),\n",
    "- ‚úÖ Qualitative insights (example outputs),\n",
    "- ‚úÖ Improvements through **prompt iteration**.\n",
    "\n",
    "\n",
    "## üîç Learning Objectives\n",
    "\n",
    "1. Understand how to craft and evaluate **different prompt-based approaches**.\n",
    "2. Learn how **few-shot examples** and **chain-of-thought reasoning** can boost classification performance.\n",
    "3. Explore **automatic prompt optimization** to maximize accuracy without fine-tuning.\n",
    "\n",
    "\n",
    "Let‚Äôs get started by **loading and sampling** the dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 70157\n",
      "Columns: Index(['comment_text', 'toxic'], dtype='object')\n",
      "toxic\n",
      "0    35080\n",
      "1    35077\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Demo set label distribution:\n",
      " toxic\n",
      "0    150\n",
      "1    150\n",
      "Name: count, dtype: int64\n",
      "Dev set label distribution:\n",
      " toxic\n",
      "0    50\n",
      "1    50\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>hello please review discussion page talk shotg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>come sucka punch fucking face family son bitch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>course could try recreate scratch good sourcin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>another sockpuppet zay zay loose time editing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mean sprited dumb asses hope get guys name not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comment_text  toxic\n",
       "92   hello please review discussion page talk shotg...      1\n",
       "15   come sucka punch fucking face family son bitch...      0\n",
       "294  course could try recreate scratch good sourcin...      1\n",
       "153  another sockpuppet zay zay loose time editing ...      0\n",
       "17   mean sprited dumb asses hope get guys name not...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"AiresPucrs/toxic-comments\", split=\"train\")\n",
    "\n",
    "# Convert to pandas\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Show dataset size and column names\n",
    "print(\"Dataset size:\", len(df))\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Explore label distribution\n",
    "print(df[\"toxic\"].value_counts())\n",
    "\n",
    "# Sample 150 toxic (toxic=1) and 150 non-toxic (toxic=0) for main demo set\n",
    "df_toxic = df[df[\"toxic\"] == 1].sample(150, random_state=42)\n",
    "df_non_toxic = df[df[\"toxic\"] == 0].sample(150, random_state=42)\n",
    "\n",
    "df_demo = pd.concat([df_toxic, df_non_toxic]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Sample 100 examples as dev set (balanced)\n",
    "df_toxic_dev = df[df[\"toxic\"] == 1].drop(df_toxic.index).sample(50, random_state=42)\n",
    "df_non_toxic_dev = df[df[\"toxic\"] == 0].drop(df_non_toxic.index).sample(50, random_state=42)\n",
    "\n",
    "df_dev = pd.concat([df_toxic_dev, df_non_toxic_dev]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Show basic stats\n",
    "print(\"\\nDemo set label distribution:\\n\", df_demo[\"toxic\"].value_counts())\n",
    "print(\"Dev set label distribution:\\n\", df_dev[\"toxic\"].value_counts())\n",
    "\n",
    "# Quick sample preview\n",
    "df_demo.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Introduction to LiteLLM\n",
    "\n",
    "For this session, we‚Äôre using **LiteLLM**, a powerful **unified API** for calling multiple **LLM (Large Language Model) providers** ‚Äî including **OpenAI, Mistral AI, Anthropic, Cohere, and more**.\n",
    "\n",
    "### üöÄ Why LiteLLM?\n",
    "\n",
    "Traditionally, if you want to:\n",
    "\n",
    "‚úÖ Use different providers (OpenAI, Mistral AI, etc.),  \n",
    "‚úÖ Test and compare them,  \n",
    "‚úÖ Switch between **model versions** easily,  \n",
    "\n",
    "‚Ä¶you would need to write **different API calls** for each one ‚Äî which can be time-consuming and error-prone.\n",
    "\n",
    "### üéØ How LiteLLM Solves This\n",
    "\n",
    "With LiteLLM:\n",
    "\n",
    "- ü™Ñ **One unified interface** for calling **many providers**:  \n",
    "  ```python\n",
    "  from litellm import completion\n",
    "  response = completion(model=\"gpt-4o-mini\", messages=messages)\n",
    "  ```\n",
    "\n",
    "* üß© **Seamless switching**: You can change `\"gpt-4o-mini\"` to `\"claude-3-opus\"` or `\"azure-gpt-4\"` with no other code changes.\n",
    "* üèÉ **Built-in rate limiting, retries, and error handling**.\n",
    "* üî¨ **Transparent logging** and **tracing** for reproducibility.\n",
    "\n",
    "\n",
    "### üåç Why This Matters for Prompt Engineering\n",
    "\n",
    "Prompt engineering is **provider-agnostic** ‚Äî the techniques we‚Äôll explore (like few-shot learning or chain-of-thought) work **across LLMs**.\n",
    "\n",
    "Using LiteLLM lets us:\n",
    "\n",
    "- ‚úÖ Focus on **prompt crafting**, not on API differences.\n",
    "- ‚úÖ Quickly compare LLM behavior and performance.\n",
    "- ‚úÖ **Prototype faster** ‚Äî perfect for rapid iteration!\n",
    "\n",
    "### ü§ñ LLMClient ‚Äì Wrapping LLM-based Toxicity Classification\n",
    "\n",
    "We'll use the `LLMClient` class to **interface with LiteLLM** and evaluate different **prompt engineering strategies**. Let‚Äôs break down what this class does:\n",
    "\n",
    "#### üß© Key Components\n",
    "\n",
    "‚úÖ **Initialization (`__init__`)**  \n",
    "- Sets the **prompt template** to control the LLM‚Äôs instructions,  \n",
    "- Chooses the **model** (here, `gpt-4o-mini` by default) and **temperature** (for creativity vs. reliability).\n",
    "\n",
    "‚úÖ **`.predict(comments)`**  \n",
    "- For each comment in the input list:\n",
    "  - **Fills in the prompt** with the actual comment text.\n",
    "  - Calls the LLM (via LiteLLM) to **generate a classification**.\n",
    "  - **Parses the JSON output** to extract whether the comment is considered **toxic** (`1`) or **non-toxic** (`0`).\n",
    "\n",
    "‚úÖ **`.parse_answer(answer)`**  \n",
    "- Uses **regex** to find the JSON part of the LLM‚Äôs output (between ```json ‚Ä¶ ```),\n",
    "- This ensures we **only read the structured part** of the answer, even if the LLM adds extra commentary or explanations.\n",
    "\n",
    "‚úÖ **`.metric(y_true, y_pred)`**  \n",
    "- Uses `sklearn` to calculate **precision, recall, F1-score** (macro),  \n",
    "- Prints the results as a **clean table** for easy analysis.\n",
    "\n",
    "‚úÖ **`.analyze_error(y_true, y_pred)`**  \n",
    "- Builds a **confusion matrix** to see where the LLM is making errors,  \n",
    "- Lists **false positives** (non-toxic predicted as toxic) and **false negatives** (toxic predicted as non-toxic).\n",
    "\n",
    "#### üõë Why Constrain the Output to JSON?\n",
    "\n",
    "LLMs often produce **verbose natural language answers**, which makes it **hard to parse** and use programmatically.\n",
    "\n",
    "By **explicitly instructing the LLM** to output a valid JSON object (e.g., `{ \"toxic\": true }`), we ensure:\n",
    "\n",
    "- üîç **Easier parsing** (no ambiguity in answers),  \n",
    "- ‚úÖ **Consistency** across runs,  \n",
    "- ‚ö° **Automation-friendly**: critical when running many predictions!\n",
    "\n",
    "This practice is especially important when using LLMs for **automated pipelines** or **batch processing**.\n",
    "\n",
    "üí° Let‚Äôs now apply this class to different **prompt engineering techniques** to see how well each works for **toxic comment detection**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from jinja2 import Template\n",
    "import json\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, prompt_template, model_name=\"gpt-4o-mini\", temperature=0.2):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.prompt_template = Template(prompt_template)\n",
    "\n",
    "    def parse_answer(self, answer):\n",
    "        pattern = r\"```json(.*?)```\"\n",
    "        matches = re.findall(pattern, answer, re.DOTALL)\n",
    "        questions = json.loads(matches[0])\n",
    "        return questions\n",
    "\n",
    "    def predict(self, comments):\n",
    "        predictions = []\n",
    "        for comment in tqdm(comments):\n",
    "            # Build prompt\n",
    "            prompt = self.prompt_template.render(comment=comment)\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in detecting toxic content.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            # Call LiteLLM\n",
    "            response = completion(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "            answer = response[\"choices\"][0][\"message\"][\"content\"].lower().strip()\n",
    "            try:\n",
    "                answer = self.parse_answer(answer)\n",
    "                answer = answer.get('toxic')\n",
    "            except:\n",
    "                answer = False\n",
    "            \n",
    "            if answer:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        return predictions\n",
    "\n",
    "    def metric(self, y_true, y_pred):\n",
    "        report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        print(\"\\nüìä Classification Report:\\n\", df[[\"precision\", \"recall\", \"f1-score\"]])\n",
    "        return df\n",
    "\n",
    "    def analyze_error(self, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        df_cm = pd.DataFrame(cm, index=[\"Actual Non-toxic\", \"Actual Toxic\"], columns=[\"Pred Non-toxic\", \"Pred Toxic\"])\n",
    "        print(\"\\nüîç Confusion Matrix:\\n\", df_cm)\n",
    "\n",
    "        # False Positives / False Negatives\n",
    "        false_positives = np.where((y_true == 0) & (y_pred == 1))[0]\n",
    "        false_negatives = np.where((y_true == 1) & (y_pred == 0))[0]\n",
    "\n",
    "        print(f\"\\n‚ö†Ô∏è False Positives: {false_positives.shape[0]}\")\n",
    "        print(f\"‚ö†Ô∏è False Negatives: {false_negatives.shape[0]}\")\n",
    "\n",
    "        return false_positives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a toxicity classifier. \n",
    "Your task is to determine if the following comment is toxic or non-toxic. Return only a JSON object with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": True/False\n",
    "}\n",
    "```\n",
    "\n",
    "Comment:\n",
    "{{comment}}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "# Create instance\n",
    "simple_prompt = LLMClient(prompt_template=prompt_template, \n",
    "                          model_name=\"gpt-4o-mini\", \n",
    "                          temperature=0.2)\n",
    "\n",
    "\n",
    "texts = df_demo[\"comment_text\"].tolist()\n",
    "labels = df_demo[\"toxic\"].tolist()\n",
    "\n",
    "predictions = simple_prompt.predict(texts)\n",
    "\n",
    "# Metrics\n",
    "simple_prompt.metric(1 - np.array(labels), np.array(predictions))\n",
    "\n",
    "# Error analysis\n",
    "simple_prompt.analyze_error(1 - np.array(labels), np.array(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Prompt-Only Performance\n",
    "\n",
    "Our **Prompt-Only baseline** for toxic comment classification using LiteLLM achieves:\n",
    "\n",
    "- **Macro F1-score**: ~0.92  \n",
    "- **Balanced precision and recall** across both classes.\n",
    "\n",
    "‚úÖ The model performs well **overall**, but there are still:\n",
    "\n",
    "- **14 false positives** (non-toxic labeled as toxic),\n",
    "- **9 false negatives** (toxic labeled as non-toxic).\n",
    "\n",
    "### üéØ Next Step: Boosting with Few-Shot Examples\n",
    "\n",
    "To further improve classification, especially for **difficult or ambiguous cases**, we‚Äôll:\n",
    "\n",
    "- Use these **false positives and false negatives** from the dev set as **few-shot examples**,\n",
    "- Test if adding **1 or 2 examples per label** helps the model **clarify edge cases** and boost performance.\n",
    "\n",
    "Let‚Äôs dive in and see how few-shot learning can help us refine the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Confusion Matrix:\n",
      "                   Pred Non-toxic  Pred Toxic\n",
      "Actual Non-toxic              43           7\n",
      "Actual Toxic                   6          44\n",
      "\n",
      "‚ö†Ô∏è False Positives: 7\n",
      "‚ö†Ô∏è False Negatives: 6\n"
     ]
    }
   ],
   "source": [
    "dev_texts = df_dev[\"comment_text\"].tolist()\n",
    "dev_labels = df_dev[\"toxic\"].tolist()\n",
    "\n",
    "dev_predictions = simple_prompt.predict(dev_texts)\n",
    "\n",
    "false_positives, false_negatives = simple_prompt.analyze_error(1 - np.array(dev_labels), np.array(dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives:\n",
      "stop harassing complain user keegscee started attacking nowhere way around easy verify leave alone complain get blocked\n",
      "----------------------------------------------------------------------------------------------------\n",
      "nambiar looks obsessed associate certain terms professions ezhava community many ignorant people editing wikipedia articles doubt one among consider looking like dalit bad fair skin superior sick mind churn epithets want know looks need know first order tipu sultan gave nairs capturing malabar order orginal form seen mysore museum recently found one sitesyou read must proceed opposite manner dwell quietly pay dues like good subjects since practice one women associate ten men leave mothers sisters unconstrained obscene practices hence born adultery shameless connections heasts filed hereby require forsake sinful practices live like rest mankind disobedient commands made repeated vows honour whole islam march chief persons seat government make introspection tell good today tapping prostitution dalit looks european iranian looks suggest get basic knowledge history archeology anthropology etc think editing encyclopedia meant immature minds martial race british invention martial races intellectually inferior people would fight someone war get killed somebody one reasons pakistan invaded india times many martial tribes pakistan assumed defeat bigger army indian one pakistani equal hindus proven wrong noted nairs never challenged supremacy brahmins kashtriyas varma always obedient servants fulfilled every wish masters anil kumar\n",
      "----------------------------------------------------------------------------------------------------\n",
      "perhaps external link human_rights_and_equal_opportunity_commission page writings order demonstrate great example sort comment promotes racial religious intollerance strong feelings either way regarding external links\n",
      "----------------------------------------------------------------------------------------------------\n",
      "request member board trustees ask remove false misleading libellous information contained wikipedia remind continuing hold information libel illegal refer specifically webpage additionally talk pages user pages address blanked preserve integrity service failure would constitute smear campaign wikipedia whilst initiated persons longhair wikipedia hence sued result damages causes strongly recommend delete lies falsehoods\n",
      "----------------------------------------------------------------------------------------------------\n",
      "alpha rascals\n",
      "----------------------------------------------------------------------------------------------------\n",
      "wikipedia freedom speech reliable sources bunch jack asses wanna feel important imposing rules everyone wikipedia reliable gossip mag care guy says chinese democracy recorde source mistaken album recorded tossed slash left anyone likes band would know obviously controlfreak\n",
      "----------------------------------------------------------------------------------------------------\n",
      "know christopher columbus born november likes smell eat poop spanish people taught poop mini vans\n",
      "----------------------------------------------------------------------------------------------------\n",
      "False Negatives:\n",
      "nah far ass sun shine\n",
      "----------------------------------------------------------------------------------------------------\n",
      "really strange much edits alike looks like sockpuppets\n",
      "----------------------------------------------------------------------------------------------------\n",
      "thirteen days life old fart\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hypocrite liar merely thinking release sheltered wikipedia policy bound cocoon say floor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "balance section people care think monkees abomination like fred cocklskew frampington said gives like think would good yeahhh\n",
      "----------------------------------------------------------------------------------------------------\n",
      "blocked marking link rot got blocked daring mark articles link rot problems blocked tagging problem could rest link rot specialists possibly come defence thanks much\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"False Positives:\")\n",
    "for fp in false_positives:\n",
    "    print(dev_texts[fp])\n",
    "    print(\"-\"*100)\n",
    "\n",
    "print(\"False Negatives:\")\n",
    "for fn in false_negatives:\n",
    "    print(dev_texts[fn])\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Few-Shot Learning with Edge Case Examples\n",
    "\n",
    "To further refine our LLM-based toxicity classification, we‚Äôll use **few-shot learning**:\n",
    "\n",
    "‚úÖ We‚Äôll provide **1 and 2 real examples** from the dev set (using the **false positives and false negatives** identified earlier).  \n",
    "‚úÖ Each example includes the **comment** and its **true label** (toxic or non-toxic).  \n",
    "‚úÖ We‚Äôll test if providing these examples helps the LLM generalize better to edge cases and **reduce errors**.\n",
    "\n",
    "\n",
    "### üß© Our Few-Shot Prompt Format\n",
    "\n",
    "Here‚Äôs the **template** for the few-shot prompts:\n",
    "\n",
    "```text\n",
    "You are an expert classifier for toxic content.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Comment: \"{comment_1}\"\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": true\n",
    "}\n",
    "```\n",
    "Comment: \"{comment_2}\"\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": false\n",
    "}\n",
    "```\n",
    "...\n",
    "\n",
    "Comment: \"{new_comment}\"\n",
    "\n",
    "Answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [04:55<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a toxicity classifier. \n",
    "Your task is to determine if the following comment is toxic or non-toxic. Return only a JSON object with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": True/False\n",
    "}\n",
    "```\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Comment: thirteen days life old fart\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": true\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: stop harassing complain user keegscee started attacking nowhere way around easy verify leave alone complain get blocked\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": false\n",
    "}\n",
    "```\n",
    "\n",
    "Comment:\n",
    "{{comment}}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "texts = df_demo[\"comment_text\"].tolist()\n",
    "labels = df_demo[\"toxic\"].tolist()\n",
    "\n",
    "fsl_1_prompt = LLMClient(prompt_template=prompt_template, \n",
    "                          model_name=\"gpt-4o-mini\", \n",
    "                          temperature=0.2)\n",
    "\n",
    "fsl_1_predictions = fsl_1_prompt.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [04:58<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a toxicity classifier. \n",
    "Your task is to determine if the following comment is toxic or non-toxic. Return only a JSON object with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": True/False\n",
    "}\n",
    "```\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Comment: thirteen days life old fart\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": true\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: stop harassing complain user keegscee started attacking nowhere way around easy verify leave alone complain get blocked\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": false\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: perhaps external link human_rights_and_equal_opportunity_commission page writings order demonstrate great example sort comment promotes racial religious intollerance strong feelings either way regarding external links\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": false\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: hypocrite liar merely thinking release sheltered wikipedia policy bound cocoon say floor\n",
    "Answer: \n",
    "```json\n",
    "{\n",
    "    \"toxic\": true\n",
    "}\n",
    "```\n",
    "\n",
    "Comment:\n",
    "{{comment}}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "# Create instance\n",
    "fsl_2_prompt = LLMClient(prompt_template=prompt_template, \n",
    "                          model_name=\"gpt-4o-mini\", \n",
    "                          temperature=0.2)\n",
    "\n",
    "fsl_2_predictions = fsl_2_prompt.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score\n",
      "0              0.946309  0.940000  0.943144\n",
      "1              0.940397  0.946667  0.943522\n",
      "accuracy       0.943333  0.943333  0.943333\n",
      "macro avg      0.943353  0.943333  0.943333\n",
      "weighted avg   0.943353  0.943333  0.943333\n",
      "\n",
      "üìä Classification Report:\n",
      "               precision  recall  f1-score\n",
      "0              0.959184    0.94  0.949495\n",
      "1              0.941176    0.96  0.950495\n",
      "accuracy       0.950000    0.95  0.950000\n",
      "macro avg      0.950180    0.95  0.949995\n",
      "weighted avg   0.950180    0.95  0.949995\n",
      "\n",
      "üîç Confusion Matrix:\n",
      "                   Pred Non-toxic  Pred Toxic\n",
      "Actual Non-toxic             141           9\n",
      "Actual Toxic                   8         142\n",
      "\n",
      "‚ö†Ô∏è False Positives: 9\n",
      "‚ö†Ô∏è False Negatives: 8\n",
      "\n",
      "üîç Confusion Matrix:\n",
      "                   Pred Non-toxic  Pred Toxic\n",
      "Actual Non-toxic             141           9\n",
      "Actual Toxic                   6         144\n",
      "\n",
      "‚ö†Ô∏è False Positives: 9\n",
      "‚ö†Ô∏è False Negatives: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 45,  84,  86,  87,  88, 120, 170, 198, 278]),\n",
       " array([ 12,  63, 144, 153, 193, 288]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "fsl_1_prompt.metric(1 - np.array(labels), np.array(fsl_1_predictions))\n",
    "fsl_2_prompt.metric(1 - np.array(labels), np.array(fsl_2_predictions))\n",
    "\n",
    "# Error analysis\n",
    "fsl_1_prompt.analyze_error(1 - np.array(labels), np.array(fsl_1_predictions))\n",
    "fsl_2_prompt.analyze_error(1 - np.array(labels), np.array(fsl_2_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Impact of Adding Few-Shot Examples\n",
    "\n",
    "By incorporating **false positive and false negative examples** as few-shot demonstrations in the prompt, we observe clear **improvements in the model‚Äôs performance**:\n",
    "\n",
    "#### üìà Summary of Results\n",
    "\n",
    "| Few-Shot Examples  | Macro F1   | False Positives | False Negatives |\n",
    "|---------------------|------------|-----------------|-----------------|\n",
    "| **Prompt-Only** | 0.9200     | 14               | 9               |\n",
    "| **1 pair (2 examples)** | 0.9433     | 9               | 8               |\n",
    "| **2 pairs (4 examples)** | 0.9500     | 9               | 6               |\n",
    "\n",
    "#### üü¢ Key Observations\n",
    "\n",
    "‚úÖ **F1-score improved** from ~0.92 (prompt-only) ‚Üí **0.9433** (1 pair) ‚Üí **0.95** (2 pairs).  \n",
    "‚úÖ **False negatives decreased** from 9 ‚Üí 8 ‚Üí 6, indicating better coverage of toxic examples.  \n",
    "‚úÖ False positives remained stable, showing **more robust classification of non-toxic comments**.\n",
    "\n",
    "#### üí° Takeaway\n",
    "\n",
    "Adding **a few well-chosen examples** helps the model:\n",
    "\n",
    "- **Resolve ambiguities** in edge cases,\n",
    "- Better understand **nuances of toxic vs. non-toxic language**,\n",
    "- And achieve **better macro-average performance** ‚Äî critical for balanced datasets!\n",
    "\n",
    "We would need to know when to stop adding more examples. Now let's try to add chain-of-thought reasoning to the mix and see if it helps.\n",
    "\n",
    "--- \n",
    "\n",
    "## üß† Chain-of-Thought (CoT) Reasoning for Toxicity Classification\n",
    "\n",
    "### üí° What is Chain-of-Thought (CoT)?\n",
    "\n",
    "Chain-of-Thought prompting is a powerful strategy that **guides the LLM to explicitly reason** through intermediate steps, rather than jumping straight to an answer.\n",
    "\n",
    "Instead of giving the model a single classification instruction, we prompt it to **explain its reasoning process** step by step. This can:\n",
    "\n",
    "‚úÖ Help the model **unpack subtle cues** in language (like sarcasm, implied aggression),  \n",
    "‚úÖ Reduce overconfidence in borderline cases,  \n",
    "‚úÖ Improve robustness and consistency.\n",
    "\n",
    "### üî¨ Applying CoT to Toxic Comment Classification\n",
    "\n",
    "In the **CoT** version, we‚Äôll:\n",
    "\n",
    "- Provide a short **reasoning explanation** for each example in the few-shot prompt,  \n",
    "- Instruct the LLM to also **explain its decision** when analyzing new comments,  \n",
    "- Still **end the output with a JSON object** for easy parsing!\n",
    "\n",
    "This helps the LLM **learn and apply a structured thought process** to classify nuanced comments.\n",
    "\n",
    "Let‚Äôs set it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:30<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score\n",
      "0              0.979452  0.953333  0.966216\n",
      "1              0.954545  0.980000  0.967105\n",
      "accuracy       0.966667  0.966667  0.966667\n",
      "macro avg      0.966999  0.966667  0.966661\n",
      "weighted avg   0.966999  0.966667  0.966661\n",
      "\n",
      "üîç Confusion Matrix:\n",
      "                   Pred Non-toxic  Pred Toxic\n",
      "Actual Non-toxic             143           7\n",
      "Actual Toxic                   3         147\n",
      "\n",
      "‚ö†Ô∏è False Positives: 7\n",
      "‚ö†Ô∏è False Negatives: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 45,  84,  86, 120, 170, 195, 257]), array([ 49,  63, 144]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_cot = \"\"\"\n",
    "You are a toxicity classifier. \n",
    "Your task is to determine if the following comment is toxic or non-toxic. \n",
    "First, provide a short reasoning about the comment's language and tone.\n",
    "Finally, return only a JSON object with the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": true/false\n",
    "}\n",
    "````\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Comment: thirteen days life old fart\n",
    "Reasoning: This comment uses derogatory language, indicating disrespectful and toxic behavior.\n",
    "Answer:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": true\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: stop harassing complain user keegscee started attacking nowhere way around easy verify leave alone complain get blocked\n",
    "Reasoning: This comment is a complaint about user interactions, but it does not contain toxic or aggressive language.\n",
    "Answer:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": false\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: perhaps external link human_rights_and_equal_opportunity_commission page writings order demonstrate great example sort comment promotes racial religious intollerance strong feelings either way regarding external links\n",
    "Reasoning: This comment references external links but does not show direct toxicity.\n",
    "Answer:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": false\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: hypocrite liar merely thinking release sheltered wikipedia policy bound cocoon say floor\n",
    "Reasoning: This comment calls someone a hypocrite and liar, using negative and toxic language.\n",
    "Answer:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"toxic\": true\n",
    "}\n",
    "```\n",
    "\n",
    "Comment: {{comment}}\n",
    "Reasoning:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "client_cot = LLMClient(prompt_template=prompt_template_cot, model_name=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "# Evaluate on dev set\n",
    "preds_cot = client_cot.predict(texts)\n",
    "\n",
    "# Show metrics\n",
    "client_cot.metric(1 - np.array(labels), np.array(preds_cot))\n",
    "\n",
    "# Analyze errors\n",
    "client_cot.analyze_error(1 - np.array(labels), np.array(preds_cot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Chain-of-Thought (CoT)** prompt, combining **few-shot examples + explicit reasoning**, has further boosted the model‚Äôs performance!\n",
    "\n",
    "### üìà Performance Summary\n",
    "\n",
    "| Technique                   | Macro F1   | False Positives | False Negatives |\n",
    "|-----------------------------|------------|-----------------|-----------------|\n",
    "| **Prompt-Only**            | ~0.92      | 14              | 9               |\n",
    "| **Few-Shot (1 pair)**      | ~0.94      | 9               | 8               |\n",
    "| **Few-Shot (2 pairs)**     | ~0.95      | 9               | 6               |\n",
    "| **Chain-of-Thought (2 pairs)** | **0.967** | **7**           | **3**           |\n",
    "\n",
    "\n",
    "### üîç Key Observations\n",
    "\n",
    "‚úÖ **Macro F1-score improved** significantly ‚Äî from ~0.92 (prompt-only) ‚Üí 0.967 (CoT),  \n",
    "‚úÖ **False negatives** reduced dramatically: from 9 ‚Üí 3,  \n",
    "‚úÖ **False positives** also decreased: from 14 ‚Üí 7.\n",
    "\n",
    "This shows that the **explicit reasoning in CoT** helps the model better:\n",
    "\n",
    "- **Understand nuances** of borderline or ambiguous comments,\n",
    "- Distinguish subtle forms of toxicity,\n",
    "- Achieve a more **balanced classification** across both classes.\n",
    "\n",
    "### üü° Trade-off: Inference Time\n",
    "\n",
    "While CoT offers **strong performance gains**, it does come with a trade-off:\n",
    "\n",
    "‚ö†Ô∏è **Longer Inference Time**  \n",
    "- CoT requires the model to **generate a step-by-step reasoning process** before outputting the JSON answer.  \n",
    "- In real-world applications, this may impact **latency** for high-volume deployments.\n",
    "\n",
    "### üìå Next Steps & Robustness Check\n",
    "\n",
    "To **validate** these gains and ensure they‚Äôre **not just from random LLM variability**:\n",
    "\n",
    "- üß™ **Compare FP/FN differences** in detail to ensure they‚Äôre truly ‚Äúhard cases‚Äù now correctly handled.\n",
    "- üîÅ **Repeat experiments** multiple times to measure **standard deviation** and confirm these improvements are **robust**.\n",
    "\n",
    "This would give us more confidence in **deploying** this approach for real-world toxic content moderation!\n",
    "\n",
    "--- \n",
    "\n",
    "## ü§ñ Beyond Manual Crafting: Automatic Prompt Engineering (APE)\n",
    "\n",
    "So far, we‚Äôve manually:\n",
    "\n",
    "‚úÖ Crafted **prompt-only** and **few-shot** examples,  \n",
    "‚úÖ Added **chain-of-thought reasoning** for more nuanced predictions,  \n",
    "‚úÖ Observed **clear improvements** in both **macro F1** and error reduction.\n",
    "\n",
    "### üü° The Challenge\n",
    "\n",
    "üß† **Manual prompt engineering** can be **time-consuming** and **subjective** ‚Äî it depends on human intuition and small-scale error analysis.\n",
    "\n",
    "üí° But what if we could **automate this process**?\n",
    "\n",
    "### üåç Enter APE ‚Äì Automatic Prompt Engineering\n",
    "\n",
    "The **APE framework** (Zhou et al., 2022) proposes:\n",
    "\n",
    "1Ô∏è‚É£ Starting with an **initial prompt**,  \n",
    "2Ô∏è‚É£ Generating candidate prompts through **variations and mutations**,  \n",
    "3Ô∏è‚É£ Using **LLM self-evaluation** (or performance on a dev set) to identify the **best-performing prompt**,  \n",
    "4Ô∏è‚É£ Iteratively refining the prompt to **optimize performance**.\n",
    "\n",
    "### üî¨ Benefits of APE\n",
    "\n",
    "‚úÖ Discover **better prompts** than what humans can manually guess,  \n",
    "‚úÖ Continuously adapt prompts to new data, edge cases, or deployment needs,  \n",
    "‚úÖ Combine **robustness testing** (standard deviation across runs) with prompt improvement.\n",
    "\n",
    "### üìä The Bigger Picture\n",
    "\n",
    "To truly validate our best prompt:\n",
    "\n",
    "- We should **compare it to other LLM-based classifiers** (like Claude, Mistral, or open-source models).\n",
    "- And even **benchmark** against **fine-tuned transformer models** trained specifically on toxic comments.\n",
    "\n",
    "This gives us the **full picture**:\n",
    "\n",
    "| Approach                      | What it‚Äôs good at                        |\n",
    "|-------------------------------|------------------------------------------|\n",
    "| üü© Prompt Engineering (LiteLLM) | Quick deployment, no training, flexible |\n",
    "| üü¶ Fine-tuned models           | Fast inference, consistent performance   |\n",
    "| üüß APE                         | Best of both worlds, **adaptive prompts** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse-nlp-DetGwK6_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
