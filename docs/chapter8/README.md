## 🎓 Course Materials - Practical NLP - 2

## Session 8: Fine-Tuning BERT, Few-Shot Learning, and Bias in NLP

In this hands-on session, we explore cutting-edge approaches in **advanced NLP**, including **fine-tuning BERT**, leveraging **few-shot learning with SetFit**, and investigating **biases in NLP models** (like gender biases using WinoGrad schemas).

This notebook is designed as a modular, reusable blueprint for state-of-the-art NLP techniques.

### 📓 Notebooks

* [Session 8: BERT, Few-Shot Learning, and Bias in NLP](Session_8.ipynb)

---

### 🎯 Learning Objectives

1. **Fine-tune BERT** for text classification with a small dataset.
2. Understand and implement **few-shot learning** using the SetFit framework.
3. Evaluate models using **standard metrics** (accuracy, precision, recall, F1-score, confusion matrix, ROC/AUC).
4. Analyze and **identify biases** in BERT models using WinoGrad schemas.
5. Discuss **model fairness** and interpretability in modern NLP.

---

### 📖 Bibliography & Recommended Reading

* **Hugging Face Transformers Documentation** – [Link](https://huggingface.co/docs/transformers/index)
* **SetFit: Efficient Few-Shot Classification** – [Link](https://github.com/huggingface/setfit)
* **WinoGrad Schema Challenge** – [Link](https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html)
* **Fairness in Machine Learning** – [Link](https://fairmlbook.org/)

---

### 💻 Practical Components

* 🏗️ **Fine-tune BERT** on AG News corpus using Hugging Face Transformers.
* 🔄 **Train a few-shot classifier** with **SetFit** using just 32 examples.
* 🧪 **Experiment with data augmentation** via prompt-based methods.
* 🕵️‍♂️ **Evaluate model fairness** and **gender bias** in predictions.
* 🎯 **Compare models** using quantitative metrics (ROC/AUC, F1, etc.) and **qualitative outputs** (example-level analysis).
