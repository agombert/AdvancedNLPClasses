## ğŸ“ Course Materials - Practical NLP - 2

## Session 8: Fine-Tuning BERT, Few-Shot Learning, and Bias in NLP

In this hands-on session, we explore cutting-edge approaches in **advanced NLP**, including **fine-tuning BERT**, leveraging **few-shot learning with SetFit**, and investigating **biases in NLP models** (like gender biases using WinoGrad schemas).

This notebook is designed as a modular, reusable blueprint for state-of-the-art NLP techniques.

### ğŸ““ Notebooks

* [Session 8: BERT, Few-Shot Learning, and Bias in NLP](Session_8.ipynb)

---

### ğŸ¯ Learning Objectives

1. **Fine-tune BERT** for text classification with a small dataset.
2. Understand and implement **few-shot learning** using the SetFit framework.
3. Evaluate models using **standard metrics** (accuracy, precision, recall, F1-score, confusion matrix, ROC/AUC).
4. Analyze and **identify biases** in BERT models using WinoGrad schemas.
5. Discuss **model fairness** and interpretability in modern NLP.

---

### ğŸ“– Bibliography & Recommended Reading

* **Hugging Face Transformers Documentation** â€“ [Link](https://huggingface.co/docs/transformers/index)
* **SetFit: Efficient Few-Shot Classification** â€“ [Link](https://github.com/huggingface/setfit)
* **WinoGrad Schema Challenge** â€“ [Link](https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html)
* **Fairness in Machine Learning** â€“ [Link](https://fairmlbook.org/)

---

### ğŸ’» Practical Components

* ğŸ—ï¸ **Fine-tune BERT** on AG News corpus using Hugging Face Transformers.
* ğŸ”„ **Train a few-shot classifier** with **SetFit** using just 32 examples.
* ğŸ§ª **Experiment with data augmentation** via prompt-based methods.
* ğŸ•µï¸â€â™‚ï¸ **Evaluate model fairness** and **gender bias** in predictions.
* ğŸ¯ **Compare models** using quantitative metrics (ROC/AUC, F1, etc.) and **qualitative outputs** (example-level analysis).
