{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Types\n",
    "\n",
    "This notebook covers the fundamental and advanced aspects of Python's type system. Understanding types is crucial for writing robust and maintainable code, especially in data science and NLP applications.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Basic Types](#basic)\n",
    "2. [Advanced Types](#advanced)\n",
    "3. [Exercises](#exercises)\n",
    "4. [Real-World Applications](#applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Types <a name=\"basic\"></a>\n",
    "\n",
    "Python is a dynamically typed language, which means you don't need to declare the type of a variable when you create it. The interpreter infers the type based on the value assigned.\n",
    "\n",
    "### 1.1 Primitive Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age is of type: <class 'int'>\n",
      "height is of type: <class 'float'>\n",
      "is_student is of type: <class 'bool'>\n",
      "name is of type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Integers\n",
    "age = 25\n",
    "print(f\"age is of type: {type(age)}\")\n",
    "\n",
    "# Floating-point numbers\n",
    "height = 1.75\n",
    "print(f\"height is of type: {type(height)}\")\n",
    "\n",
    "# Booleans\n",
    "is_student = True\n",
    "print(f\"is_student is of type: {type(is_student)}\")\n",
    "\n",
    "# Strings\n",
    "name = \"Alice\"\n",
    "print(f\"name is of type: {type(name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Complex Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruits is of type: <class 'list'>\n",
      "coordinates is of type: <class 'tuple'>\n",
      "person is of type: <class 'dict'>\n",
      "unique_numbers is of type: <class 'set'>\n",
      "unique_numbers contains: {1, 2, 3, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "# Lists - ordered, mutable collections\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "print(f\"fruits is of type: {type(fruits)}\")\n",
    "\n",
    "# Tuples - ordered, immutable collections\n",
    "coordinates = (10.5, 20.8)\n",
    "print(f\"coordinates is of type: {type(coordinates)}\")\n",
    "\n",
    "# Dictionaries - key-value pairs\n",
    "person = {\"name\": \"Bob\", \"age\": 30, \"is_student\": False}\n",
    "print(f\"person is of type: {type(person)}\")\n",
    "\n",
    "# Sets - unordered collections of unique elements\n",
    "unique_numbers = {1, 2, 3, 3, 4, 5}  # Note: duplicates are automatically removed\n",
    "print(f\"unique_numbers is of type: {type(unique_numbers)}\")\n",
    "print(f\"unique_numbers contains: {unique_numbers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Type Conversion\n",
    "\n",
    "Python allows you to convert between different types using built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted '25' to 25 of type <class 'int'>\n",
      "Converted 42 to '42' of type <class 'str'>\n",
      "Converted '19.99' to 19.99 of type <class 'float'>\n",
      "Converted [1, 2, 2, 3, 4, 4, 5] to {1, 2, 3, 4, 5} of type <class 'set'>\n"
     ]
    }
   ],
   "source": [
    "# String to integer\n",
    "age_str = \"25\"\n",
    "age_int = int(age_str)\n",
    "print(f\"Converted '{age_str}' to {age_int} of type {type(age_int)}\")\n",
    "\n",
    "# Integer to string\n",
    "num = 42\n",
    "num_str = str(num)\n",
    "print(f\"Converted {num} to '{num_str}' of type {type(num_str)}\")\n",
    "\n",
    "# String to float\n",
    "price_str = \"19.99\"\n",
    "price_float = float(price_str)\n",
    "print(f\"Converted '{price_str}' to {price_float} of type {type(price_float)}\")\n",
    "\n",
    "# List to set (removes duplicates)\n",
    "numbers_list = [1, 2, 2, 3, 4, 4, 5]\n",
    "numbers_set = set(numbers_list)\n",
    "print(f\"Converted {numbers_list} to {numbers_set} of type {type(numbers_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Checking Types\n",
    "\n",
    "You can check the type of a variable using the `type()` function or the `isinstance()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of value is: <class 'int'>\n",
      "Is value an int? True\n",
      "Is value an int? True\n",
      "Is value a float? False\n",
      "Is value a number? True\n"
     ]
    }
   ],
   "source": [
    "value = 42\n",
    "\n",
    "# Using type()\n",
    "print(f\"Type of value is: {type(value)}\")\n",
    "print(f\"Is value an int? {type(value) is int}\")\n",
    "\n",
    "# Using isinstance()\n",
    "print(f\"Is value an int? {isinstance(value, int)}\")\n",
    "print(f\"Is value a float? {isinstance(value, float)}\")\n",
    "\n",
    "# Check if value is a number (int or float)\n",
    "print(f\"Is value a number? {isinstance(value, (int, float))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Types <a name=\"advanced\"></a>\n",
    "\n",
    "### 2.1 Type Hints (Static Typing)\n",
    "\n",
    "Python 3.5+ supports type hints, which allow you to specify the expected types of variables, function parameters, and return values. This helps with code documentation and can be used by tools like mypy for static type checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample tex...\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Set, Optional, Union, Any\n",
    "\n",
    "# Function with type hints\n",
    "def process_text(text: str, max_length: int = 100) -> str:\n",
    "    \"\"\"Process a text string and return a modified version.\"\"\"\n",
    "    if len(text) > max_length:\n",
    "        return text[:max_length] + \"...\"\n",
    "    return text\n",
    "\n",
    "# Using the function\n",
    "result = process_text(\"This is a sample text for processing.\", 20)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apple': 3, 'banana': 2, 'cherry': 1}\n"
     ]
    }
   ],
   "source": [
    "# More complex type hints\n",
    "\n",
    "# A function that takes a list of strings and returns a dictionary\n",
    "def count_word_frequencies(words: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"Count the frequency of each word in a list.\"\"\"\n",
    "    frequencies: Dict[str, int] = {}\n",
    "    for word in words:\n",
    "        if word in frequencies:\n",
    "            frequencies[word] += 1\n",
    "        else:\n",
    "            frequencies[word] = 1\n",
    "    return frequencies\n",
    "\n",
    "# Using the function\n",
    "words = [\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\", \"apple\"]\n",
    "word_counts = count_word_frequencies(words)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Custom Types with Classes\n",
    "\n",
    "You can create custom types using classes. This is a fundamental concept in object-oriented programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextDocument(title='Introduction to NLP', words=22, tags=['NLP', 'AI', 'introduction'])\n",
      "Word count: 22\n",
      "Type of doc: <class '__main__.TextDocument'>\n"
     ]
    }
   ],
   "source": [
    "class TextDocument:\n",
    "    def __init__(self, title: str, content: str, tags: List[str] = None):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.tags = tags or []\n",
    "        \n",
    "    def word_count(self) -> int:\n",
    "        \"\"\"Count the number of words in the document.\"\"\"\n",
    "        return len(self.content.split())\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"TextDocument(title='{self.title}', words={self.word_count()}, tags={self.tags})\"\n",
    "\n",
    "# Create a document\n",
    "doc = TextDocument(\n",
    "    \"Introduction to NLP\",\n",
    "    \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\",\n",
    "    [\"NLP\", \"AI\", \"introduction\"]\n",
    ")\n",
    "\n",
    "print(doc)\n",
    "print(f\"Word count: {doc.word_count()}\")\n",
    "print(f\"Type of doc: {type(doc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Advanced Type Concepts for NLP\n",
    "\n",
    "In NLP, you'll often work with complex data structures. Here are some advanced type concepts that are particularly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(text='apple', pos_tag='NOUN', is_stop=False)\n",
      "Token text: apple, POS tag: NOUN\n",
      "{'title': 'Advanced NLP Techniques', 'author': 'Jane Smith', 'year': 2023, 'keywords': ['NLP', 'machine learning', 'transformers']}\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, TypedDict\n",
    "\n",
    "# NamedTuple - an immutable, typed version of a tuple with named fields\n",
    "class Token(NamedTuple):\n",
    "    text: str\n",
    "    pos_tag: str\n",
    "    is_stop: bool\n",
    "\n",
    "# Create a token\n",
    "token = Token(text=\"apple\", pos_tag=\"NOUN\", is_stop=False)\n",
    "print(token)\n",
    "print(f\"Token text: {token.text}, POS tag: {token.pos_tag}\")\n",
    "\n",
    "# TypedDict - a dictionary with a fixed set of keys, each with a specified type\n",
    "class DocumentMetadata(TypedDict):\n",
    "    title: str\n",
    "    author: str\n",
    "    year: int\n",
    "    keywords: List[str]\n",
    "\n",
    "# Create document metadata\n",
    "metadata: DocumentMetadata = {\n",
    "    \"title\": \"Advanced NLP Techniques\",\n",
    "    \"author\": \"Jane Smith\",\n",
    "    \"year\": 2023,\n",
    "    \"keywords\": [\"NLP\", \"machine learning\", \"transformers\"]\n",
    "}\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, World! This is a TEST.\n",
      "Processed: hello world this is a test\n"
     ]
    }
   ],
   "source": [
    "# Function types with Callable\n",
    "from typing import Callable\n",
    "\n",
    "# Define a type for text processing functions\n",
    "TextProcessor = Callable[[str], str]\n",
    "\n",
    "def apply_processors(text: str, processors: List[TextProcessor]) -> str:\n",
    "    \"\"\"Apply a series of text processors to a string.\"\"\"\n",
    "    result = text\n",
    "    for processor in processors:\n",
    "        result = processor(result)\n",
    "    return result\n",
    "\n",
    "# Define some text processors\n",
    "def lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    import string\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Apply processors\n",
    "text = \"Hello, World! This is a TEST.\"\n",
    "processed_text = apply_processors(text, [lowercase, remove_punctuation])\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Processed: {processed_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Union Types and Optional\n",
    "\n",
    "Union types allow a variable to have one of several types. Optional is a shorthand for Union[T, None]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "hello world\n",
      "['John', 'New York', 'Google']\n",
      "['John']\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "# A function that can take either a string or a list of strings\n",
    "def normalize_text(text: Union[str, List[str]]) -> str:\n",
    "    if isinstance(text, list):\n",
    "        return \" \".join(text).lower()\n",
    "    return text.lower()\n",
    "\n",
    "# Examples\n",
    "print(normalize_text(\"Hello World\"))\n",
    "print(normalize_text([\"Hello\", \"World\"]))\n",
    "\n",
    "# Optional parameter (can be None)\n",
    "def extract_entities(text: str, entity_types: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Extract entities of specified types from text.\"\"\"\n",
    "    # In a real implementation, this would use an NLP library\n",
    "    # For demonstration, we'll just return some dummy data\n",
    "    if entity_types is None:\n",
    "        return [\"John\", \"New York\", \"Google\"]\n",
    "    elif \"PERSON\" in entity_types:\n",
    "        return [\"John\"]\n",
    "    elif \"LOCATION\" in entity_types:\n",
    "        return [\"New York\"]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Examples\n",
    "print(extract_entities(\"John works at Google in New York.\"))\n",
    "print(extract_entities(\"John works at Google in New York.\", [\"PERSON\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercises <a name=\"exercises\"></a>\n",
    "\n",
    "Now it's your turn to practice working with Python types. Complete the following exercises to reinforce your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Type Conversion\n",
    "\n",
    "Write a function `parse_numeric_data` that takes a list of strings, converts each string to a number (float or int as appropriate), and returns a list of numbers. If a string cannot be converted, it should be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_numeric_data(string_list: List[str]) -> List[Union[int, float]]:\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test cases\n",
    "test_data = [\"42\", \"3.14\", \"not a number\", \"99\", \"0.5\"]\n",
    "# Expected output: [42, 3.14, 99, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Working with Dictionaries\n",
    "\n",
    "Create a function `word_statistics` that takes a text string and returns a dictionary with the following statistics:\n",
    "- 'word_count': the number of words in the text\n",
    "- 'char_count': the number of characters (excluding spaces)\n",
    "- 'avg_word_length': the average length of words\n",
    "- 'unique_words': the number of unique words (case-insensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_statistics(text: str) -> Dict[str, Union[int, float]]:\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test case\n",
    "sample_text = \"Natural Language Processing is fascinating. NLP combines linguistics and computer science.\"\n",
    "# Expected output: a dictionary with word_count, char_count, avg_word_length, and unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Types for NLP\n",
    "\n",
    "Create a `Sentence` class that represents a sentence in an NLP context. It should:\n",
    "- Store the original text\n",
    "- Have a method to tokenize the sentence into words\n",
    "- Have a method to count the frequency of each word\n",
    "- Have a method to identify potential named entities (words that start with a capital letter, excluding the first word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, text: str):\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def tokenize(self) -> List[str]:\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def word_frequencies(self) -> Dict[str, int]:\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def potential_named_entities(self) -> List[str]:\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "# Test case\n",
    "test_sentence = Sentence(\"John visited New York last summer with his friend Mary.\")\n",
    "# Expected: methods should return appropriate results for this sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Type Hints in Practice\n",
    "\n",
    "Implement a function `process_documents` that takes a list of documents (each represented as a dictionary with 'id', 'title', and 'content' keys) and a processing function. The function should apply the processing function to each document's content and return a new list of documents with the processed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Callable, Any\n",
    "\n",
    "Document = Dict[str, Any]  # Type alias for a document\n",
    "TextProcessor = Callable[[str], str]  # Type alias for a text processing function\n",
    "\n",
    "def process_documents(documents: List[Document], processor: TextProcessor) -> List[Document]:\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test case\n",
    "docs = [\n",
    "    {\"id\": 1, \"title\": \"Introduction\", \"content\": \"This is an introduction to NLP.\"},\n",
    "    {\"id\": 2, \"title\": \"Methods\", \"content\": \"We use various methods in NLP.\"}\n",
    "]\n",
    "\n",
    "def uppercase_processor(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "# Expected: a new list of documents with uppercase content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-World Applications <a name=\"applications\"></a>\n",
    "\n",
    "Here are some examples of how Python's type system is used in real-world NLP projects:\n",
    "\n",
    "### spaCy\n",
    "\n",
    "spaCy is a popular NLP library that makes extensive use of Python's type system. It uses type hints throughout its codebase to ensure type safety and provide better documentation. The library defines custom types for tokens, documents, and other NLP concepts.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# doc is of type Doc\n",
    "# Each token in doc is of type Token\n",
    "# Spans of tokens are of type Span\n",
    "```\n",
    "\n",
    "### Hugging Face Transformers\n",
    "\n",
    "The Hugging Face Transformers library uses type hints to provide clear interfaces for its models and tokenizers. This helps users understand what types of inputs and outputs to expect when working with complex transformer models.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# inputs is a dictionary of torch.Tensor objects\n",
    "# outputs contains logits of type torch.Tensor\n",
    "```\n",
    "\n",
    "### NLTK\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is one of the oldest and most comprehensive NLP libraries. While it predates Python's type hints, it uses Python's object-oriented features to define clear types for linguistic concepts.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Natural language processing is a field of computer science and linguistics.\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# tokens is a list of strings\n",
    "# tagged is a list of tuples (word, tag)\n",
    "```\n",
    "\n",
    "### FastAPI\n",
    "\n",
    "FastAPI is a modern web framework that leverages Python's type hints to automatically generate API documentation and perform request validation. It's often used to deploy NLP models as web services.\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class SentimentResponse(BaseModel):\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "\n",
    "@app.post(\"/sentiment\", response_model=SentimentResponse)\n",
    "def analyze_sentiment(request: TextRequest):\n",
    "    # In a real application, this would use an NLP model\n",
    "    return {\"sentiment\": \"positive\", \"confidence\": 0.95}\n",
    "```\n",
    "\n",
    "These examples demonstrate how Python's type system is used in real-world NLP applications to create more robust, maintainable, and well-documented code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse-nlp-DetGwK6_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
