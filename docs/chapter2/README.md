# Deep Learning for NLP

## Session 2: 2015 Deep Learning

This session introduces the fundamental concepts of deep learning as applied to Natural Language Processing, focusing on the breakthroughs that emerged around 2015.

### Learning Objectives

- Understand the core principles of neural networks and backpropagation
- Learn about recurrent neural networks and their application to sequential data
- Explore attention mechanisms and their role in improving language models
- Gain practical knowledge of implementing deep learning models for NLP tasks

### Topics Covered

#### Backpropagation in Neural Networks

- Fundamentals of neural network architecture
- The mathematics behind backpropagation
- Gradient descent and optimization techniques
- Common activation functions and their properties
- Challenges in training deep neural networks

#### LSTM, Attention Processes & Language Models

- Recurrent Neural Networks (RNNs) and their limitations
- Long Short-Term Memory (LSTM) networks
- Gated Recurrent Units (GRUs)
- Attention mechanisms: allowing models to focus on relevant parts of input
- Early language models based on recurrent architectures
- Sequence-to-sequence models for machine translation

### Recommended Reading

- Karpathy, Andrej (2016) "Yes you should understand Backprop"
- Karpathy, Andrej (2015) "The Unreasonable Effectiveness of Recurrent Neural Networks"
- Olah, Christopher (2015) "Understanding LSTM Networks"
- Olah, Christopher (2016) "Attention and Augmented Recurrent Neural Networks"

### Practical Components

- Implementing a simple neural network with backpropagation from scratch
- Building and training an LSTM model for text generation
- Experimenting with attention mechanisms in sequence tasks
- Evaluating model performance on standard NLP benchmarks
