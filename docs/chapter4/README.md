## ğŸ“ Course Materials - Practical NLP - 1

## Session 4: Text Classification Pipelines and Explainability

In this hands-on session, we walk through the evolution of **text classification pipelines**, from traditional approaches like **TF-IDF + linear classifiers** to modern **deep learning models with LSTM** and **pretrained word embeddings** like **Word2Vec**. The session closes with an introduction to **model explainability** using **LIME**, giving students insight into how models make decisions.

This notebook is designed as a modular blueprint that can be reused and extended for many text classification tasks.

### ğŸ““ Notebooks

- [Session 4: From TF-IDF to LSTMs and Explainability](Session_4.ipynb)

---

### ğŸ¯ Learning Objectives

1. Understand how to turn raw text into machine-readable input (TF-IDF, tokenization, embeddings).
2. Build baseline and deep models (logistic regression, BiLSTM).
3. Integrate **pre-trained embeddings** (Word2Vec) into custom pipelines.
4. Apply **explainability tools** like LIME to interpret model behavior.
5. Compare models using quantitative and qualitative evaluation (metrics & examples).

---

### ğŸ“– Bibliography & Recommended Reading

- **scikit-learn TF-IDF Documentation** â€“ [Link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
- **spaCy Tokenizer Docs** â€“ [Link](https://spacy.io/api/tokenizer)
- **Gensim Word2Vec Tutorial** â€“ [Link](https://radimrehurek.com/gensim/models/word2vec.html)
- **LIME GitHub Repo** â€“ [Link](https://github.com/marcotcr/lime)

---

### ğŸ’» Practical Components

- ğŸ§ª Build a **text classifier from scratch** using `TF-IDF + LogisticRegression`.
- ğŸ§  Train an **LSTM** with one-hot or pre-trained embeddings.
- ğŸ“¦ Use **Word2Vec** embeddings from Hugging Face.
- ğŸ” Explain and debug predictions with **LIME** for real-world NLP workflows.
- ğŸ¯ Compare models using both **metrics** and **example-level outputs**.
