{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù LLM as a Judge for Summarization: Evaluating CNN/DailyMail Highlights\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we‚Äôll explore how **Large Language Models (LLMs)** like **GPT-4o** can act as **judges** to assess the **quality of summaries**.  \n",
    "We‚Äôll use the **CNN/DailyMail dataset**, which pairs news articles with human-written highlights.\n",
    "\n",
    "### üìö What We‚Äôll Do\n",
    "\n",
    "- üîç Evaluate highlights for:\n",
    "  - **Faithfulness** (are they true to the original article?)\n",
    "  - **Coverage** (do they capture the main points?)\n",
    "  - **Clarity & conciseness** (how well they read)\n",
    "- ‚úèÔ∏è Use the **LLM itself** to suggest improvements to highlights.\n",
    "- ‚öñÔ∏è Use a **second LLM-as-Judge** to compare the **original** vs. **improved** highlights.\n",
    "- üé≤ Test for **order bias** (do LLM preferences change based on which highlight is shown first?).\n",
    "\n",
    "### üõ†Ô∏è Tools & Dataset\n",
    "\n",
    "We‚Äôll use:\n",
    "- **GPT-4o** (for both evaluation and improvement suggestions),\n",
    "- **CNN/DailyMail dataset** (via Hugging Face),\n",
    "- A small sample of **news articles + highlights**.\n",
    "\n",
    "The **CNN/DailyMail dataset** is ideal because:\n",
    "‚úÖ It‚Äôs large and widely used in summarization tasks.  \n",
    "‚úÖ It includes **human-written highlights** ‚Äî perfect for benchmarking.  \n",
    "‚úÖ News articles provide **rich, real-world context** for LLM judgment.\n",
    "\n",
    "### üéØ Use Case: Summarization Evaluation & Improvement\n",
    "\n",
    "This mirrors real-world tasks in:\n",
    "- **Content moderation** (detect low-quality or inaccurate summaries),\n",
    "- **Media verification** (ensure fair and accurate reporting),\n",
    "- **Newsroom editorial workflows** (automate summary improvement).\n",
    "\n",
    "### üß™ Key Objectives\n",
    "\n",
    "1Ô∏è‚É£ See how LLMs can **rate summary quality** and spot errors.  \n",
    "2Ô∏è‚É£ Use LLMs to **rewrite** or **improve** highlights.  \n",
    "3Ô∏è‚É£ Assess if LLMs **agree** on which version is best ‚Äî and if order bias affects that.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Step 1: Load the CNN/DailyMail Dataset\n",
    "\n",
    "We‚Äôll start by loading a **sample** of the dataset using the Hugging Face `datasets` library.  \n",
    "This gives us real-world news articles and their **human-crafted highlights** ‚Äî our ground truth for testing.\n",
    "\n",
    "Let‚Äôs load the data and inspect a few examples!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 100\n",
      "\n",
      "üîç Example sample:\n",
      "Article:\n",
      "(CNN) -- On the London Underground there is a saying which has become synonymous with any tourist visit to the British capital. \"Mind the Gap\". It's up there with the Queen, a wonderfully indulgent cream tea, Beefeaters outside the Tower of London and all those quintessentially English pastimes which tend to draw those flocking from overseas. It is a saying which has been emblazoned across numerous t-shirts and posters, while children on the underground seem to delight in repeating the famous phrase. But now, in London at least, those three words have been hijacked by the supporters of Tottenham and Arsenal, who will now no doubt continue to mock each other until the end of the season. Following Tottenham's 2-1 win over its city rival Sunday, the \"gap\" is seven points with Andre Villas-Boas' side now occupying third place and more importantly, a qualification spot for next year's Champions League. But Arsenal has been here before. Last season, Tottenham blew a 10-point advantage to all...\n",
      "\n",
      "Highlight:\n",
      "Tottenham wins derby match against Arsenal 2-1 Sunday .\n",
      "Gareth Bale has now scored 20 goals for Spurs so far this season .\n",
      "Inter Milan fights back to win 3-2 at Catania in Serie A .\n",
      "Bayern Munich wins again to stay 17 points clear at summit .\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
    "dataset = dataset.shuffle(seed=42).select(np.arange(100))\n",
    "print(f\"Number of examples: {len(dataset)}\")\n",
    "\n",
    "# Show a sample\n",
    "sample_idx = 0\n",
    "print(\"\\nüîç Example sample:\")\n",
    "print(f\"Article:\\n{dataset[sample_idx]['article'][:1000]}...\\n\")\n",
    "print(f\"Highlight:\\n{dataset[sample_idx]['highlights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö LLMs as Judges\n",
    "\n",
    "The approach we‚Äôll explore in this notebook is **inspired by the paper ‚ÄúLLMs as a Judge: A Benchmark and Comparison of Human and LLM Evaluation‚Äù**  \n",
    "([Zheng et al., 2023](https://arxiv.org/pdf/2306.05685)).\n",
    "\n",
    "### üéØ The Key Idea\n",
    "\n",
    "In traditional NLP pipelines, **human evaluation** is considered the gold standard for assessing generated text ‚Äî e.g., checking if summaries are:\n",
    "- Factually correct\n",
    "- Coherent\n",
    "- Grammatically sound\n",
    "- Non-redundant\n",
    "\n",
    "But‚Ä¶ üõë **Human evaluation is expensive and slow.**\n",
    "\n",
    "The paper proposes that **LLMs themselves** can **act as ‚Äújudges‚Äù** to evaluate and score the quality of outputs.  \n",
    "They do so by comparing two (or more) candidate answers side-by-side and determining which is better along dimensions like:\n",
    "- Factuality\n",
    "- Conciseness\n",
    "- Relevance\n",
    "- Overall quality\n",
    "\n",
    "The LLM‚Äôs decision can be used as:\n",
    "‚úÖ A **proxy** for human evaluation  \n",
    "‚úÖ A scalable way to **improve datasets, prompts, or models**  \n",
    "\n",
    "Let‚Äôs start by defining the first **LLM judge evaluation prompt** and see how it scores the existing highlights!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert news editor with 10+ years of experience evaluating content quality. Your task is to assess how well a highlight summarizes the main points of an article.\n",
      "\n",
      "## Evaluation Criteria:\n",
      "\n",
      "**Factual Accuracy (1-5):**\n",
      "- 5: All facts are correct and verifiable from the article\n",
      "- 4: Mostly accurate with minor factual issues\n",
      "- 3: Generally accurate but some noticeable errors\n",
      "- 2: Several factual errors or misrepresentations\n",
      "- 1: Major factual errors or completely inaccurate\n",
      "\n",
      "**Conciseness (1-5):**\n",
      "- 5: Perfectly concise - captures key points without unnecessary words\n",
      "- 4: Very concise with minimal redundancy\n",
      "- 3: Reasonably concise but could be tighter\n",
      "- 2: Somewhat verbose with unnecessary details\n",
      "- 1: Very verbose or poorly structured\n",
      "\n",
      "**Relevance (1-5):**\n",
      "- 5: Covers all main points and ignores minor details\n",
      "- 4: Covers most main points effectively\n",
      "- 3: Covers some main points but misses important ones\n",
      "- 2: Focuses on minor details while missing main points\n",
      "- 1: Completely irrelevant or misses the article's focus\n",
      "\n",
      "## Article:\n",
      "(CNN) -- On the London Underground there is a saying which has become synonymous with any tourist visit to the British capital. \"Mind the Gap\". It's up there with the Queen, a wonderfully indulgent cream tea, Beefeaters outside the Tower of London and all those quintessentially English pastimes which tend to draw those flocking from overseas. It is a saying which has been emblazoned across numerous t-shirts and posters, while children on the underground seem to delight in repeating the famous ph\n",
      "\n",
      "## Highlight to Evaluate:\n",
      "Tottenham wins derby match against Arsenal 2-1 Sunday .\n",
      "Gareth Bale has now scored 20 goals for Spurs so far this season .\n",
      "Inter Milan fights back to win 3-2 at Catania in Serie A .\n",
      "Bayern Munich wins again to stay 17 points clear at summit .\n",
      "\n",
      "## Instructions:\n",
      "Evaluate the highlight against the article using the criteria above. Provide scores and explanations for each dimension, then calculate an overall quality score.\n",
      "\n",
      "**IMPORTANT: Respond ONLY with valid JSON in this exact format:**\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"factual_accuracy\": {\n",
      "    \"score\": [1-5],\n",
      "    \"explanation\": \"[2-3 sentence explanation]\"\n",
      "  },\n",
      "  \"conciseness\": {\n",
      "    \"score\": [1-5], \n",
      "    \"explanation\": \"[2-3 sentence explanation]\"\n",
      "  },\n",
      "  \"relevance\": {\n",
      "    \"score\": [1-5],\n",
      "    \"explanation\": \"[2-3 sentence explanation]\"\n",
      "  },\n",
      "  \"overall_quality\": {\n",
      "    \"score\": [1-5],\n",
      "    \"explanation\": \"[2-3 sentence summary explaining the overall score]\"\n",
      "  },\n",
      "  \"strengths\": [\"strength 1\", \"strength 2\"],\n",
      "  \"weaknesses\": [\"weakness 1\", \"weakness 2\"],\n",
      "  \"recommendation\": \"[improve/acceptable/excellent]\"\n",
      "}\n",
      "```\n",
      "\n",
      "Do not include any text outside the JSON response.\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "You are an expert news editor with 10+ years of experience evaluating content quality. Your task is to assess how well a highlight summarizes the main points of an article.\n",
    "\n",
    "## Evaluation Criteria:\n",
    "\n",
    "**Factual Accuracy (1-5):**\n",
    "- 5: All facts are correct and verifiable from the article\n",
    "- 4: Mostly accurate with minor factual issues\n",
    "- 3: Generally accurate but some noticeable errors\n",
    "- 2: Several factual errors or misrepresentations\n",
    "- 1: Major factual errors or completely inaccurate\n",
    "\n",
    "**Conciseness (1-5):**\n",
    "- 5: Perfectly concise - captures key points without unnecessary words\n",
    "- 4: Very concise with minimal redundancy\n",
    "- 3: Reasonably concise but could be tighter\n",
    "- 2: Somewhat verbose with unnecessary details\n",
    "- 1: Very verbose or poorly structured\n",
    "\n",
    "**Relevance (1-5):**\n",
    "- 5: Covers all main points and ignores minor details\n",
    "- 4: Covers most main points effectively\n",
    "- 3: Covers some main points but misses important ones\n",
    "- 2: Focuses on minor details while missing main points\n",
    "- 1: Completely irrelevant or misses the article's focus\n",
    "\n",
    "## Article:\n",
    "{{article}}\n",
    "\n",
    "## Highlight to Evaluate:\n",
    "{{highlight}}\n",
    "\n",
    "## Instructions:\n",
    "Evaluate the highlight against the article using the criteria above. Provide scores and explanations for each dimension, then calculate an overall quality score.\n",
    "\n",
    "**IMPORTANT: Respond ONLY with valid JSON in this exact format:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"factual_accuracy\": {\n",
    "    \"score\": [1-5],\n",
    "    \"explanation\": \"[2-3 sentence explanation]\"\n",
    "  },\n",
    "  \"conciseness\": {\n",
    "    \"score\": [1-5], \n",
    "    \"explanation\": \"[2-3 sentence explanation]\"\n",
    "  },\n",
    "  \"relevance\": {\n",
    "    \"score\": [1-5],\n",
    "    \"explanation\": \"[2-3 sentence explanation]\"\n",
    "  },\n",
    "  \"overall_quality\": {\n",
    "    \"score\": [1-5],\n",
    "    \"explanation\": \"[2-3 sentence summary explaining the overall score]\"\n",
    "  },\n",
    "  \"strengths\": [\"strength 1\", \"strength 2\"],\n",
    "  \"weaknesses\": [\"weakness 1\", \"weakness 2\"],\n",
    "  \"recommendation\": \"[improve/acceptable/excellent]\"\n",
    "}\n",
    "```\n",
    "\n",
    "Do not include any text outside the JSON response.\n",
    "\"\"\"\n",
    "\n",
    "JUDGE_PROMPT = Template(JUDGE_PROMPT)\n",
    "\n",
    "# Example usage\n",
    "sample_article = dataset[0]['article']\n",
    "sample_highlight = dataset[0]['highlights']\n",
    "print(JUDGE_PROMPT.render(article=sample_article[:500], highlight=sample_highlight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä **Plan to Grade Highlights with Multiple LLM Judges**\n",
    "\n",
    "* We‚Äôll create **3 separate LLM calls** using **GPT-4o-mini** (or any other model you prefer).\n",
    "* Each LLM will **evaluate the highlight** independently using the **JUDGE\\_PROMPT**.\n",
    "* We‚Äôll **parse** the JSON responses and **aggregate** the scores:\n",
    "    * Average score for each criterion\n",
    "    * Strengths and weaknesses\n",
    "    * Final consensus on overall quality\n",
    "\n",
    "‚úÖ This approach mimics a **‚Äú3 independent judges‚Äù** panel ‚Äî just like real editorial teams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë©‚Äç‚öñÔ∏è Calling LLM Judge 1...\n",
      "üë©‚Äç‚öñÔ∏è Calling LLM Judge 2...\n",
      "üë©‚Äç‚öñÔ∏è Calling LLM Judge 3...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def evaluate_highlight_with_judge(article, highlight, judge_prompt, client, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Calls the LLM to act as a judge and return JSON-formatted evaluation.\n",
    "    \"\"\"\n",
    "    prompt_text = judge_prompt.render(article=article, highlight=highlight)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise JSON-only responder.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_text}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract JSON response\n",
    "    json_pattern = r'```json\\s*({[\\s\\S]*?})\\s*```'\n",
    "    match = re.search(json_pattern, response.choices[0].message.content.strip())\n",
    "    if match:\n",
    "        raw_output = match.group(1)\n",
    "    else:\n",
    "        raise ValueError(f\"No JSON found in the response: {response.choices[0].message.content.strip()}\")\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON\n",
    "        parsed_json = json.loads(raw_output)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON response:\", e)\n",
    "        print(\"Raw output:\", raw_output)\n",
    "        parsed_json = None\n",
    "    \n",
    "    return parsed_json\n",
    "\n",
    "\n",
    "# üìù Generate evaluations from 3 different judges\n",
    "def generate_3_judges_evaluations(article, highlight, judge_prompt, client):\n",
    "    evaluations = []\n",
    "    for i in range(3):\n",
    "        print(f\"üë©‚Äç‚öñÔ∏è Calling LLM Judge {i+1}...\")\n",
    "        evaluation = evaluate_highlight_with_judge(article, highlight, judge_prompt, client)\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    return evaluations\n",
    "\n",
    "\n",
    "# üìù Example usage on the first example in the dataset\n",
    "sample_article = dataset[0]['article'][:1000]  # Use first 1000 chars for brevity\n",
    "sample_highlight = dataset[0]['highlights']\n",
    "\n",
    "evaluations = generate_3_judges_evaluations(sample_article, sample_highlight, JUDGE_PROMPT, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë©‚Äç‚öñÔ∏è Evaluation 1:\n",
      "{\n",
      "    \"factual_accuracy\": {\n",
      "        \"score\": 3,\n",
      "        \"explanation\": \"The highlight correctly states Tottenham's 2-1 victory over Arsenal, but it includes unrelated information about other matches, specifically Inter Milan and Bayern Munich, which do not appear in the article.\"\n",
      "    },\n",
      "    \"conciseness\": {\n",
      "        \"score\": 2,\n",
      "        \"explanation\": \"While the highlight starts by summarizing the key match, it includes irrelevant details about other teams' matches, making it somewhat verbose and scattered.\"\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"score\": 2,\n",
      "        \"explanation\": \"The highlight mostly fails to focus on the main points of the article, mainly regarding the significance of Tottenham's win and its implications, while introducing extraneous information about other teams.\"\n",
      "    },\n",
      "    \"overall_quality\": {\n",
      "        \"score\": 2,\n",
      "        \"explanation\": \"The highlight includes some accurate information concerning the match but is marred by irrelevant details and fails to adequately reflect the article's primary focus on Tottenham's victory and its implications.\"\n",
      "    },\n",
      "    \"strengths\": [\n",
      "        \"Accurately mentions the match result\",\n",
      "        \"Includes specific player performance (Gareth Bale)\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "        \"Includes irrelevant information about other matches\",\n",
      "        \"Lacks focus on the implications of the Tottenham victory\"\n",
      "    ],\n",
      "    \"recommendation\": \"improve\"\n",
      "}\n",
      "\n",
      "üë©‚Äç‚öñÔ∏è Evaluation 2:\n",
      "{\n",
      "    \"factual_accuracy\": {\n",
      "        \"score\": 4,\n",
      "        \"explanation\": \"The highlight accurately states that Tottenham won 2-1 against Arsenal. However, it incorrectly adds information about Gareth Bale's goals and results from other matches, which were not mentioned in the article.\"\n",
      "    },\n",
      "    \"conciseness\": {\n",
      "        \"score\": 3,\n",
      "        \"explanation\": \"The highlight could be more concise as it includes extraneous details about other matches that are unrelated to the main article focus. The primary point could be communicated more succinctly.\"\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"score\": 2,\n",
      "        \"explanation\": \"While it mentions the Tottenham vs. Arsenal match, the additional information about Inter Milan and Bayern Munich is irrelevant to the article's focus, which centers on the rivalry between the two London teams.\"\n",
      "    },\n",
      "    \"overall_quality\": {\n",
      "        \"score\": 3,\n",
      "        \"explanation\": \"The highlight provides some accurate information regarding the Tottenham-Arsenal match, but it includes irrelevant details and is not as concise as it could be. Overall, it captures some main points but misses on clarity and focus.\"\n",
      "    },\n",
      "    \"strengths\": [\n",
      "        \"Correctly identifies the result of the Tottenham vs Arsenal match\",\n",
      "        \"Highlights Tottenham's positioning in the league\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "        \"Includes irrelevant match results\",\n",
      "        \"Could be more concise\"\n",
      "    ],\n",
      "    \"recommendation\": \"improve\"\n",
      "}\n",
      "\n",
      "üë©‚Äç‚öñÔ∏è Evaluation 3:\n",
      "{\n",
      "    \"factual_accuracy\": {\n",
      "        \"score\": 3,\n",
      "        \"explanation\": \"The highlight accurately states Tottenham's 2-1 victory over Arsenal, but the mention of Gareth Bale's goals is not supported by the article. Also, the other two sentences about Inter Milan and Bayern Munich are not relevant to the article.\"\n",
      "    },\n",
      "    \"conciseness\": {\n",
      "        \"score\": 4,\n",
      "        \"explanation\": \"The highlight is mostly concise, with brief statements about the matches. However, it includes details unrelated to the main focus of the article about the Tottenham and Arsenal match.\"\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"score\": 2,\n",
      "        \"explanation\": \"While it includes the main result of the Tottenham vs Arsenal match, it strays into unrelated content on Inter Milan and Bayern Munich, missing the overall focus of the article.\"\n",
      "    },\n",
      "    \"overall_quality\": {\n",
      "        \"score\": 2,\n",
      "        \"explanation\": \"The highlight includes some accurate information about the main event but diverges significantly into unrelated topics, diminishing its usefulness as a summary of the article.\"\n",
      "    },\n",
      "    \"strengths\": [\n",
      "        \"Includes correct score of the main match\",\n",
      "        \"Briefly states Tottenham's current position\"\n",
      "    ],\n",
      "    \"weaknesses\": [\n",
      "        \"Includes unrelated match results\",\n",
      "        \"Mentions of specific player goals lack support from the article\"\n",
      "    ],\n",
      "    \"recommendation\": \"improve\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show evaluations\n",
    "for idx, eval_json in enumerate(evaluations, 1):\n",
    "    print(f\"\\nüë©‚Äç‚öñÔ∏è Evaluation {idx}:\")\n",
    "    print(json.dumps(eval_json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Evaluation aggregation\n",
    "\n",
    "* For each criterion (factual accuracy, conciseness, relevance, overall quality):\n",
    "    * Compute the average score\n",
    "    * Summarize the explanations (highlight common insights)\n",
    "* For strengths and weaknesses:\n",
    "    * Merge them into a single list, deduplicating and highlighting most frequent mentions.\n",
    "* For recommendation:\n",
    "    * Use a majority vote: If 2/3 judges say ‚Äúexcellent,‚Äù that‚Äôs the consensus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ CONSENSUS AGGREGATED EVALUATION:\n",
      "{\n",
      "  \"average_scores\": {\n",
      "    \"factual_accuracy\": 3.33,\n",
      "    \"conciseness\": 3.0,\n",
      "    \"relevance\": 2.0,\n",
      "    \"overall_quality\": 2.33\n",
      "  },\n",
      "  \"explanations\": {\n",
      "    \"factual_accuracy\": \"The highlight correctly states Tottenham's 2-1 victory over Arsenal, but it includes unrelated information about other matches, specifically Inter Milan and Bayern Munich, which do not appear in the article. / The highlight accurately states that Tottenham won 2-1 against Arsenal. However, it incorrectly adds information about Gareth Bale's goals and results from other matches, which were not mentioned in the article. / The highlight accurately states Tottenham's 2-1 victory over Arsenal, but the mention of Gareth Bale's goals is not supported by the article. Also, the other two sentences about Inter Milan and Bayern Munich are not relevant to the article.\",\n",
      "    \"conciseness\": \"While the highlight starts by summarizing the key match, it includes irrelevant details about other teams' matches, making it somewhat verbose and scattered. / The highlight could be more concise as it includes extraneous details about other matches that are unrelated to the main article focus. The primary point could be communicated more succinctly. / The highlight is mostly concise, with brief statements about the matches. However, it includes details unrelated to the main focus of the article about the Tottenham and Arsenal match.\",\n",
      "    \"relevance\": \"The highlight mostly fails to focus on the main points of the article, mainly regarding the significance of Tottenham's win and its implications, while introducing extraneous information about other teams. / While it mentions the Tottenham vs. Arsenal match, the additional information about Inter Milan and Bayern Munich is irrelevant to the article's focus, which centers on the rivalry between the two London teams. / While it includes the main result of the Tottenham vs Arsenal match, it strays into unrelated content on Inter Milan and Bayern Munich, missing the overall focus of the article.\",\n",
      "    \"overall_quality\": \"The highlight includes some accurate information concerning the match but is marred by irrelevant details and fails to adequately reflect the article's primary focus on Tottenham's victory and its implications. / The highlight provides some accurate information regarding the Tottenham-Arsenal match, but it includes irrelevant details and is not as concise as it could be. Overall, it captures some main points but misses on clarity and focus. / The highlight includes some accurate information about the main event but diverges significantly into unrelated topics, diminishing its usefulness as a summary of the article.\"\n",
      "  },\n",
      "  \"strengths\": [\n",
      "    \"Includes specific player performance (Gareth Bale)\",\n",
      "    \"Correctly identifies the result of the Tottenham vs Arsenal match\",\n",
      "    \"Includes correct score of the main match\",\n",
      "    \"Briefly states Tottenham's current position\",\n",
      "    \"Accurately mentions the match result\",\n",
      "    \"Highlights Tottenham's positioning in the league\"\n",
      "  ],\n",
      "  \"weaknesses\": [\n",
      "    \"Includes irrelevant match results\",\n",
      "    \"Includes unrelated match results\",\n",
      "    \"Could be more concise\",\n",
      "    \"Mentions of specific player goals lack support from the article\",\n",
      "    \"Includes irrelevant information about other matches\",\n",
      "    \"Lacks focus on the implications of the Tottenham victory\"\n",
      "  ],\n",
      "  \"consensus_recommendation\": \"improve\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def aggregate_judge_scores(evaluations):\n",
    "    \"\"\"\n",
    "    Aggregates scores and insights from multiple judges.\n",
    "    \"\"\"\n",
    "    criteria = [\"factual_accuracy\", \"conciseness\", \"relevance\", \"overall_quality\"]\n",
    "    aggregated_scores = {c: 0 for c in criteria}\n",
    "    explanations = {c: [] for c in criteria}\n",
    "    \n",
    "    strengths = []\n",
    "    weaknesses = []\n",
    "    recommendations = []\n",
    "    \n",
    "    valid_judges = 0  # in case some JSON parsing failed\n",
    "    \n",
    "    for eval_json in evaluations:\n",
    "        if eval_json is None:\n",
    "            continue  # skip if no valid data\n",
    "        valid_judges += 1\n",
    "        \n",
    "        for c in criteria:\n",
    "            aggregated_scores[c] += eval_json[c][\"score\"]\n",
    "            explanations[c].append(eval_json[c][\"explanation\"])\n",
    "        \n",
    "        strengths.extend(eval_json[\"strengths\"])\n",
    "        weaknesses.extend(eval_json[\"weaknesses\"])\n",
    "        recommendations.append(eval_json[\"recommendation\"])\n",
    "    \n",
    "    if valid_judges == 0:\n",
    "        return None\n",
    "    \n",
    "    # Compute average scores\n",
    "    avg_scores = {c: round(aggregated_scores[c]/valid_judges, 2) for c in criteria}\n",
    "    \n",
    "    # Merge explanations\n",
    "    merged_explanations = {c: \" / \".join(explanations[c]) for c in criteria}\n",
    "    \n",
    "    # Deduplicate strengths and weaknesses\n",
    "    unique_strengths = list(set(strengths))\n",
    "    unique_weaknesses = list(set(weaknesses))\n",
    "    \n",
    "    # Determine majority recommendation\n",
    "    rec_counter = Counter(recommendations)\n",
    "    majority_recommendation = rec_counter.most_common(1)[0][0]\n",
    "    \n",
    "    # Final aggregated result\n",
    "    consensus_result = {\n",
    "        \"average_scores\": avg_scores,\n",
    "        \"explanations\": merged_explanations,\n",
    "        \"strengths\": unique_strengths,\n",
    "        \"weaknesses\": unique_weaknesses,\n",
    "        \"consensus_recommendation\": majority_recommendation\n",
    "    }\n",
    "    \n",
    "    return consensus_result\n",
    "\n",
    "\n",
    "# üìù Example usage on the three judges' evaluations\n",
    "consensus = aggregate_judge_scores(evaluations)\n",
    "\n",
    "print(\"\\nüéØ CONSENSUS AGGREGATED EVALUATION:\")\n",
    "print(json.dumps(consensus, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Full Evaluation Workflow for 50 Examples\n",
    "\n",
    "In this section, we‚Äôll:\n",
    "\n",
    "‚úÖ Generate **three independent LLM-based evaluations** (judges) for **50 random examples** from the CNN/DailyMail dataset highlights.  \n",
    "‚úÖ **Aggregate the judges‚Äô scores and insights** to produce a **consensus evaluation** for each highlight.  \n",
    "‚úÖ Analyze the **distribution of grades** (mean, standard deviation) to see how consistent the judges are.  \n",
    "‚úÖ Concatenate and review the **common strengths and weaknesses** for highlights overall.\n",
    "\n",
    "This **multi-judge evaluation** ensures that:\n",
    "- The final scores are more robust (less subjective bias from a single judge).\n",
    "- We get a nuanced picture of strengths and weaknesses.\n",
    "- We can refine our prompts and highlight-generation techniques based on solid evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [14:40<00:00, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done evaluating 50 examples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Randomly select 50 samples\n",
    "subset_indices = random.sample(range(len(dataset)), 50)\n",
    "subset_articles = [dataset[i]['article'] for i in subset_indices]\n",
    "subset_highlights = [dataset[i]['highlights'] for i in subset_indices]\n",
    "\n",
    "all_consensus_results = []\n",
    "\n",
    "# Loop over each example and do the full workflow\n",
    "for idx, (article, highlight) in tqdm(enumerate(zip(subset_articles, subset_highlights)), total=len(subset_articles)):\n",
    "    \n",
    "    # Generate the prompt for judges\n",
    "    prompt = JUDGE_PROMPT.render(article=article, highlight=highlight)\n",
    "    \n",
    "    # Get 3 independent LLM-based evaluations (using mini versions for speed)\n",
    "    evaluations = []\n",
    "    for _ in range(3):\n",
    "        evaluation = evaluate_highlight_with_judge(article, highlight, JUDGE_PROMPT, client)\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    # Aggregate the evaluations\n",
    "    consensus = aggregate_judge_scores(evaluations)\n",
    "    if consensus:\n",
    "        all_consensus_results.append(consensus)\n",
    "\n",
    "print(\"\\n‚úÖ Done evaluating 50 examples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Mean Scores Across 50 Examples:\n",
      "{'factual_accuracy': 4.1, 'conciseness': 4.81, 'relevance': 4.11, 'overall_quality': 4.09}\n",
      "\n",
      "üìä Standard Deviation of Scores:\n",
      "{'factual_accuracy': 0.39, 'conciseness': 0.32, 'relevance': 0.4, 'overall_quality': 0.39}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute mean and std for each criterion across all examples\n",
    "criteria = [\"factual_accuracy\", \"conciseness\", \"relevance\", \"overall_quality\"]\n",
    "\n",
    "mean_scores = {}\n",
    "std_scores = {}\n",
    "\n",
    "for c in criteria:\n",
    "    scores = [c_res[\"average_scores\"][c] for c_res in all_consensus_results]\n",
    "    mean_scores[c] = round(np.mean(scores), 2)\n",
    "    std_scores[c] = round(np.std(scores), 2)\n",
    "\n",
    "print(\"\\nüìà Mean Scores Across 50 Examples:\")\n",
    "print(mean_scores)\n",
    "\n",
    "print(\"\\nüìä Standard Deviation of Scores:\")\n",
    "print(std_scores)\n",
    "\n",
    "# Create the bar chart with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "criteria_labels = [c.replace('_', ' ').title() for c in criteria]\n",
    "means = [mean_scores[c] for c in criteria]\n",
    "stds = [std_scores[c] for c in criteria]\n",
    "\n",
    "bars = plt.bar(criteria_labels, means, yerr=stds, capsize=5, \n",
    "               color=['#2E8B57', '#4682B4', '#FF6347', '#9370DB'], alpha=0.7)\n",
    "\n",
    "plt.title('LLM-as-a-Judge: Mean Scores with Standard Deviation', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Score (1-5 scale)', fontsize=12)\n",
    "plt.ylim(0, 5)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    plt.text(i, mean + std + 0.1, f'{mean}¬±{std}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí™ Most Common Strengths:\n",
      "[('Concise summary of key points', 16), ('Concise and to the point', 9), ('Concise summary of main points', 9), ('Concise summary of key events', 8), ('High factual accuracy', 6), ('Accurate representation of facts', 4), ('Concise presentation of information', 4), ('Concise presentation of key points', 4), ('Accurate representation of key points', 3), ('Concise and clear presentation of information', 3)]\n",
      "\n",
      "‚ö†Ô∏è Most Common Weaknesses:\n",
      "[('Lacks some contextual details', 5), ('Misses some important details', 2), ('Lacks specific details on the total number of canceled flights', 1), ('Does not mention the ongoing disruptions clearly enough', 1), ('Lacks some detail on the full impact of the incident', 1), (\"Does not include the information about the airport's warning of ongoing travel disruption\", 1), ('Misses broader thematic context related to political influence', 1), ('Lacks detail on personal struggles', 1), ('Misses mentioning the political influences on her songwriting', 1), ('Lacks detail on specific health struggles', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all strengths and weaknesses\n",
    "all_strengths = []\n",
    "all_weaknesses = []\n",
    "\n",
    "for res in all_consensus_results:\n",
    "    all_strengths.extend(res[\"strengths\"])\n",
    "    all_weaknesses.extend(res[\"weaknesses\"])\n",
    "\n",
    "# Deduplicate and sort by frequency\n",
    "strengths_counter = Counter(all_strengths)\n",
    "weaknesses_counter = Counter(all_weaknesses)\n",
    "\n",
    "print(\"\\nüí™ Most Common Strengths:\")\n",
    "print(strengths_counter.most_common(10))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Most Common Weaknesses:\")\n",
    "print(weaknesses_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Using Strengths & Weaknesses for Prompt Refinement\n",
    "\n",
    "We‚Äôll leverage:\n",
    "\n",
    "‚úÖ **Top Strengths** ‚Äì What the judges said was consistently good  \n",
    "‚úÖ **Top Weaknesses** ‚Äì The most common issues or errors  \n",
    "‚úÖ **Best Summaries** ‚Äì Examples that scored highest  \n",
    "‚úÖ **Worst Summaries** ‚Äì Examples that scored lowest  \n",
    "\n",
    "**Goal:** Use these insights to craft an **improved prompt** for LLMs that **maximizes strengths** and **minimizes weaknesses** when generating new highlights!\n",
    "\n",
    "Here‚Äôs the next step:\n",
    "- Create a **meta-prompt**: It will feed in the list of strengths, weaknesses, best examples, and worst examples.\n",
    "- The LLM will output a **refined prompt** we can use to generate better highlights in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert editor tasked with refining a prompt that generates high-quality highlights for news articles.\n",
      "\n",
      "Here are the insights from a recent evaluation of 50 highlights:\n",
      "\n",
      "**Top Strengths:**\n",
      "- Concise summary of key points\n",
      "- Concise and to the point\n",
      "- Concise summary of main points\n",
      "- Concise summary of key events\n",
      "- High factual accuracy\n",
      "- Accurate representation of facts\n",
      "- Concise presentation of information\n",
      "- Concise presentation of key points\n",
      "- Accurate representation of key points\n",
      "- Concise and clear presentation of information\n",
      "\n",
      "**Top Weaknesses:**\n",
      "- Lacks some contextual details\n",
      "- Misses some important details\n",
      "- Lacks specific details on the total number of canceled flights\n",
      "- Does not mention the ongoing disruptions clearly enough\n",
      "- Lacks some detail on the full impact of the incident\n",
      "- Does not include the information about the airport's warning of ongoing travel disruption\n",
      "- Misses broader thematic context related to political influence\n",
      "- Lacks detail on personal struggles\n",
      "- Misses mentioning the political influences on her songwriting\n",
      "- Lacks detail on specific health struggles\n",
      "\n",
      "**Best Summaries:**\n",
      "\n",
      "- \"The 47-year-old woman was killed Saturday morning .\n",
      "The Taliban accused her of adultery, a local official said .\n",
      "Woman was whipped before she was shot .\"\n",
      "\n",
      "- \"Frida Ghitis: Kurdish region in Syria approved new law ordering equality for women .\n",
      "Move shows ideological battleground between ISIS, which enslaves women, and Kurds .\n",
      "Ghitis says it's message to West on human rights and equality, to encourage West's support .\n",
      "Ghitis: Battle against ISIS helping Kurdish women achieve equality .\"\n",
      "\n",
      "- \"More airlines providing Wi-Fi on their flights .\n",
      "Norwegian Air first airline in Europe to offer free Wi-Fi .\n",
      "Gogo system installed on 1,600 U.S. jets, according to CEO .\n",
      "Row 44 provides satellite-based system that can access internet even over the ocean .\"\n",
      "\n",
      "\n",
      "**Worst Summaries:**\n",
      "\n",
      "- \"Baby born in makeshift hospital was \"perfect delivery in a very imperfect environment\"\n",
      "Roughly 300,000 pregnant women, new mothers in need of food, government says .\n",
      "U.N. agency says it's distributing solar-powered lamps to curb \"gender-based violence\"\"\n",
      "\n",
      "- \"NEW: Hospital staff makes release recommendations .\n",
      "John Hinckley's sister says she supports letting him eventually live in Williamsburg .\n",
      "She says he feels comfortable there and is accepted by many community members .\n",
      "Hinckley has been hospitalized since he shot President Reagan in 1981 .\"\n",
      "\n",
      "- \"Police say Imran Khawaja, of west London, trained with a terror group in Syria last year .\n",
      "He was in a promotional video showing him holding a severed head, police say .\n",
      "The terror group said he'd died, but he really was trying to return to the UK, where he was arrested, police say .\"\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "**TASK:**\n",
      "Using these insights, create a new prompt to generate highlights that:\n",
      "- Emphasizes the listed strengths.\n",
      "- Avoids the listed weaknesses.\n",
      "- Learns from the best summaries.\n",
      "- Steers clear of the mistakes in the worst summaries.\n",
      "\n",
      "**Your Response Should Be:**\n",
      "A single improved prompt that can be used to guide an LLM to generate better highlights.\n",
      "\n",
      "**Format:**\n",
      "Return only the new prompt ‚Äî no explanations or extra text.\n"
     ]
    }
   ],
   "source": [
    "grade = [(i, all_consensus_results[i]['average_scores']['overall_quality']) for i in range(len(all_consensus_results))]\n",
    "best_summaries = [dataset[i]['highlights'] for i, _ in sorted(grade, key=lambda x: x[1], reverse=True)[:3]]\n",
    "worst_summaries = [dataset[i]['highlights'] for i, _ in sorted(grade, key=lambda x: x[1])[:3]]\n",
    "\n",
    "top_strengths = [res[0] for res in strengths_counter.most_common(10)]\n",
    "top_weaknesses = [res[0] for res in weaknesses_counter.most_common(10)]\n",
    "\n",
    "# Create a meta-prompt template\n",
    "META_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert editor tasked with refining a prompt that generates high-quality highlights for news articles.\n",
    "\n",
    "Here are the insights from a recent evaluation of 50 highlights:\n",
    "\n",
    "**Top Strengths:**\n",
    "{{ strengths }}\n",
    "\n",
    "**Top Weaknesses:**\n",
    "{{ weaknesses }}\n",
    "\n",
    "**Best Summaries:**\n",
    "{% for summary in best_summaries %}\n",
    "- \"{{ summary }}\"\n",
    "{% endfor %}\n",
    "\n",
    "**Worst Summaries:**\n",
    "{% for summary in worst_summaries %}\n",
    "- \"{{ summary }}\"\n",
    "{% endfor %}\n",
    "\n",
    "---\n",
    "\n",
    "**TASK:**\n",
    "Using these insights, create a new prompt to generate highlights that:\n",
    "- Emphasizes the listed strengths.\n",
    "- Avoids the listed weaknesses.\n",
    "- Learns from the best summaries.\n",
    "- Steers clear of the mistakes in the worst summaries.\n",
    "\n",
    "**Your Response Should Be:**\n",
    "A single improved prompt that can be used to guide an LLM to generate better highlights.\n",
    "\n",
    "**Format:**\n",
    "Return only the new prompt ‚Äî no explanations or extra text.\n",
    "\"\"\"\n",
    "\n",
    "meta_prompt = Template(META_PROMPT_TEMPLATE).render(\n",
    "    strengths=\"\\n\".join([f\"- {s}\" for s in top_strengths]),\n",
    "    weaknesses=\"\\n\".join([f\"- {w}\" for w in top_weaknesses]),\n",
    "    best_summaries=best_summaries,\n",
    "    worst_summaries=worst_summaries\n",
    ")\n",
    "\n",
    "print(meta_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Refined Prompt:\n",
      "\n",
      "\"Generate a concise and clear summary of the news article that accurately represents the key points and facts. Ensure high factual accuracy by including specific details about major events or statements. Provide necessary context, such as ongoing impacts or broader themes like political or personal influences, to give a fuller understanding. Avoid omitting critical information and aim to enhance reader comprehension by integrating specific statistics or figures where relevant.\"\n"
     ]
    }
   ],
   "source": [
    "# Use LLM to generate the refined prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in prompt engineering.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": meta_prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get and display the final refined prompt\n",
    "refined_prompt = response.choices[0].message.content.strip()\n",
    "print(\"\\nüéØ Refined Prompt:\\n\")\n",
    "print(refined_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Generating New Highlights with the Refined Prompt\n",
    "\n",
    "We now have a **refined prompt** that integrates:\n",
    "- ‚úÖ The top strengths & weaknesses  \n",
    "- ‚úÖ Lessons from the best and worst summaries\n",
    "\n",
    "We‚Äôll use this refined prompt to guide the **LLM** (GPT-4o) in generating **improved highlights** for the **50 articles** we previously evaluated.\n",
    "\n",
    "The workflow is:\n",
    "\n",
    "1Ô∏è‚É£ Use the **refined prompt** as the system message.  \n",
    "2Ô∏è‚É£ Provide the **article text** as the user message.  \n",
    "3Ô∏è‚É£ Generate a **new highlight** for each article.\n",
    "\n",
    "We‚Äôll store these new highlights for **further evaluation** and **comparison**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:57<00:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done generating 50 new highlights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "HIGHLIGHT_GENERATION_PROMPT = \"\"\"\n",
    "Generate a concise and clear summary of the news article that accurately represents the key points and facts. \n",
    "Ensure high factual accuracy by including specific details about major events or statements. \n",
    "Provide necessary context, such as ongoing impacts or broader themes like political or personal influences, to give a fuller understanding. \n",
    "Avoid omitting critical information and aim to enhance reader comprehension by integrating specific statistics or figures where relevant.\n",
    "\n",
    "Article:\n",
    "{{article}}\n",
    "\n",
    "Highlight:\n",
    "\"\"\"\n",
    "\n",
    "HIGHLIGHT_GENERATION_PROMPT = Template(HIGHLIGHT_GENERATION_PROMPT)\n",
    "\n",
    "concurrent_highlights = []\n",
    "for idx, article in tqdm(enumerate(subset_articles), total=len(subset_articles)):\n",
    "    \n",
    "    # Generate the prompt for judges\n",
    "    prompt = HIGHLIGHT_GENERATION_PROMPT.render(article=article)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional news editor.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    highlight = response.choices[0].message.content.strip()\n",
    "    concurrent_highlights.append(highlight)\n",
    "\n",
    "print(\"\\n‚úÖ Done generating 50 new highlights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Evaluating & Comparing Highlights with Order-Balanced Judging\n",
    "\n",
    "Now that we‚Äôve generated **new highlights** using the refined prompt, it‚Äôs time to **evaluate** and **compare** them with the **original highlights**. However, we know that LLMs can be sensitive to the **order of presentation** in prompts, leading to **order bias**.\n",
    "\n",
    "### üîç What We‚Äôll Do\n",
    "\n",
    "‚úÖ Use the **same evaluation criteria** as before (**factual accuracy, conciseness, relevance, overall quality**).  \n",
    "‚úÖ Evaluate each pair of highlights (**original** vs. **new**) in **both orders**:\n",
    "- **Order 1:** Original highlight first, new highlight second.\n",
    "- **Order 2:** New highlight first, original highlight second.\n",
    "\n",
    "‚úÖ Use **2 independent LLMs** to **cross-check** the evaluations.  \n",
    "‚úÖ Analyze if the **order of presentation** affects the results, helping us identify and **debias** the evaluation process.\n",
    "\n",
    "### üìä Key Objectives\n",
    "\n",
    "1Ô∏è‚É£ **Compare the quality** of the new highlights vs. the original highlights.  \n",
    "2Ô∏è‚É£ **Understand** how LLM order bias may affect evaluation.  \n",
    "3Ô∏è‚É£ Explore strategies for **debiasing** LLM-based judgments.\n",
    "\n",
    "Let‚Äôs start by defining a prompt that presents **both highlights side-by-side** and asks for **direct scoring & comparison**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:34<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "# Evaluation prompt with dual highlight comparison\n",
    "DUAL_JUDGE_PROMPT = \"\"\"\n",
    "You are an expert news editor with 10+ years of experience evaluating content quality. Your task is to compare two highlights summarizing the same article.\n",
    "\n",
    "## Evaluation Criteria:\n",
    "\n",
    "**For Each Highlight consider the following criteria:**\n",
    "- Factual Accuracy\n",
    "- Conciseness\n",
    "- Relevance\n",
    "\n",
    "**Comparison Instructions:**\n",
    "- Identify which highlight is better in each category.\n",
    "- Provide brief explanations for your ratings.\n",
    "- Summarize your overall assessment and **declare which highlight is better**.\n",
    "\n",
    "## Article:\n",
    "{{article}}\n",
    "\n",
    "## Highlight A:\n",
    "{{highlight_a}}\n",
    "\n",
    "## Highlight B:\n",
    "{{highlight_b}}\n",
    "\n",
    "## IMPORTANT: Provide your response in valid JSON with this exact format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"better_highlight\": \"A or B\",\n",
    "  \"summary\": \"A short explanation of why one highlight is better overall.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Do not include any text outside the JSON response.\n",
    "\"\"\"\n",
    "\n",
    "DUAL_JUDGE_PROMPT = Template(DUAL_JUDGE_PROMPT)\n",
    "\n",
    "# Store evaluations\n",
    "evaluations = []\n",
    "\n",
    "for idx, article, highlight_a, highlight_b in tqdm(zip(range(50), subset_articles, subset_highlights, concurrent_highlights), total=50):\n",
    "    article_text = article\n",
    "\n",
    "    # Order 1: Original A, New B\n",
    "    prompt1 = DUAL_JUDGE_PROMPT.render(article=article_text, highlight_a=highlight_a, highlight_b=highlight_b)\n",
    "    \n",
    "    # Order 2: New A, Original B\n",
    "    prompt2 = DUAL_JUDGE_PROMPT.render(article=article_text, highlight_a=highlight_b, highlight_b=highlight_a)\n",
    "\n",
    "    # Evaluate with LLM1 for Order 1\n",
    "    response1 = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt1}]\n",
    "    )\n",
    "    output = re.findall(r'```json(.*)```', response1.choices[0].message.content, re.DOTALL)\n",
    "    scores1 = json.loads(output[0])\n",
    "\n",
    "    # Evaluate with LLM2 for Order 2\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt2}]\n",
    "    )\n",
    "    \n",
    "    output = re.findall(r'```json(.*)```', response2.choices[0].message.content, re.DOTALL)\n",
    "    scores2 = json.loads(output[0])\n",
    "\n",
    "    evaluations.append({\n",
    "        \"index\": idx,\n",
    "        \"order_1\": scores1,\n",
    "        \"order_2\": scores2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'better_highlight': 'B',\n",
       "  'summary': \"Highlight B provides a comprehensive overview of the situation, including key details about Hultz's identity, circumstances of his abduction, and the context surrounding his release. It captures essential facts while maintaining a clear narrative, making it more informative than Highlight A, which is less detailed and lacks coherence.\"},\n",
       " {'better_highlight': 'A',\n",
       "  'summary': \"Highlight A is better overall because it presents a comprehensive and factually accurate summary of the article, encapsulating the key details about Hultz's abduction, identity confusion, and release. It is concise yet thorough enough to cover the complexity of the situation, including his business ties and the Pentagon's confirmation. Highlight B, while more concise, lacks important details about Hultz's background and the contextual significance of his capture and release.\"})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1, scores2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Analyzing Order Bias in LLM Judgments\n",
    "\n",
    "Now that we have evaluations for **50 examples** in **both orders** (Original-first and New-first), it‚Äôs time to analyze:\n",
    "\n",
    "**1Ô∏è‚É£ Which highlight was judged better overall in each order?**  \n",
    "**2Ô∏è‚É£ How often the ‚Äúbetter highlight‚Äù switches based on the order?**  \n",
    "**3Ô∏è‚É£ Mean and standard deviation of the scores** to see stability or fluctuations.\n",
    "\n",
    "This will help us answer:\n",
    "\n",
    "üëâ **Do LLM judgments change depending on the order of highlights?**  \n",
    "üëâ **How can we adjust our methodology to mitigate bias?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü© **Summary of Order Bias** üü©\n",
      "Total highlight pairs: 50\n",
      "Order flips (different better highlight based on order): 0 (0.00%)\n",
      "Best highlights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "better_order_1\n",
       "B    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataframe to store results\n",
    "results_df = pd.DataFrame([{\n",
    "    \"index\": e[\"index\"],\n",
    "    \"better_order_1\": e[\"order_1\"][\"better_highlight\"],\n",
    "    \"better_order_2\": 'B' if e[\"order_2\"][\"better_highlight\"] == 'A' else 'A' #as we changed the order of the highlights\n",
    "} for e in evaluations])\n",
    "\n",
    "# Check for order flips\n",
    "results_df[\"order_flip\"] = results_df[\"better_order_1\"] != results_df[\"better_order_2\"]\n",
    "\n",
    "# Summary stats\n",
    "total_flips = results_df[\"order_flip\"].sum()\n",
    "flip_percentage = total_flips / len(results_df) * 100\n",
    "\n",
    "print(\"üü© **Summary of Order Bias** üü©\")\n",
    "print(f\"Total highlight pairs: {len(results_df)}\")\n",
    "print(f\"Order flips (different better highlight based on order): {total_flips} ({flip_percentage:.2f}%)\")\n",
    "print(\"Best highlights:\")\n",
    "results_df.better_order_1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü© **Comments on the Results**\n",
    "\n",
    "‚úÖ **No Order Flips:**\n",
    "Surprisingly, there were **0 flips** ‚Äî the better highlight **remained consistent** regardless of the order.\n",
    "This suggests that our **LLMs are robust** to order bias in this setup.\n",
    "\n",
    "‚úÖ **Better Highlight: Consistently ‚ÄúB‚Äù**\n",
    "In **all 50 pairs**, the second highlight (`B`) was judged as better.\n",
    "This consistent result can indicate:\n",
    "\n",
    "* **Real improvement:** The new highlights generated after leveraging LLM feedback were **genuinely better**.\n",
    "* **LLM bias towards new highlights:** There might be a subtle **recency bias** in LLM responses, favoring the second highlight.\n",
    "\n",
    "### üí° **Key Takeaways**\n",
    "\n",
    "* In this scenario, there was **no evidence of order-induced bias**.\n",
    "* However, the consistent preference for the ‚ÄúB‚Äù highlights suggests that our improvement loop (using weaknesses and strengths) was **effective**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ **Conclusion: LLMs as a Judge**\n",
    "\n",
    "This experiment reinforces the **power and practicality** of using **LLMs to replace human judges** in evaluation tasks:\n",
    "\n",
    "‚úÖ **1Ô∏è‚É£ Excellent Proxy for Human Evaluation**\n",
    "The LLM showed **strong consistency** and **nuanced explanations** across 50 highlight pairs.\n",
    "This demonstrates that LLMs can **reliably evaluate** content quality, even at scale.\n",
    "\n",
    "‚úÖ **2Ô∏è‚É£ Alignment with Human Feedback**\n",
    "To ensure **maximum precision and relevance**, it‚Äôs crucial to **align LLM-based evaluations** with actual **human feedback**.\n",
    "Examining **correlations** (e.g., Pearson or Spearman) with human ratings can help calibrate the LLM‚Äôs judgments and reduce gaps.\n",
    "\n",
    "‚úÖ **3Ô∏è‚É£ Potential Biases Toward LLM-Generated Outputs**\n",
    "We noted the LLM **consistently preferred** the newly generated highlights (by another LLM) ‚Äî even if they were not always clearly superior.\n",
    "This highlights the **risk of bias**: LLMs might favor content generated in their own ‚Äústyle,‚Äù potentially overvaluing it.\n",
    "\n",
    "‚úÖ **4Ô∏è‚É£ Ranking-Based Judgments Anchor Scores**\n",
    "By **ranking pairs directly** rather than relying only on 1-5 absolute scores, the evaluations become **anchored** and more **robust**.\n",
    "This relative comparison can reduce variability and better capture **true differences** in quality between two strategies.\n",
    "\n",
    "---\n",
    "\n",
    "**üîç Final Thought:**\n",
    "While LLMs offer a **powerful, scalable, and explainable** judging mechanism, it‚Äôs essential to validate and **continuously refine** them with **human evaluations**.\n",
    "This ensures we get the **best of both worlds**: the speed and scale of LLMs, plus the **human grounding** that ensures true accuracy and relevance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse-nlp-DetGwK6_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
