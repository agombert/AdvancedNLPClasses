# Transformers & BERT

## Session 5: The Transformer Revolution

This session explores the transformer architecture and its groundbreaking impact on NLP, with a special focus on BERT (Bidirectional Encoder Representations from Transformers) and its variants.

### Learning Objectives

- Understand the transformer architecture and its advantages over recurrent models
- Learn about self-attention mechanisms and their role in capturing contextual relationships
- Explore BERT's architecture, pre-training objectives, and fine-tuning approaches
- Gain practical knowledge of implementing and using transformer-based models
- Understand the significance of bidirectional context in language understanding

### Topics Covered

#### Transformer Intuition

- The transformer architecture: encoders and decoders
- Self-attention layers and multi-head attention
- Position encodings and handling sequential information without recurrence
- Comparisons with recurrent neural networks (RNNs, LSTMs, GRUs)
- Advantages of parallelization in transformers
- Scaling properties of transformer models

#### BERT Architecture

- Bidirectional context and masked language modeling
- Next sentence prediction as a pre-training objective
- BERT's input representation (tokens, segments, positions)
- Pre-training on large corpora
- Fine-tuning for downstream tasks
- The revolution of Large Language Models: how BERT changed NLP
- BERT variants and improvements (RoBERTa, DistilBERT, etc.)

### Recommended Reading

- Vaswani et al. (2017) "Attention Is All You Need"
- Uszkoreit, Jakop (2017) "Transformer: A Novel Neural Network Architecture for Language Understanding"
- Alamar, Jay (2018) "The Illustrated Transformer"
- Devlin et al. (2018) "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
- Liu et al. (2019) "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
- Wolf et al. (2019) "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
- Adaloglou, Nikola (2020) "How Transformers work in deep learning and NLP: an intuitive introduction"

### Practical Components

- Implementing a simplified transformer model
- Using pre-trained BERT models from Hugging Face
- Fine-tuning BERT for text classification tasks
- Visualizing attention patterns in transformer models
- Comparing performance of transformer-based models with previous approaches
