{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66hrP5h7ttNn"
      },
      "source": [
        "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)\n",
        "\n",
        "# Advanced Methods in Natural Language Processing - Session 4\n",
        "\n",
        "# Text Classification with AG News Corpus\n",
        "\n",
        "This notebook will guide you through different approaches to text classification using the AG News corpus. We will start with a simple baseline model and gradually move towards more complex and sophisticated models.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. **[Part 1: Baseline Pipeline with TF-IDF and Linear Model](#part1)**\n",
        "   - 1.1. Loading and Exploring Data\n",
        "   - 1.2. Feature Extraction with TF-IDF\n",
        "   - 1.3. Training a Linear Model\n",
        "   - 1.4. Model Evaluation\n",
        "\n",
        "2. **[Part 2: LSTM Pipeline with One-Hot Encoding](#part2)**\n",
        "   - 2.1. Preprocessing for LSTM\n",
        "   - 2.2. Building a Bidirectional LSTM Model\n",
        "   - 2.3. Training the LSTM Model\n",
        "   - 2.4. Model Evaluation\n",
        "\n",
        "3. **[Part 3: Word Embedding Add-Ons with Word2Vec](#part3)**\n",
        "   - 3.1. Loading Pre-trained Word2Vec Embeddings\n",
        "   - 3.2. Integrating Word2Vec into LSTM Model\n",
        "   - 3.3. Training and Evaluating the Model\n",
        "\n",
        "4. **[Part 4: Model Explainability (LIME / SHAP)](#part4)**\n",
        "   - 4.1. Why Explainability Matters  \n",
        "   - 4.2. Applying LIME to the TF-IDF Model  \n",
        "   - 4.3. Comparing Explanation for LSTM with Word2Vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab7s_m2RvmAh"
      },
      "source": [
        "## Part 0: Metrics Functions to Consider\n",
        "\n",
        "Before diving into the model building and training, it's crucial to establish the metrics we'll use to evaluate our models. In this part, we will define and discuss the different metrics functions that are commonly used in NLP tasks, particularly for text classification:\n",
        "\n",
        "1. **Accuracy**: Measures the proportion of correct predictions among the total number of cases examined. It's a straightforward metric but can be misleading if the classes are imbalanced.\n",
        "\n",
        "2. **Precision and Recall**: Precision measures the proportion of positive identifications that were actually correct, while recall measures the proportion of actual positives that were identified correctly. These metrics are especially important when dealing with imbalanced datasets.\n",
        "\n",
        "3. **F1 Score**: The harmonic mean of precision and recall. It's a good way to show that a classifer has a good balance between precision and recall.\n",
        "\n",
        "4. **Confusion Matrix**: A table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
        "\n",
        "5. **ROC and AUC**: The receiver operating characteristic curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system. The area under the curve (AUC) represents measure of separability.\n",
        "\n",
        "We will implement these metrics functions using libraries such as scikit-learn, and they will be used to assess and compare the performance of our different models throughout this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vFvU7C3OwKA6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def run(self, y_true, y_pred, method_name, average='macro'):\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average=average)\n",
        "        recall = recall_score(y_true, y_pred, average=average)\n",
        "        f1 = f1_score(y_true, y_pred, average=average)\n",
        "\n",
        "        # Store results\n",
        "        self.results[method_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "        }\n",
        "\n",
        "    def plot(self):\n",
        "        # Create subplots\n",
        "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Plot each metric\n",
        "        for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n",
        "            ax = axs[i//2, i%2]\n",
        "            values = [res[metric] * 100 for res in self.results.values()]\n",
        "            ax.bar(self.results.keys(), values)\n",
        "            ax.set_title(metric)\n",
        "            ax.set_ylim(0, 100)\n",
        "\n",
        "            # Add values on the bars\n",
        "            for j, v in enumerate(values):\n",
        "                ax.text(j, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7FZ8v7quizs"
      },
      "source": [
        "## Part 1: Baseline Pipeline with TF-IDF and Linear Model\n",
        "\n",
        "In this part, we will create a baseline model for text classification. This involves:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPqforMD2v6U"
      },
      "source": [
        "###Â 1. **Loading and Exploring Data**:\n",
        "\n",
        "We will load the AG News corpus and perform necessary preprocessing steps like exploring the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459,
          "referenced_widgets": [
            "a22b7df70c344a0ebf09ee0007b87073",
            "8bd2302b39ac4570ad55e16ca0d6f671",
            "bd11478bf72a4f019f43cfa8dbc1e783",
            "1bdc08e69e4042f18ba93227baa32ac1",
            "2a525e1ad5c44034a68af8927908eb4a",
            "2b7edca124334cf4972038dcf1c02d52",
            "7469455593294f079cd3ebe84ee307f5",
            "923edb3530a54421a5b9d495f7365376",
            "7a94329efae34f1cb787c6b9fa216fd7",
            "e6af5ccf347244c9b50208a3b63a70dd",
            "9c7f258dde654c19899836832eebeb1b",
            "12d2e29130bb437789a541635ec79c94",
            "2b4729cde3c64ff2adc42f5a755a909c",
            "aea6809cb472452289ba2ad2d23d81dd",
            "146775826d8b4cfdb266cb6508895338",
            "af78846a6c5d474596a4ca183fa91a37",
            "430570f44f6d4b908721e9cdec55db65",
            "f834ec3ced2b4e3ea429003d436dc691",
            "a6c1b9106d27445facffcd7fae940b5c",
            "23b671b0254d48f0af8403f2bc157e89",
            "61e16de06a3b4085b91bc9b26d2cf8d1",
            "8c67c5dfe148433595ac047873e61b7c",
            "56cf96a858814490b6a0e71bc8950221",
            "b49c9b2cf95a4f36b04b51dab225b7d4",
            "1d261de3cff34b7fbb079c9afc5809d2",
            "786cf4c1e6604d289277334cfa710350",
            "faf57723768b48b99a2ff547099e5a52",
            "b07e6e23f24c49c68f724d3dbbb65df6",
            "ceb1b84013b54e4ea46e6eb99c6a6e52",
            "4298532691da464f8fcf85f6aff78ae3",
            "438a293d56344bbda0c123e95eedfbd5",
            "cca526a8d17c4d64946af5fa8468ed11",
            "ea6bc7e91240460e9e366921fc4a68f8",
            "8f23a9383aa940debdf5045f06480cbe",
            "996e38a0cdce474cad7154102c6241c4",
            "c439b1ea83fd4d16a9974a243fb76aa3",
            "5cd2c79a4602466785f8cd5a5fc35e23",
            "12ac756a0257424cbe3f24fbf5acf9eb",
            "a159206133e94744ac85c73a03c21797",
            "74da8262aff642d686a404a95ef9a2f1",
            "43988dc7dcd745a89e73aa0ad0ea3833",
            "415f34bad60c465eb8055b56b361158c",
            "f7687706375441b49b5286f60183efa1",
            "1a9467e3b6a24c63b4f52f336e6fca38"
          ]
        },
        "id": "KrJTmiYZzaR9",
        "outputId": "57b20501-c42a-4f3f-a9a8-82813efdcb9b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the 'ag_news' dataset\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# Explore the structure of the dataset\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzVwWxcjzgNy"
      },
      "source": [
        "Let's create stratified samples for training and validation sets ensuring that each class is represented in proportion to its frequency. It will go faster with just a sample, and we will be able to make tests on validation test before trying to work on the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qrRvFzrGzdAx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = dataset['train']['text']\n",
        "labels = dataset['train']['label']\n",
        "\n",
        "test_data = dataset['test']['text']\n",
        "test_labels = dataset['test']['label']\n",
        "\n",
        "# Stratified split to create a smaller training and validation set\n",
        "train_data, valid_data, train_labels, valid_labels = train_test_split(\n",
        "    data, labels, stratify=labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Further split to get 10k and 2k samples respectively\n",
        "train_data, _, train_labels, _ = train_test_split(\n",
        "    train_data, train_labels, stratify=train_labels, train_size=10000, random_state=42\n",
        ")\n",
        "valid_data, _, valid_labels, _ = train_test_split(\n",
        "    valid_data, valid_labels, stratify=valid_labels, train_size=2000, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "HXWQzYN91YDA",
        "outputId": "657b5ada-0523-40f0-bb7f-9eb1bb8d5d11"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "labels = {0: 'World', 1: 'Sports',\n",
        "          2: 'Business', 3: 'Sci/Tech'}\n",
        "\n",
        "# Prepare data for wordclouds\n",
        "label_data = defaultdict(lambda: '')\n",
        "\n",
        "for text, label in zip(train_data, train_labels):\n",
        "    label_data[label] += text\n",
        "\n",
        "# Generate and plot wordclouds for each label\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 6))  # Create 2x2 subplots\n",
        "axs = axs.flatten()  # Flatten the axis array\n",
        "\n",
        "for ax, (label, text) in zip(axs, label_data.items()):\n",
        "    wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(text)\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.set_title('WordCloud for Label {}'.format(labels.get(label)))\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "bTITVZia2aTG",
        "outputId": "cd3d704a-50aa-4ff7-b4bd-c4157edf1d24"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the frequency of each label\n",
        "label_counts = Counter(train_labels)\n",
        "\n",
        "# Data to plot\n",
        "_labels = [labels.get(lab) for lab in label_counts.keys()]\n",
        "sizes = label_counts.values()\n",
        "colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.pie(sizes, labels=_labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title('Proportion of Each Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJl2q_rlvZGK"
      },
      "source": [
        "### 2. **Feature Extraction with TF-IDF**:\n",
        "\n",
        "We will convert the text data into numerical form using the TF-IDF vectorization technique. We will use the `Pipeline` class from scikit-learn which is really practical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdthwJiTueEE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a pipeline with TF-IDF and Logistic Regression\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2),\n",
        "                              min_df=5,\n",
        "                              stop_words='english')),\n",
        "    ('clf', LogisticRegression(solver='liblinear')),\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(train_data, train_labels)\n",
        "\n",
        "valid_preds = pipeline.predict(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BFknIrrF4TW9"
      },
      "outputs": [],
      "source": [
        "metrics_val= Metrics()\n",
        "metrics_val.run(valid_labels, valid_preds, \"basic TF-IDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEaSlme-vagC"
      },
      "source": [
        "### 3. **Training with Cross Validation**:\n",
        "\n",
        "We will train a linear classifier (such as Logistic Regression) using the extracted features, `Pipeline` module and cross validation with `GridSearchCV`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2R52Bt5vcmB",
        "outputId": "a7a4aa67-0329-46c2-bf49-3e131fe4aea4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'tfidf__min_df': [1, 2, 5],  # Example values, you can choose others\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2)]  # Unigrams, bigrams or both\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(train_data, train_labels)\n",
        "\n",
        "# Best parameters found by grid search\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "\n",
        "# Evaluate on the validation set\n",
        "valid_preds = grid_search.predict(valid_data)\n",
        "metrics_val.run(valid_labels, valid_preds, \"CV-ed TF-IDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwwFma1dvcsd"
      },
      "source": [
        "### 4. **Model Evaluation**:\n",
        "\n",
        "We will evaluate the performance of our model on a separate test set using various metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "nAxJ_28MvanR",
        "outputId": "25261fcb-dc1a-496d-86ef-e8d52fbb4753"
      },
      "outputs": [],
      "source": [
        "metrics_val.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4eZdme7vokh"
      },
      "source": [
        "## Part 2: LSTM Pipeline with One-Hot Encoding\n",
        "\n",
        "In this part, we'll explore a more complex model using LSTM:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yndnfwXa8qQC"
      },
      "source": [
        "### 1. **Preprocessing for LSTM**:\n",
        "\n",
        "We'll prepare the text data for LSTM, which involves tokenization and converting words to one-hot encoded vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tHsnsGm48qc9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 5000  # This is a hyperparameter, adjust as needed\n",
        "max_length = 128    # This is another hyperparameter\n",
        "\n",
        "# Initialize and fit the tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "# Convert texts to sequences of integers\n",
        "sequences_train = tokenizer.texts_to_sequences(train_data)\n",
        "sequences_valid = tokenizer.texts_to_sequences(valid_data)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length,\n",
        "                                 padding='post', truncating='post')\n",
        "padded_sequences_valid = pad_sequences(sequences_valid, maxlen=max_length,\n",
        "                                 padding='post', truncating='post')\n",
        "\n",
        "\n",
        "# Assuming train_labels are integer labels\n",
        "num_classes = len(set(train_labels))  # Determine the number of unique classes\n",
        "\n",
        "# Convert labels to one-hot vectors\n",
        "train_labels_lstm = to_categorical(train_labels, num_classes=num_classes)\n",
        "valid_labels_lstm = to_categorical(valid_labels, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-l49eA8qp-"
      },
      "source": [
        "### 2. **Building a Bidirectional LSTM Model**:\n",
        "\n",
        "We'll design a neural network with a Bidirectional LSTM layer to capture context from both directions in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27WvWMEo8q2-",
        "outputId": "1f14bab3-6305-414c-d5a6-df624133c8c2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, output_dim=64, input_length=max_length),\n",
        "    Bidirectional(LSTM(64)),  # First bidirectional LSTM layer\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvxIuYkb8rDo"
      },
      "source": [
        "### 3. **Training the LSTM Model**:\n",
        "\n",
        "We'll train our LSTM model on the preprocessed text data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BTowIVP8rRL",
        "outputId": "f0c25cdf-2da9-4139-881c-a37e35376992"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    padded_sequences_train,\n",
        "    train_labels_lstm,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_data=(padded_sequences_valid, valid_labels_lstm)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jo6_aZh81lt"
      },
      "source": [
        "### 4. **Model Evaluation**:\n",
        "\n",
        "*Similar* to Part 1, we will evaluate our model's performance using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30qA_9FCtntO",
        "outputId": "77462ad4-5a62-4c36-b37c-d68699277f51"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(padded_sequences_valid)\n",
        "valid_preds = np.argmax(predictions, axis=1)\n",
        "metrics_val.run(valid_labels, valid_preds, \"BiLSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "GauRhtEfE5Vw",
        "outputId": "91ae719d-5a4c-4307-aae7-43708a041e28"
      },
      "outputs": [],
      "source": [
        "metrics_val.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pECtNMJ5vpoh"
      },
      "source": [
        "## Part 3: Word Embedding Add-Ons with Word2Vec\n",
        "\n",
        "This part focuses on integrating pre-trained word embeddings into our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CStwSkObFFne"
      },
      "source": [
        "## 1. **Loading Pre-trained Word2Vec Embeddings**:\n",
        "\n",
        "We'll load Word2Vec embeddings pre-trained on a large corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGMnHbICFCMA",
        "outputId": "b53cdbd5-b228-4815-fcca-b9b574fc05c8"
      },
      "outputs": [],
      "source": [
        "from staticvectors import StaticVectors\n",
        "\n",
        "word2vec_model = StaticVectors(\"neuml/word2vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJZNdvmpFCeS"
      },
      "source": [
        "## 2. **Integrating Word2Vec into LSTM Model**:\n",
        "We'll use these embeddings as inputs to our LSTM model, potentially enhancing its ability to understand context and semantics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjOCkI44FCxc",
        "outputId": "d13266e5-3642-402b-cf66-d4d567dc43c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, 300))  # 300 is the dimensionality of GoogleNews vectors\n",
        "\n",
        "for word, i in tqdm(tokenizer.word_index.items()):\n",
        "    if i < vocab_size:\n",
        "        try:\n",
        "            embedding_vector = word2vec_model.embeddings([word])\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            # Word not found in the model, leave as zeros\n",
        "            continue\n",
        "# Define the model\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 300, weights=[embedding_matrix],\n",
        "              input_length=max_length, trainable=False),  # Set trainable to False\n",
        "    Bidirectional(LSTM(64)),  # First bidirectional LSTM layer\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7ioVYxFDC1"
      },
      "source": [
        "## 3. **Training and Evaluating the Model**:\n",
        "We'll train our model with these new embeddings and evaluate to see if there's an improvement in performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "a96kqkopFDUX",
        "outputId": "15c7a2d2-2dd8-4e2f-aa04-93a1da736d48"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Setup early stopping to stop training when validation loss stops improving\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=5,  # How many epochs to wait after min has been hit\n",
        "    verbose=1,  # Verbosity level\n",
        "    mode='min',  # Mode for the monitored quantity (minimizing loss)\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    padded_sequences_train,\n",
        "    train_labels_lstm,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_data=(padded_sequences_valid, valid_labels_lstm),\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHa1yN24GWHU"
      },
      "source": [
        "## 4. **Model Evaluation**:\n",
        "\n",
        "Similar to previous parts, we will evaluate our model's performance using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p2xTLGFwvs8a",
        "outputId": "88b06616-4fbc-42bd-ca1a-f7e8053d45df"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(padded_sequences_valid)\n",
        "valid_preds = np.argmax(predictions, axis=1)\n",
        "metrics_val.run(valid_labels, valid_preds, \"BiLSTM + W2V\")\n",
        "metrics_val.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xASMzlbvtgF"
      },
      "source": [
        "## Part 4: Model Explainability (LIME / SHAP)\n",
        "\n",
        "As machine learning models become more powerful, they also become more complex and opaque â especially deep learning models like LSTMs or transformer-based architectures. This complexity makes it harder to understand *why* a model makes a specific prediction.\n",
        "\n",
        "In this part, we will explore **model explainability**, a crucial step in building trustworthy and transparent machine learning systems.\n",
        "\n",
        "### 4.1 Why Explainability Matters\n",
        "\n",
        "Imagine youâve built a model that predicts whether a customer is likely to churn, or whether a loan should be approved. Even if your model is 90% accurate, you might be asked:\n",
        "\n",
        "> ð¤ \"But why did the model make that decision?\"\n",
        "\n",
        "This is where **explainability** becomes essential.\n",
        "\n",
        "#### Why it's important:\n",
        "\n",
        "- **Trust**: Users are more likely to trust predictions they can understand.\n",
        "- **Debugging**: Helps identify spurious correlations or biases in the model.\n",
        "- **Fairness & Ethics**: Ensures decisions are not based on sensitive or discriminatory attributes.\n",
        "- **Regulatory Compliance**: In some domains (like finance or healthcare), explainability is required by law.\n",
        "\n",
        "#### Two Main Categories of Explainability:\n",
        "\n",
        "1. **Global Explanations**: Understanding the overall model behavior  \n",
        "   _Example: Which words generally influence sentiment predictions the most?_\n",
        "\n",
        "2. **Local Explanations**: Understanding individual predictions  \n",
        "   _Example: Why was *this* review classified as negative?_\n",
        "\n",
        "We'll focus primarily on **local explanations** using:\n",
        "- `LIME` (for simpler models like TF-IDF + Logistic Regression)\n",
        "\n",
        "Letâs begin by applying LIME to our TF-IDF baseline model.\n",
        "\n",
        "### 4.2 ð§ª Applying LIME to the TF-IDF Model\n",
        "\n",
        "Now that we understand **why explainability matters**, let's start by applying **LIME** (Local Interpretable Model-agnostic Explanations) to our first model:  \n",
        "â¡ï¸ A **TF-IDF + Logistic Regression** pipeline.\n",
        "\n",
        "LIME works by slightly perturbing the input text and seeing how the model prediction changes.  \n",
        "From this, it builds a **local, interpretable surrogate model** (like a linear regression) to approximate the complex model's behavior near that input.\n",
        "\n",
        "We'll explain:\n",
        "- A single prediction for a text sample\n",
        "- Which words had the most impact (positive or negative) on the predicted label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search.best_estimator_.steps[1][1].coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Fixed version of the TF-IDF LIME explainer function\n",
        "def explain_tfidf_prediction(text_instance, pipeline, class_names):\n",
        "    # Create a LIME text explainer\n",
        "    explainer = LimeTextExplainer(class_names=class_names)\n",
        "    \n",
        "    # Get explanation for the prediction\n",
        "    exp = explainer.explain_instance(\n",
        "        text_instance, \n",
        "        pipeline.predict_proba, \n",
        "        num_features=10,\n",
        "        top_labels=len(class_names)  # Explain all classes\n",
        "    )\n",
        "    \n",
        "    # Display basic information\n",
        "    print(f\"Text: {text_instance}\")\n",
        "    pred_class = pipeline.predict([text_instance])[0]\n",
        "    print(f\"Predicted class: {class_names[pred_class]}\")\n",
        "    \n",
        "    # Get probabilities for all classes\n",
        "    probs = pipeline.predict_proba([text_instance])[0]\n",
        "    print(\"\\nClass probabilities:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name}: {probs[i]:.4f}\")\n",
        "    \n",
        "    # Create visualization for each class\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    \n",
        "    # Get the labels that LIME actually explained\n",
        "    top_labels = exp.available_labels()\n",
        "    \n",
        "    for i, label_id in enumerate(top_labels):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        \n",
        "        # Get the explanation for this class\n",
        "        exp_list = exp.as_list(label=label_id)\n",
        "        \n",
        "        # Extract words and weights\n",
        "        words = [x[0] for x in exp_list]\n",
        "        weights = [x[1] for x in exp_list]\n",
        "        \n",
        "        # Sort for better visualization\n",
        "        pairs = sorted(zip(words, weights), key=lambda x: x[1])\n",
        "        words = [x[0] for x in pairs]\n",
        "        weights = [x[1] for x in pairs]\n",
        "        \n",
        "        # Create bar chart\n",
        "        colors = ['red' if w < 0 else 'green' for w in weights]\n",
        "        y_pos = np.arange(len(words))\n",
        "        \n",
        "        plt.barh(y_pos, weights, color=colors)\n",
        "        plt.yticks(y_pos, words)\n",
        "        plt.title(f\"Explanation for class: {class_names[label_id]}\")\n",
        "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print top contributing words for each class\n",
        "    for label_id in top_labels:\n",
        "        print(f\"\\nTop features for class: {class_names[label_id]}\")\n",
        "        exp_list = exp.as_list(label=label_id)\n",
        "        for word, weight in exp_list:\n",
        "            print(f\"{word}: {weight:.4f}\")\n",
        "    \n",
        "    return exp\n",
        "\n",
        "# Example text from test set\n",
        "example_text = test_data[0]\n",
        "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "lime_exp_tfidf = explain_tfidf_prediction(example_text, grid_search.best_estimator_, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ð§  Interpretation of LIME Explanation\n",
        "\n",
        "Let's break down what LIME revealed about the model's reasoning for this particular prediction.\n",
        "\n",
        "#### â Predicted Class: **Business**\n",
        "\n",
        "LIME shows us the **top 10 words** that contributed **positively** or **negatively** to each possible class (Business, World, Sci/Tech, Sports).\n",
        "\n",
        "#### ð¬ For Class: **Business**\n",
        "- Words like **\"Federal\"**, **\"firm\"**, and **\"pension\"** have **positive weights**, meaning they support the Business prediction.\n",
        "- The word **\"talks\"** actually detracts from the Business prediction (negative weight), suggesting it's a bit ambiguous.\n",
        "\n",
        "#### ð For Class: **World**\n",
        "- Interestingly, **\"talks\"** strongly supports the **World** class here.\n",
        "- Other Business-related terms (e.g., **\"Federal\"**, **\"firm\"**) detract from a World prediction.\n",
        "\n",
        "#### ð¡ Insights:\n",
        "- Words like **\"workers\"**, **\"Unions\"**, and **\"say\"** appear across multiple classes with small influence, showing theyâre more generic.\n",
        "- **\"talks\"** is context-dependent â LIME helps us **disentangle how the same word can shift meaning** depending on the rest of the sentence.\n",
        "\n",
        "ð§­ **Takeaway**: LIME helps us peek inside the model's black box and see *which features are driving predictions*. It also shows that certain words may support multiple classes, but with different intensities.\n",
        "\n",
        "---\n",
        "\n",
        "Ready to move on? Letâs now explore:\n",
        "\n",
        "### 4.3 ð Comparing Explanations for LSTM and Word2Vec Models\n",
        "\n",
        "â¡ï¸ Here, we'll try to interpret more **complex models** (like LSTM and Word2Vec-based models) using LIME and compare how their reasoning differs from the simpler TF-IDF model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_text_for_lstm(text, tokenizer, max_length):\n",
        "    \"\"\"Prepare text input for LSTM model\"\"\"\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded_seq = pad_sequences(sequences, maxlen=max_length)\n",
        "    return padded_seq\n",
        "\n",
        "def lstm_predict_proba(texts):\n",
        "    \"\"\"Prediction function for LIME to use with LSTM model\"\"\"\n",
        "    result = np.zeros((len(texts), len(class_names)))\n",
        "    for i, text in enumerate(texts):\n",
        "        padded = prepare_text_for_lstm(text, tokenizer, max_length)\n",
        "        pred = model.predict(padded, verbose=0)\n",
        "        result[i] = pred[0]\n",
        "    return result\n",
        "\n",
        "def explain_lstm_prediction(text_instance, class_names):\n",
        "    # Create a LIME text explainer\n",
        "    explainer = LimeTextExplainer(class_names=class_names)\n",
        "    \n",
        "    # Get explanation for the prediction\n",
        "    exp = explainer.explain_instance(\n",
        "        text_instance, \n",
        "        lstm_predict_proba, \n",
        "        num_features=10,\n",
        "        top_labels=len(class_names)  # Explain all classes\n",
        "    )\n",
        "    \n",
        "    # Display basic information\n",
        "    print(f\"Text: {text_instance}\")\n",
        "    padded = prepare_text_for_lstm(text_instance, tokenizer, max_length)\n",
        "    prediction = model.predict(padded, verbose=0)\n",
        "    predicted_class = np.argmax(prediction[0])\n",
        "    print(f\"Predicted class: {class_names[predicted_class]}\")\n",
        "    \n",
        "    # Get probabilities for all classes\n",
        "    probs = prediction[0]\n",
        "    print(\"\\nClass probabilities:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name}: {probs[i]:.4f}\")\n",
        "    \n",
        "    # Create visualization for each class\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    \n",
        "    # Get the labels that LIME actually explained\n",
        "    top_labels = exp.available_labels()\n",
        "    \n",
        "    for i, label_id in enumerate(top_labels):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        \n",
        "        # Get the explanation for this class\n",
        "        exp_list = exp.as_list(label=label_id)\n",
        "        \n",
        "        # Extract words and weights\n",
        "        words = [x[0] for x in exp_list]\n",
        "        weights = [x[1] for x in exp_list]\n",
        "        \n",
        "        # Sort for better visualization\n",
        "        pairs = sorted(zip(words, weights), key=lambda x: x[1])\n",
        "        words = [x[0] for x in pairs]\n",
        "        weights = [x[1] for x in pairs]\n",
        "        \n",
        "        # Create bar chart\n",
        "        colors = ['red' if w < 0 else 'green' for w in weights]\n",
        "        y_pos = np.arange(len(words))\n",
        "        \n",
        "        plt.barh(y_pos, weights, color=colors)\n",
        "        plt.yticks(y_pos, words)\n",
        "        plt.title(f\"Explanation for class: {class_names[label_id]}\")\n",
        "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print top contributing words for each class\n",
        "    for label_id in top_labels:\n",
        "        print(f\"\\nTop features for class: {class_names[label_id]}\")\n",
        "        exp_list = exp.as_list(label=label_id)\n",
        "        for word, weight in exp_list:\n",
        "            print(f\"{word}: {weight:.4f}\")\n",
        "    \n",
        "    return exp\n",
        "\n",
        "example_text = test_data[0]\n",
        "model_exp_lime = explain_lstm_prediction(example_text, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ð LIME Explanation for LSTM Model\n",
        "\n",
        "#### ð§  Sentence Analyzed:\n",
        "> \"Fears for T N pension after talks Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"\n",
        "\n",
        "#### â Predicted Class: **Sci/Tech**  \n",
        "With a **very high probability (78.35%)**, the LSTM model predicted this text belongs to **Sci/Tech**.\n",
        "\n",
        "#### ð Class Probabilities:\n",
        "- Sci/Tech: **0.7835**\n",
        "- World: 0.0995\n",
        "- Business: 0.0839\n",
        "- Sports: 0.0331\n",
        "\n",
        "---\n",
        "\n",
        "### ð¡ What LIME Reveals\n",
        "\n",
        "#### Top Features Supporting **Sci/Tech**:\n",
        "- Common connectors like **\"they\"**, **\"after\"**, **\"are\"**, **\"with\"**, and **\"at\"** surprisingly contribute **positively** to Sci/Tech.\n",
        "- Words like **\"firm\"** and **\"talks\"**, which might intuitively relate to Business or World, actually **reduce** the Sci/Tech probability here.\n",
        "\n",
        "#### Across Other Classes:\n",
        "- Most of the same function words (**\"they\"**, **\"after\"**, **\"with\"**) appear as **negative** contributors to **World**, **Business**, and **Sports** classes.\n",
        "- Words like **\"firm\"** slightly **boost** the Business class but are downplayed for Sci/Tech.\n",
        "- In the **Sports** class, all top words negatively influence the prediction, confirming itâs a poor match.\n",
        "\n",
        "---\n",
        "\n",
        "### ð§  Interpretation:\n",
        "Unlike the TF-IDF model, which focused on *specific content words*, the LSTM model seems to be relying heavily on **syntactic or structural features** (like function words and word order). This could be due to:\n",
        "- The sequential nature of LSTM, which captures **contextual dependencies**\n",
        "- A possible **lack of domain-specific keywords** driving this prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FJZNdvmpFCeS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bse-nlp-DetGwK6_-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
