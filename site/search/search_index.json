{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Advanced NLP Classes","text":"<p>Welcome to my Advanced NLP Classes website. Here, you'll find all the information you need to follow the course, including lecture notes, slides, resources, and home assignments. I designed this course to guide you through both traditional NLP methods and modern deep learning approaches, ensuring you gain a well-rounded understanding of how to process and analyze natural language data.</p> <p>Below, I've provided a short overview of each main section of the course. Even if you're new to NLP or machine learning, don't worry\u2014this course begins with foundational concepts and gradually works toward more advanced topics.</p>"},{"location":"#lecturer","title":"\ud83d\udc68\u200d\ud83c\udfeb Lecturer","text":"<p>Arnault Gombert</p> <p>I\u2019m your lecturer for this course, and I\u2019ll be here to guide you through each topic step by step. Feel free to reach out if you have any questions or need additional support. My goal is to help you not only understand the theory but also gain hands-on practice in building NLP solutions.</p>"},{"location":"#course-materials","title":"\ud83d\udcda Course Materials","text":"<p>You can access all the key materials here on this site. In particular, you\u2019ll find:</p> <ul> <li>Slides for each lecture session, which you can use as a reference or to review later.</li> <li>Notebooks that contain hands-on exercises and examples.</li> <li>Resources linking to external articles, videos, or blogs that can expand your learning.</li> <li>Home Assignments and instructions for the Final Project.</li> </ul> <p>I\u2019ll keep this repository updated with the latest materials, so please check back regularly.</p>"},{"location":"#github-repository","title":"\ud83d\udc31 GitHub repository","text":"<p>You can find the GitHub repository here with all the notebooks and resources. If you find any bug or have any suggestion, feel free to open an issue.</p>"},{"location":"#i-about-this-course","title":"\u2139\ufe0f About This Course","text":"<p>This course navigates the evolution of Natural Language Processing (NLP) from foundational techniques to advanced concepts like Large Language Models and ChatGPT. It begins with core principles such as TF-IDF and word embeddings, advancing through deep learning innovations like LSTM and BERT.</p> <p>The course is structured into three main parts:</p> <ol> <li>Good old fashioned NLP (Sessions 1-4)</li> <li>Almost part of good old fashioned NLP (Sessions 5-8)</li> <li>LLMs, Agents &amp; Others (Sessions 9 &amp; 10)</li> </ol>"},{"location":"#getting-started","title":"\ud83d\udd30 Getting Started","text":"<ul> <li>All chapter slides will be available in the corresponding chapter. Here is the link to the first chapter.</li> <li>Check the Notebooks section for practical exercises and assignments.</li> <li>Explore the Resources section for recommended readings and materials.</li> </ul>"},{"location":"#pre-requisites","title":"\u2705 Pre-requisites","text":"<p>To get the most out of this course, you should have a good understanding of:</p> <ul> <li>Python programming (You can check the Python 1o1 notebooks starting here)</li> <li>Econometrics</li> <li>Machine learning fundamentals</li> </ul>"},{"location":"#overview-and-objectives","title":"\ud83c\udfaf Overview and objectives","text":"<p>The class is quite complex, maybe one of the densest class you'll have this year as it covers a lot of topics in a short time. We will study general concepts of NLP (Natural Language Processing) sometimes assuming you are already familiar with Machine Learning concepts. If you have general question about ML don't hesitate to ask me during the class or by email. One very good book to learn Machine Learning concepts is Elements of Statistical Learning.</p> <p>I encourage you to read the notebooks and try to run them to understand the main concepts. Even if we focus on NLP techniques, it also includes general ML concepts such as how to improve your model, how to evaluate it, how to deal with overfitting, etc.</p> <p>Text is complex data which is available in abundance on the web, inside firms and public organizations. This course navigates the evolution of NLP from foundational techniques to advanced concepts like Large Language Models and ChatGPT.</p> <p>It begins with core principles such as TF-IDF and word embeddings, advancing through deep learning innovations like LSTM and BERT. It emphasizes practical application, allowing you to build and evaluate NLP pipelines.</p> <p>Key topics include transformers, few-shot and transfer learning, and ethical considerations in NLP. The course culminates in exploring cutting-edge developments, offering hands-on experience with modern NLP challenges. Its goal is to equip students with the skills to analyze and apply NLP technologies effectively and ethically in real-world scenarios.</p>"},{"location":"#course-outline","title":"\ud83d\udccb Course outline","text":""},{"location":"#1-part-1-good-old-fashioned-nlp-sessions-1-4","title":"1\ufe0f\u20e3 Part 1: Good old fashioned NLP (Sessions 1-4)","text":""},{"location":"#baselines-and-sparse-representations-session-1","title":"\ud83d\udd39 Baselines and Sparse representations (Session 1)","text":"<ul> <li>Baseline &amp; Evaluations</li> <li>TF-IDF and improvements</li> </ul>"},{"location":"#2015-deep-learning-session-2","title":"\ud83d\udd39 2015 Deep learning (Session 2)","text":"<ul> <li>Backpropagation in Neural Network</li> <li>LSTM, attention processes &amp; Language Models</li> </ul>"},{"location":"#word-embeddings-sessions-3","title":"\ud83d\udd39 Word Embeddings (Sessions 3)","text":"<ul> <li>Static word embedding (Word2Vec, GloVe, FastText\u2026)</li> <li>Contextual embeddings (ELMo, BERT\u2026)</li> </ul>"},{"location":"#practical-session-session-4","title":"\ud83d\udd39 Practical Session (Session 4)","text":"<ul> <li>Baseline pipeline &amp; Metrics evaluation</li> <li>LSTM-pipeline</li> <li>Word embedding add-ons</li> <li>Training our own embeddings</li> </ul>"},{"location":"#2-part-2-almost-part-of-good-old-fashioned-nlp-sessions-5-8","title":"2\ufe0f\u20e3 Part 2: Almost part of good old fashioned NLP (Sessions 5-8)","text":""},{"location":"#transformers-bert-session-5","title":"\ud83d\udd39 Transformers &amp; BERT (Session 5)","text":"<ul> <li>Transformer intuition: the architecture, the self-attention layers, comparisons with recurrent neural networks</li> <li>BERT architecture: the revolution of Large Language Models</li> </ul>"},{"location":"#few-shot-learning-transfer-learning-session-6","title":"\ud83d\udd39 Few shot learning, Transfer learning (Session 6)","text":"<ul> <li>Language Models are few shot learners: fine-tuning BERT architecture to transfer learning on downstream task</li> <li>Leveraging existing knowledge: using prompts to generate labels</li> </ul>"},{"location":"#injustice-biases-in-nlp-detecting-and-mitigating-session-7","title":"\ud83d\udd39 Injustice &amp; biases in NLP: detecting and mitigating (Session 7)","text":"<ul> <li>Are Large Language Models stochastic parrots? Are Large Language Models useful for everyone?</li> <li>Detect and mitigate biases of Large Language Models</li> </ul>"},{"location":"#practical-session-session-8","title":"\ud83d\udd39 Practical Session (Session 8)","text":"<ul> <li>Fine-tuning a BERT model</li> <li>How much data to get the best results?</li> <li>Low ressource? No problem</li> <li>Detecting biases</li> </ul>"},{"location":"#3-part-3-llms-agents-others-sessions-9-10","title":"3\ufe0f\u20e3 Part 3: LLMs, Agents &amp; Others (Sessions 9 &amp; 10)","text":""},{"location":"#prompt-engineering-fine-tuning","title":"\ud83d\udd39 Prompt engineering &amp; Fine-tuning","text":"<ul> <li>Zero shot learning, Chain of thoughts and format the outputs</li> <li>Fine-tuning</li> </ul>"},{"location":"#hallucinations-introduction-to-agents","title":"\ud83d\udd39 Hallucinations &amp; Introduction to Agents","text":"<ul> <li>Detect hallucinations</li> <li>Other limitations</li> <li>Introduction to Agentic framework</li> </ul>"},{"location":"#required-activities","title":"\ud83d\udcdd Required activities","text":"<p>Students will be required to conduct a couple of independent homeworks. This will include fine-tuning their own NLP model, assessing some models on some benchmark and proposing improvement.</p> <p>Students will form teams of up to 4 students to develop their own text NLP project in which they will be asked to transfer the knowledge they have gained in class to solve a specific NLP problem.</p>"},{"location":"#evaluation","title":"\ud83d\udcca Evaluation","text":"<ul> <li>Class Participation: 10%</li> <li>Homework: 20%</li> <li>Term project and presentation: 70%</li> </ul>"},{"location":"notebooks/","title":"\ud83d\udcd3 NLP Course Notebooks","text":"<p>This directory contains all the Jupyter notebooks for the Advanced NLP Classes. These notebooks provide hands-on experience with the concepts covered in the lectures.</p>"},{"location":"notebooks/#setup-instructions","title":"\u2699\ufe0f Setup Instructions","text":""},{"location":"notebooks/#prerequisites","title":"\ud83d\udd27 Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>Poetry (for dependency management)</li> </ul>"},{"location":"notebooks/#installation","title":"\ud83d\udce6 Installation","text":"<p>We use Poetry to manage dependencies. Follow these steps to set up your environment:</p>"},{"location":"notebooks/#1-install-poetry","title":"1\ufe0f\u20e3 Install Poetry","text":"<p>macOS / Linux: <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre></p> <p>Windows: <pre><code>(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n</code></pre></p>"},{"location":"notebooks/#2-clone-the-repository-and-install-dependencies","title":"2\ufe0f\u20e3 Clone the repository and install dependencies","text":"<pre><code>git clone https://github.com/agombert/AdvancedNLPClasses.git\ncd AdvancedNLPClasses\npoetry install\n</code></pre>"},{"location":"notebooks/#3-install-additional-dependencies-for-notebooks","title":"3\ufe0f\u20e3 Install additional dependencies for notebooks","text":"<pre><code>poetry add pandas numpy matplotlib scikit-learn spacy jupyter\npoetry run python -m spacy download en_core_web_sm\n</code></pre>"},{"location":"notebooks/#4-launch-jupyter-notebook","title":"4\ufe0f\u20e3 Launch Jupyter Notebook","text":"<pre><code>poetry run jupyter notebook\n</code></pre> <p>Navigate to the <code>notebooks</code> directory to access all the notebooks.</p>"},{"location":"notebooks/#troubleshooting","title":"\ud83d\udee0\ufe0f Troubleshooting","text":"<p>If you encounter issues with the installation:</p> <ul> <li>macOS: You might need to install Xcode command-line tools: <code>xcode-select --install</code></li> <li>Ubuntu: Ensure you have build essentials: <code>sudo apt-get install build-essential</code></li> <li>Windows: Make sure you have the Microsoft C++ Build Tools installed</li> </ul>"},{"location":"notebooks/#table-of-contents","title":"\ud83d\udcd6 Table of Contents","text":""},{"location":"notebooks/#python-fundamentals-session-1","title":"\ud83d\udc0d Python Fundamentals (Session 1)","text":"<p>These notebooks cover the essential Python skills needed for NLP:</p> <ul> <li>Python Types: Understanding Python's type system, from basic to advanced types</li> <li>Python Classes: Object-oriented programming in Python</li> <li>Python Dataframes: Working with pandas for data manipulation</li> <li>Python NumPy: Numerical computing with NumPy</li> <li>Python scikit-learn: Introduction to machine learning with scikit-learn</li> </ul>"},{"location":"notebooks/#nlp-techniques-session-1","title":"\ud83d\udcdd NLP Techniques (Session 1)","text":"<ul> <li>Baseline with regexes and spaCy: Implementing simple but effective baseline approaches</li> <li>TF-IDF: how to judge its quality?: Understanding and implementing TF-IDF</li> <li>BM25: a better TF-IDF, judge through different metrics: Advanced information retrieval techniques</li> </ul>"},{"location":"notebooks/#chapter-2-neural-networks-backpropagation-rnns","title":"\ud83d\udcdd Chapter 2: Neural Networks, Backpropagation &amp; RNNs","text":"<ul> <li>Intro to Neural Nets &amp; Backprop (NumPy): Implementing neural networks with NumPy</li> <li>Simple RNN for Text Generation (Tiny Shakespeare): Generating text with a simple RNN</li> <li>LSTM for Sequence Classification: Building an LSTM for sequence classification</li> </ul>"},{"location":"notebooks/#chapter-3-word-embeddings","title":"\ud83d\udcdd Chapter 3: Word Embeddings","text":"<ul> <li>Word2Vec from Scratch - with negative sampling: Implementing Word2Vec from scratch with negative sampling</li> <li>Embedding Evaluation: Intrinsic and Extrinsic: Evaluating word embeddings using both intrinsic and extrinsic metrics</li> <li>Classification with Embeddings: Using embeddings for classification tasks</li> </ul>"},{"location":"notebooks/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>If you find errors or have suggestions for improving these notebooks, please open an issue or submit a pull request.</p>"},{"location":"notebooks/#license","title":"\ud83d\udcc4 License","text":"<p>These notebooks are provided for educational purposes as part of the Advanced NLP Classes at Barcelona School of Economics.</p>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#recommended-materials","title":"Recommended Materials","text":""},{"location":"resources/#neural-networks-bert-attention-transformers-word-embeddings-llms","title":"Neural Networks, BERT, attention, Transformers, Word Embeddings, LLMs","text":"<ul> <li>Elements of Statistical Learning</li> <li>Van Rijsbergen, C. J. (1979). Information Retrieval (2nd ed.). Butterworth-Heinemann.</li> <li>Wang et al. (2019) GLUE: A Multi-Task Benchmark And Analysis Platform For Natural Language Understanding</li> <li>Hue et al. (2020) X-TREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization</li> <li>Strubell et al. (2019) Energy and Policy Considerations for Deep Learning in NLP</li> <li>Dodge et al. (2022) Measuring the Carbon Intensity of AI in Cloud Instances</li> <li>Sheng et al. (2019) The Woman Worked as a Babysitter: On Biases in Language Generation</li> <li>Gupta et al. (2014) \u200b\u200bImproved pattern learning for bootstrapped entity extraction</li> <li>Dou et al. (2016) Word Alignment by Fine-tuning Embeddings on Parallel Corpora</li> <li>Karpathy, Andrej  (2016) Yes you should understand Backprop</li> <li>Karpathy, Andrej (2015) The Unreasonable Effectiveness of Recurrent Neural Networks</li> <li>Olah, Christopher (2015) Understanding LSTM Networks</li> <li>Olah, Christopher (2016) Attention and Augmented Recurrent Neural Networks</li> <li>Mikolov et al. (2013) Efficient Estimation of Word Representations in Vector Space</li> <li>Pennington et al (2014) GloVe: Global Vectors for Word Representation</li> <li>Bojanowski et al. (2016) Enriching Word Vectors with Subword Information</li> <li>Peters et al., (2018) Deep contextualized word representations</li> <li>Howard &amp; Ruder, (2018) Universal Language Model Fine-tuning for Text Classification</li> <li>Devlin et al., (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li> <li>Alamar, Jay, (2018) The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</li> <li>Vaswani et al., (2017), Attention Is All You Need</li> <li>Uszkoreit, Jakop, (2017) Transformer: A Novel Neural Network Architecture for Language Understanding</li> <li>Alamar, Jay, (2018) The Illustrated Transformer</li> <li>Adaloglou, Nikola, (2020) How Transformers work in deep learning and NLP: an intuitive introduction</li> <li>Liu et al., (2019) RoBERTa: A Robustly Optimized BERT Pretraining Approach</li> <li>Wolf et al. (2019) HuggingFace's Transformers: State-of-the-art Natural Language Processing</li> <li>Sun et al., (2019), How to Fine-Tune BERT for Text Classification?</li> <li>Brown et al., (2020), Language Models are Few-Shot Learners</li> <li>Gao et al., (2020), Making Pre-trained Language Models Better Few-shot Learners</li> <li>Gao, Tianyu, (2021), Prompting: Better Ways of Using Language Models for NLP Tasks</li> <li>Timo Schick and Hinrich Sch\u00fctze (2021). Generating Datasets with Pretrained Language Models.</li> <li>Timo Schick and Hinrich Sch\u00fctze (2021). Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference</li> <li>Bender et al., (2021), On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c</li> <li>Kirk et al., (2021), How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases</li> <li>Timo Schick et al.,  (2021). Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP</li> <li>Le Scao et al. (2022), BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</li> <li>Suau et al., (2022), Self-conditioning Pre-Trained Language Models</li> <li>Ag\u00fcera et al. (2022)  Do Large Language Models Understand Us?</li> <li>Touvron et al. (2023), LLaMA: Open and Efficient Foundation Language Models</li> <li>Manakul et al. (2023) SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</li> <li>Kaswan et al. (2023), The (Ab)Use of Open Source Code to Train Large Language Models</li> <li>Luccioni et al. (2023), Power Hungry Processing: Watts Driving the Cost of AI Deployment?</li> <li>Yao et al. (2023), ReAct: Synergizing reasoning and acting in Language Models</li> <li>Huyen (2025), AI Engineering</li> <li>Warner et al. (2024) Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Fine Tuning and Inference</li> <li>Chen et al. (2024) What is the Role of Small Models in the LLM Era: A Survey</li> <li>Weng (2024)  Extrinsic Hallucinations in LLMs</li> <li>Mitchell (2025), LLMs and World Models</li> <li>Vafa et al. (2024), Evaluating the World Model Implicit in a Generative Model</li> <li>Feng et al. (2024), Were RNNs All We Needed?</li> </ul>"},{"location":"chapter1/","title":"Session 1: Introduction to the class","text":""},{"location":"chapter1/#course-materials","title":"\ud83c\udf93 Course Materials","text":""},{"location":"chapter1/#slides","title":"\ud83d\udcd1 Slides:","text":"<p>Download Session 1 Slides (PDF)</p>"},{"location":"chapter1/#notebooks","title":"\ud83d\udcd3 Notebooks:","text":"<ul> <li>[Python 1o1]:<ul> <li>Python types</li> <li>Python classes</li> <li>Python dataframes</li> <li>Python numpy</li> <li>Python scikit-learn</li> </ul> </li> <li>Baseline with regexes and spaCy</li> <li>TF-IDF: how to judge its quality?</li> <li>BM25: a better TF-IDF, judge through different metrics</li> </ul>"},{"location":"chapter1/#session-1-baselines-and-sparse-representations","title":"\ud83d\ude80 Session 1: Baselines and Sparse Representations","text":"<p>In our first session, I\u2019ll introduce you to baseline approaches\u2014simple yet powerful starting points for many NLP tasks. These baselines serve as reference points, helping you measure whether your more sophisticated models actually bring improvements. We\u2019ll also explore the concept of sparse representations, such as bag-of-words or TF-IDF, which have been fundamental in text analysis for years.</p>"},{"location":"chapter1/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ol> <li>Grasp the challenges in processing natural language data.</li> <li>Understand and create sparse vector representations of text.</li> <li>Explore baseline models for typical NLP tasks.</li> <li>Learn about model evaluation and choosing the right metrics.</li> <li>Gain hands-on practice by building basic NLP pipelines.</li> </ol> <p>Even if you have little background in NLP, don\u2019t worry. This session walks you through each concept step by step and shows you how to practically implement them in Python.</p>"},{"location":"chapter1/#topics-covered","title":"\ud83d\udcda Topics Covered","text":""},{"location":"chapter1/#baseline-evaluations","title":"\ud83d\udcca Baseline &amp; Evaluations","text":"<p>We\u2019ll discuss how to set up clear baselines for NLP tasks\u2014starting with basic data cleaning, tokenization, and simple modeling like bag-of-words classifiers. This section underlines why evaluating your model using metrics such as accuracy, F1-score, or BLEU is crucial in understanding how well the model performs and whether advanced methods are truly an improvement.</p>"},{"location":"chapter1/#tf-idf-and-improvements","title":"\ud83d\udee0\ufe0f TF-IDF and Improvements","text":"<p>I\u2019ll walk you through the TF-IDF (Term Frequency\u2013Inverse Document Frequency) technique, explaining why it\u2019s such a popular step beyond a raw bag-of-words. From there, we\u2019ll dive into tweaks and improvements, including dimensionality reduction techniques, vector space models, and uses in information retrieval and text classification.</p>"},{"location":"chapter1/#bibliography-recommended-reading","title":"\ud83d\udcd6 Bibliography &amp; Recommended Reading","text":"<ul> <li> <p>Elements of Statistical Learning.  - Book   A foundational text on the principles of machine learning. I really encourage you to read, it's the bible of Machine Learning.</p> </li> <li> <p>Zinkevich et al. (2022). \"Google\u2019s Best Practices for ML Engineering.\" - Blog Post   Offers guidelines on how to design, deploy, and maintain machine learning systems effectively.</p> </li> <li> <p>Rudi Seitz (2020). \"Understanding TF-IDF and BM-25.\" - Blog Post   A comprehensive guide to understanding TF-IDF its limitations and why BM-25 is a better alternative.</p> </li> <li> <p>Van Rijsbergen, C. J. (1979). \"Information Retrieval (2nd ed.)\" - Book   A foundational text on the principles of information retrieval systems.</p> </li> <li> <p>Gupta et al. (2014). \"Improved pattern learning for bootstrapped entity extraction.\" - Paper   Discusses pattern-based bootstrapping approaches to entity extraction.</p> </li> <li> <p>Wang et al. (2019). \"GLUE: A Multi-Task Benchmark And Analysis Platform For Natural Language Understanding.\" - Paper | GLUE Benchmark   Proposes a widely adopted multi-task benchmark for evaluating NLP models.</p> </li> <li> <p>Strubell et al. (2019). \"Energy and Policy Considerations for Deep Learning in NLP.\" - Paper   Investigates the environmental impact of large-scale NLP model training.</p> </li> <li> <p>Dodge et al. (2022). \"Software Carbon Intensity (SCI).\" - Paper   A framework for measuring the carbon intensity of software solutions.</p> </li> <li> <p>Sheng et al. (2019). \"The Woman Worked as a Babysitter: On Biases in Language Generation.\" - Paper   Examines bias in language models via prompts and generated outputs.</p> </li> <li> <p>Hu et al. (2020). \"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization.\" - Paper   A benchmark covering a range of cross-lingual transfer tasks.</p> </li> </ul> <p>These readings offer both historical perspectives and modern insights into how NLP has evolved and why these methods work.</p>"},{"location":"chapter1/#practical-components","title":"\ud83d\udcbb Practical Components","text":"<p>Finally, you\u2019ll put theory into practice by:</p> <ul> <li>Building a basic text-processing pipeline (regexes, spaCy, etc.).</li> <li>Implementing your own TF-IDF vectorizer or using existing libraries like <code>scikit-learn</code>.</li> <li>Running a simple text classification experiment on a real dataset.</li> <li>Comparing baseline results with more advanced approaches to see how improvements stack up.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_1/","title":"Python Types","text":"In\u00a0[15]: Copied! <pre># Integers\nage = 25\nprint(f\"age is of type: {type(age)}\")\n\n# Floating-point numbers\nheight = 1.75\nprint(f\"height is of type: {type(height)}\")\n\n# Booleans\nis_student = True\nprint(f\"is_student is of type: {type(is_student)}\")\n\n# Strings\nname = \"Alice\"\nprint(f\"name is of type: {type(name)}\")\n</pre> # Integers age = 25 print(f\"age is of type: {type(age)}\")  # Floating-point numbers height = 1.75 print(f\"height is of type: {type(height)}\")  # Booleans is_student = True print(f\"is_student is of type: {type(is_student)}\")  # Strings name = \"Alice\" print(f\"name is of type: {type(name)}\") <pre>age is of type: &lt;class 'int'&gt;\nheight is of type: &lt;class 'float'&gt;\nis_student is of type: &lt;class 'bool'&gt;\nname is of type: &lt;class 'str'&gt;\n</pre> In\u00a0[16]: Copied! <pre># Lists - ordered, mutable collections\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(f\"fruits is of type: {type(fruits)}\")\n\n# Tuples - ordered, immutable collections\ncoordinates = (10.5, 20.8)\nprint(f\"coordinates is of type: {type(coordinates)}\")\n\n# Dictionaries - key-value pairs\nperson = {\"name\": \"Bob\", \"age\": 30, \"is_student\": False}\nprint(f\"person is of type: {type(person)}\")\n\n# Sets - unordered collections of unique elements\nunique_numbers = {1, 2, 3, 3, 4, 5}  # Note: duplicates are automatically removed\nprint(f\"unique_numbers is of type: {type(unique_numbers)}\")\nprint(f\"unique_numbers contains: {unique_numbers}\")\n</pre> # Lists - ordered, mutable collections fruits = [\"apple\", \"banana\", \"cherry\"] print(f\"fruits is of type: {type(fruits)}\")  # Tuples - ordered, immutable collections coordinates = (10.5, 20.8) print(f\"coordinates is of type: {type(coordinates)}\")  # Dictionaries - key-value pairs person = {\"name\": \"Bob\", \"age\": 30, \"is_student\": False} print(f\"person is of type: {type(person)}\")  # Sets - unordered collections of unique elements unique_numbers = {1, 2, 3, 3, 4, 5}  # Note: duplicates are automatically removed print(f\"unique_numbers is of type: {type(unique_numbers)}\") print(f\"unique_numbers contains: {unique_numbers}\") <pre>fruits is of type: &lt;class 'list'&gt;\ncoordinates is of type: &lt;class 'tuple'&gt;\nperson is of type: &lt;class 'dict'&gt;\nunique_numbers is of type: &lt;class 'set'&gt;\nunique_numbers contains: {1, 2, 3, 4, 5}\n</pre> In\u00a0[17]: Copied! <pre># String to integer\nage_str = \"25\"\nage_int = int(age_str)\nprint(f\"Converted '{age_str}' to {age_int} of type {type(age_int)}\")\n\n# Integer to string\nnum = 42\nnum_str = str(num)\nprint(f\"Converted {num} to '{num_str}' of type {type(num_str)}\")\n\n# String to float\nprice_str = \"19.99\"\nprice_float = float(price_str)\nprint(f\"Converted '{price_str}' to {price_float} of type {type(price_float)}\")\n\n# List to set (removes duplicates)\nnumbers_list = [1, 2, 2, 3, 4, 4, 5]\nnumbers_set = set(numbers_list)\nprint(f\"Converted {numbers_list} to {numbers_set} of type {type(numbers_set)}\")\n</pre> # String to integer age_str = \"25\" age_int = int(age_str) print(f\"Converted '{age_str}' to {age_int} of type {type(age_int)}\")  # Integer to string num = 42 num_str = str(num) print(f\"Converted {num} to '{num_str}' of type {type(num_str)}\")  # String to float price_str = \"19.99\" price_float = float(price_str) print(f\"Converted '{price_str}' to {price_float} of type {type(price_float)}\")  # List to set (removes duplicates) numbers_list = [1, 2, 2, 3, 4, 4, 5] numbers_set = set(numbers_list) print(f\"Converted {numbers_list} to {numbers_set} of type {type(numbers_set)}\") <pre>Converted '25' to 25 of type &lt;class 'int'&gt;\nConverted 42 to '42' of type &lt;class 'str'&gt;\nConverted '19.99' to 19.99 of type &lt;class 'float'&gt;\nConverted [1, 2, 2, 3, 4, 4, 5] to {1, 2, 3, 4, 5} of type &lt;class 'set'&gt;\n</pre> In\u00a0[18]: Copied! <pre>value = 42\n\n# Using type()\nprint(f\"Type of value is: {type(value)}\")\nprint(f\"Is value an int? {type(value) is int}\")\n\n# Using isinstance()\nprint(f\"Is value an int? {isinstance(value, int)}\")\nprint(f\"Is value a float? {isinstance(value, float)}\")\n\n# Check if value is a number (int or float)\nprint(f\"Is value a number? {isinstance(value, (int, float))}\")\n</pre> value = 42  # Using type() print(f\"Type of value is: {type(value)}\") print(f\"Is value an int? {type(value) is int}\")  # Using isinstance() print(f\"Is value an int? {isinstance(value, int)}\") print(f\"Is value a float? {isinstance(value, float)}\")  # Check if value is a number (int or float) print(f\"Is value a number? {isinstance(value, (int, float))}\") <pre>Type of value is: &lt;class 'int'&gt;\nIs value an int? True\nIs value an int? True\nIs value a float? False\nIs value a number? True\n</pre> In\u00a0[19]: Copied! <pre>from typing import List, Dict, Tuple, Set, Optional, Union, Any\n\n# Function with type hints\ndef process_text(text: str, max_length: int = 100) -&gt; str:\n    \"\"\"Process a text string and return a modified version.\"\"\"\n    if len(text) &gt; max_length:\n        return text[:max_length] + \"...\"\n    return text\n\n# Using the function\nresult = process_text(\"This is a sample text for processing.\", 20)\nprint(result)\n</pre> from typing import List, Dict, Tuple, Set, Optional, Union, Any  # Function with type hints def process_text(text: str, max_length: int = 100) -&gt; str:     \"\"\"Process a text string and return a modified version.\"\"\"     if len(text) &gt; max_length:         return text[:max_length] + \"...\"     return text  # Using the function result = process_text(\"This is a sample text for processing.\", 20) print(result) <pre>This is a sample tex...\n</pre> In\u00a0[20]: Copied! <pre># More complex type hints\n\n# A function that takes a list of strings and returns a dictionary\ndef count_word_frequencies(words: List[str]) -&gt; Dict[str, int]:\n    \"\"\"Count the frequency of each word in a list.\"\"\"\n    frequencies: Dict[str, int] = {}\n    for word in words:\n        if word in frequencies:\n            frequencies[word] += 1\n        else:\n            frequencies[word] = 1\n    return frequencies\n\n# Using the function\nwords = [\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\", \"apple\"]\nword_counts = count_word_frequencies(words)\nprint(word_counts)\n</pre> # More complex type hints  # A function that takes a list of strings and returns a dictionary def count_word_frequencies(words: List[str]) -&gt; Dict[str, int]:     \"\"\"Count the frequency of each word in a list.\"\"\"     frequencies: Dict[str, int] = {}     for word in words:         if word in frequencies:             frequencies[word] += 1         else:             frequencies[word] = 1     return frequencies  # Using the function words = [\"apple\", \"banana\", \"apple\", \"cherry\", \"banana\", \"apple\"] word_counts = count_word_frequencies(words) print(word_counts) <pre>{'apple': 3, 'banana': 2, 'cherry': 1}\n</pre> In\u00a0[21]: Copied! <pre>class TextDocument:\n    def __init__(self, title: str, content: str, tags: List[str] = None):\n        self.title = title\n        self.content = content\n        self.tags = tags or []\n        \n    def word_count(self) -&gt; int:\n        \"\"\"Count the number of words in the document.\"\"\"\n        return len(self.content.split())\n    \n    def __str__(self) -&gt; str:\n        return f\"TextDocument(title='{self.title}', words={self.word_count()}, tags={self.tags})\"\n\n# Create a document\ndoc = TextDocument(\n    \"Introduction to NLP\",\n    \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\",\n    [\"NLP\", \"AI\", \"introduction\"]\n)\n\nprint(doc)\nprint(f\"Word count: {doc.word_count()}\")\nprint(f\"Type of doc: {type(doc)}\")\n</pre> class TextDocument:     def __init__(self, title: str, content: str, tags: List[str] = None):         self.title = title         self.content = content         self.tags = tags or []              def word_count(self) -&gt; int:         \"\"\"Count the number of words in the document.\"\"\"         return len(self.content.split())          def __str__(self) -&gt; str:         return f\"TextDocument(title='{self.title}', words={self.word_count()}, tags={self.tags})\"  # Create a document doc = TextDocument(     \"Introduction to NLP\",     \"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\",     [\"NLP\", \"AI\", \"introduction\"] )  print(doc) print(f\"Word count: {doc.word_count()}\") print(f\"Type of doc: {type(doc)}\") <pre>TextDocument(title='Introduction to NLP', words=22, tags=['NLP', 'AI', 'introduction'])\nWord count: 22\nType of doc: &lt;class '__main__.TextDocument'&gt;\n</pre> In\u00a0[22]: Copied! <pre>from typing import NamedTuple, TypedDict\n\n# NamedTuple - an immutable, typed version of a tuple with named fields\nclass Token(NamedTuple):\n    text: str\n    pos_tag: str\n    is_stop: bool\n\n# Create a token\ntoken = Token(text=\"apple\", pos_tag=\"NOUN\", is_stop=False)\nprint(token)\nprint(f\"Token text: {token.text}, POS tag: {token.pos_tag}\")\n\n# TypedDict - a dictionary with a fixed set of keys, each with a specified type\nclass DocumentMetadata(TypedDict):\n    title: str\n    author: str\n    year: int\n    keywords: List[str]\n\n# Create document metadata\nmetadata: DocumentMetadata = {\n    \"title\": \"Advanced NLP Techniques\",\n    \"author\": \"Jane Smith\",\n    \"year\": 2023,\n    \"keywords\": [\"NLP\", \"machine learning\", \"transformers\"]\n}\nprint(metadata)\n</pre> from typing import NamedTuple, TypedDict  # NamedTuple - an immutable, typed version of a tuple with named fields class Token(NamedTuple):     text: str     pos_tag: str     is_stop: bool  # Create a token token = Token(text=\"apple\", pos_tag=\"NOUN\", is_stop=False) print(token) print(f\"Token text: {token.text}, POS tag: {token.pos_tag}\")  # TypedDict - a dictionary with a fixed set of keys, each with a specified type class DocumentMetadata(TypedDict):     title: str     author: str     year: int     keywords: List[str]  # Create document metadata metadata: DocumentMetadata = {     \"title\": \"Advanced NLP Techniques\",     \"author\": \"Jane Smith\",     \"year\": 2023,     \"keywords\": [\"NLP\", \"machine learning\", \"transformers\"] } print(metadata) <pre>Token(text='apple', pos_tag='NOUN', is_stop=False)\nToken text: apple, POS tag: NOUN\n{'title': 'Advanced NLP Techniques', 'author': 'Jane Smith', 'year': 2023, 'keywords': ['NLP', 'machine learning', 'transformers']}\n</pre> In\u00a0[23]: Copied! <pre># Function types with Callable\nfrom typing import Callable\n\n# Define a type for text processing functions\nTextProcessor = Callable[[str], str]\n\ndef apply_processors(text: str, processors: List[TextProcessor]) -&gt; str:\n    \"\"\"Apply a series of text processors to a string.\"\"\"\n    result = text\n    for processor in processors:\n        result = processor(result)\n    return result\n\n# Define some text processors\ndef lowercase(text: str) -&gt; str:\n    return text.lower()\n\ndef remove_punctuation(text: str) -&gt; str:\n    import string\n    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n\n# Apply processors\ntext = \"Hello, World! This is a TEST.\"\nprocessed_text = apply_processors(text, [lowercase, remove_punctuation])\nprint(f\"Original: {text}\")\nprint(f\"Processed: {processed_text}\")\n</pre> # Function types with Callable from typing import Callable  # Define a type for text processing functions TextProcessor = Callable[[str], str]  def apply_processors(text: str, processors: List[TextProcessor]) -&gt; str:     \"\"\"Apply a series of text processors to a string.\"\"\"     result = text     for processor in processors:         result = processor(result)     return result  # Define some text processors def lowercase(text: str) -&gt; str:     return text.lower()  def remove_punctuation(text: str) -&gt; str:     import string     return text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Apply processors text = \"Hello, World! This is a TEST.\" processed_text = apply_processors(text, [lowercase, remove_punctuation]) print(f\"Original: {text}\") print(f\"Processed: {processed_text}\") <pre>Original: Hello, World! This is a TEST.\nProcessed: hello world this is a test\n</pre> In\u00a0[24]: Copied! <pre>from typing import Union, Optional\n\n# A function that can take either a string or a list of strings\ndef normalize_text(text: Union[str, List[str]]) -&gt; str:\n    if isinstance(text, list):\n        return \" \".join(text).lower()\n    return text.lower()\n\n# Examples\nprint(normalize_text(\"Hello World\"))\nprint(normalize_text([\"Hello\", \"World\"]))\n\n# Optional parameter (can be None)\ndef extract_entities(text: str, entity_types: Optional[List[str]] = None) -&gt; List[str]:\n    \"\"\"Extract entities of specified types from text.\"\"\"\n    # In a real implementation, this would use an NLP library\n    # For demonstration, we'll just return some dummy data\n    if entity_types is None:\n        return [\"John\", \"New York\", \"Google\"]\n    elif \"PERSON\" in entity_types:\n        return [\"John\"]\n    elif \"LOCATION\" in entity_types:\n        return [\"New York\"]\n    else:\n        return []\n\n# Examples\nprint(extract_entities(\"John works at Google in New York.\"))\nprint(extract_entities(\"John works at Google in New York.\", [\"PERSON\"]))\n</pre> from typing import Union, Optional  # A function that can take either a string or a list of strings def normalize_text(text: Union[str, List[str]]) -&gt; str:     if isinstance(text, list):         return \" \".join(text).lower()     return text.lower()  # Examples print(normalize_text(\"Hello World\")) print(normalize_text([\"Hello\", \"World\"]))  # Optional parameter (can be None) def extract_entities(text: str, entity_types: Optional[List[str]] = None) -&gt; List[str]:     \"\"\"Extract entities of specified types from text.\"\"\"     # In a real implementation, this would use an NLP library     # For demonstration, we'll just return some dummy data     if entity_types is None:         return [\"John\", \"New York\", \"Google\"]     elif \"PERSON\" in entity_types:         return [\"John\"]     elif \"LOCATION\" in entity_types:         return [\"New York\"]     else:         return []  # Examples print(extract_entities(\"John works at Google in New York.\")) print(extract_entities(\"John works at Google in New York.\", [\"PERSON\"])) <pre>hello world\nhello world\n['John', 'New York', 'Google']\n['John']\n</pre> In\u00a0[25]: Copied! <pre>def parse_numeric_data(string_list: List[str]) -&gt; List[Union[int, float]]:\n    # Your code here\n    pass\n\n# Test cases\ntest_data = [\"42\", \"3.14\", \"not a number\", \"99\", \"0.5\"]\n# Expected output: [42, 3.14, 99, 0.5]\n</pre> def parse_numeric_data(string_list: List[str]) -&gt; List[Union[int, float]]:     # Your code here     pass  # Test cases test_data = [\"42\", \"3.14\", \"not a number\", \"99\", \"0.5\"] # Expected output: [42, 3.14, 99, 0.5] In\u00a0[26]: Copied! <pre>def word_statistics(text: str) -&gt; Dict[str, Union[int, float]]:\n    # Your code here\n    pass\n\n# Test case\nsample_text = \"Natural Language Processing is fascinating. NLP combines linguistics and computer science.\"\n# Expected output: a dictionary with word_count, char_count, avg_word_length, and unique_words\n</pre> def word_statistics(text: str) -&gt; Dict[str, Union[int, float]]:     # Your code here     pass  # Test case sample_text = \"Natural Language Processing is fascinating. NLP combines linguistics and computer science.\" # Expected output: a dictionary with word_count, char_count, avg_word_length, and unique_words In\u00a0[27]: Copied! <pre>class Sentence:\n    def __init__(self, text: str):\n        # Your code here\n        pass\n    \n    def tokenize(self) -&gt; List[str]:\n        # Your code here\n        pass\n    \n    def word_frequencies(self) -&gt; Dict[str, int]:\n        # Your code here\n        pass\n    \n    def potential_named_entities(self) -&gt; List[str]:\n        # Your code here\n        pass\n\n# Test case\ntest_sentence = Sentence(\"John visited New York last summer with his friend Mary.\")\n# Expected: methods should return appropriate results for this sentence\n</pre> class Sentence:     def __init__(self, text: str):         # Your code here         pass          def tokenize(self) -&gt; List[str]:         # Your code here         pass          def word_frequencies(self) -&gt; Dict[str, int]:         # Your code here         pass          def potential_named_entities(self) -&gt; List[str]:         # Your code here         pass  # Test case test_sentence = Sentence(\"John visited New York last summer with his friend Mary.\") # Expected: methods should return appropriate results for this sentence In\u00a0[28]: Copied! <pre>from typing import List, Dict, Callable, Any\n\nDocument = Dict[str, Any]  # Type alias for a document\nTextProcessor = Callable[[str], str]  # Type alias for a text processing function\n\ndef process_documents(documents: List[Document], processor: TextProcessor) -&gt; List[Document]:\n    # Your code here\n    pass\n\n# Test case\ndocs = [\n    {\"id\": 1, \"title\": \"Introduction\", \"content\": \"This is an introduction to NLP.\"},\n    {\"id\": 2, \"title\": \"Methods\", \"content\": \"We use various methods in NLP.\"}\n]\n\ndef uppercase_processor(text: str) -&gt; str:\n    return text.upper()\n\n# Expected: a new list of documents with uppercase content\n</pre> from typing import List, Dict, Callable, Any  Document = Dict[str, Any]  # Type alias for a document TextProcessor = Callable[[str], str]  # Type alias for a text processing function  def process_documents(documents: List[Document], processor: TextProcessor) -&gt; List[Document]:     # Your code here     pass  # Test case docs = [     {\"id\": 1, \"title\": \"Introduction\", \"content\": \"This is an introduction to NLP.\"},     {\"id\": 2, \"title\": \"Methods\", \"content\": \"We use various methods in NLP.\"} ]  def uppercase_processor(text: str) -&gt; str:     return text.upper()  # Expected: a new list of documents with uppercase content"},{"location":"chapter1/Session_1_1_Python_1o1_1/#python-types","title":"Python Types\u00b6","text":"<p>This notebook covers the fundamental and advanced aspects of Python's type system. Understanding types is crucial for writing robust and maintainable code, especially in data science and NLP applications.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li>Basic Types</li> <li>Advanced Types</li> <li>Exercises</li> <li>Real-World Applications</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#1-basic-types","title":"1. Basic Types \u00b6","text":"<p>Python is a dynamically typed language, which means you don't need to declare the type of a variable when you create it. The interpreter infers the type based on the value assigned.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#11-primitive-types","title":"1.1 Primitive Types\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_1/#12-complex-types","title":"1.2 Complex Types\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_1/#13-type-conversion","title":"1.3 Type Conversion\u00b6","text":"<p>Python allows you to convert between different types using built-in functions.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#14-checking-types","title":"1.4 Checking Types\u00b6","text":"<p>You can check the type of a variable using the <code>type()</code> function or the <code>isinstance()</code> function.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#2-advanced-types","title":"2. Advanced Types \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_1/#21-type-hints-static-typing","title":"2.1 Type Hints (Static Typing)\u00b6","text":"<p>Python 3.5+ supports type hints, which allow you to specify the expected types of variables, function parameters, and return values. This helps with code documentation and can be used by tools like mypy for static type checking.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#22-custom-types-with-classes","title":"2.2 Custom Types with Classes\u00b6","text":"<p>You can create custom types using classes. This is a fundamental concept in object-oriented programming.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#23-advanced-type-concepts-for-nlp","title":"2.3 Advanced Type Concepts for NLP\u00b6","text":"<p>In NLP, you'll often work with complex data structures. Here are some advanced type concepts that are particularly useful:</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#24-union-types-and-optional","title":"2.4 Union Types and Optional\u00b6","text":"<p>Union types allow a variable to have one of several types. Optional is a shorthand for Union[T, None].</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#3-exercises","title":"3. Exercises \u00b6","text":"<p>Now it's your turn to practice working with Python types. Complete the following exercises to reinforce your understanding.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#exercise-1-type-conversion","title":"Exercise 1: Type Conversion\u00b6","text":"<p>Write a function <code>parse_numeric_data</code> that takes a list of strings, converts each string to a number (float or int as appropriate), and returns a list of numbers. If a string cannot be converted, it should be skipped.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#exercise-2-working-with-dictionaries","title":"Exercise 2: Working with Dictionaries\u00b6","text":"<p>Create a function <code>word_statistics</code> that takes a text string and returns a dictionary with the following statistics:</p> <ul> <li>'word_count': the number of words in the text</li> <li>'char_count': the number of characters (excluding spaces)</li> <li>'avg_word_length': the average length of words</li> <li>'unique_words': the number of unique words (case-insensitive)</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#exercise-3-custom-types-for-nlp","title":"Exercise 3: Custom Types for NLP\u00b6","text":"<p>Create a <code>Sentence</code> class that represents a sentence in an NLP context. It should:</p> <ul> <li>Store the original text</li> <li>Have a method to tokenize the sentence into words</li> <li>Have a method to count the frequency of each word</li> <li>Have a method to identify potential named entities (words that start with a capital letter, excluding the first word)</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#exercise-4-type-hints-in-practice","title":"Exercise 4: Type Hints in Practice\u00b6","text":"<p>Implement a function <code>process_documents</code> that takes a list of documents (each represented as a dictionary with 'id', 'title', and 'content' keys) and a processing function. The function should apply the processing function to each document's content and return a new list of documents with the processed content.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#4-real-world-applications","title":"4. Real-World Applications \u00b6","text":"<p>Here are some examples of how Python's type system is used in real-world NLP projects:</p>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#spacy","title":"spaCy\u00b6","text":"<p>spaCy is a popular NLP library that makes extensive use of Python's type system. It uses type hints throughout its codebase to ensure type safety and provide better documentation. The library defines custom types for tokens, documents, and other NLP concepts.</p> <pre>import spacy\nfrom spacy.tokens import Doc, Token, Span\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n\n# doc is of type Doc\n# Each token in doc is of type Token\n# Spans of tokens are of type Span\n</pre>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#hugging-face-transformers","title":"Hugging Face Transformers\u00b6","text":"<p>The Hugging Face Transformers library uses type hints to provide clear interfaces for its models and tokenizers. This helps users understand what types of inputs and outputs to expect when working with complex transformer models.</p> <pre>from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n\ninputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\noutputs = model(**inputs)\n\n# inputs is a dictionary of torch.Tensor objects\n# outputs contains logits of type torch.Tensor\n</pre>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#nltk","title":"NLTK\u00b6","text":"<p>The Natural Language Toolkit (NLTK) is one of the oldest and most comprehensive NLP libraries. While it predates Python's type hints, it uses Python's object-oriented features to define clear types for linguistic concepts.</p> <pre>import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\n\ntext = \"Natural language processing is a field of computer science and linguistics.\"\ntokens = word_tokenize(text)\ntagged = pos_tag(tokens)\n\n# tokens is a list of strings\n# tagged is a list of tuples (word, tag)\n</pre>"},{"location":"chapter1/Session_1_1_Python_1o1_1/#fastapi","title":"FastAPI\u00b6","text":"<p>FastAPI is a modern web framework that leverages Python's type hints to automatically generate API documentation and perform request validation. It's often used to deploy NLP models as web services.</p> <pre>from fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass TextRequest(BaseModel):\n    text: str\n\nclass SentimentResponse(BaseModel):\n    sentiment: str\n    confidence: float\n\n@app.post(\"/sentiment\", response_model=SentimentResponse)\ndef analyze_sentiment(request: TextRequest):\n    # In a real application, this would use an NLP model\n    return {\"sentiment\": \"positive\", \"confidence\": 0.95}\n</pre> <p>These examples demonstrate how Python's type system is used in real-world NLP applications to create more robust, maintainable, and well-documented code.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/","title":"Python Classes","text":"In\u00a0[7]: Copied! <pre>class Animal:\n    def __init__(self, name, species):\n        self.name = name      # instance attribute\n        self.species = species\n\n    def make_sound(self):\n        print(\"&lt;generic animal sound&gt;\")\n\n# Instantiate the class\ndog = Animal(\"Fido\", \"Canine\")\nprint(dog.name, dog.species)\ndog.make_sound()\n</pre> class Animal:     def __init__(self, name, species):         self.name = name      # instance attribute         self.species = species      def make_sound(self):         print(\"\")  # Instantiate the class dog = Animal(\"Fido\", \"Canine\") print(dog.name, dog.species) dog.make_sound() <pre>Fido Canine\n&lt;generic animal sound&gt;\n</pre> In\u00a0[8]: Copied! <pre>class Dog(Animal):\n    def __init__(self, name):\n        super().__init__(name, \"Canine\")\n\n    def make_sound(self):\n        print(\"Woof!\")\n\nmy_dog = Dog(\"Rex\")\nprint(my_dog.name, my_dog.species)\nmy_dog.make_sound()\n</pre> class Dog(Animal):     def __init__(self, name):         super().__init__(name, \"Canine\")      def make_sound(self):         print(\"Woof!\")  my_dog = Dog(\"Rex\") print(my_dog.name, my_dog.species) my_dog.make_sound() <pre>Rex Canine\nWoof!\n</pre> In\u00a0[9]: Copied! <pre>class MathUtils:\n    PI = 3.14159  # class attribute\n\n    @classmethod\n    def circle_area(cls, radius):\n        return cls.PI * (radius ** 2)\n\n    @staticmethod\n    def add(a, b):\n        return a + b\n\n# Using class and static methods\nprint(MathUtils.circle_area(5))\nprint(MathUtils.add(10, 20))\n</pre> class MathUtils:     PI = 3.14159  # class attribute      @classmethod     def circle_area(cls, radius):         return cls.PI * (radius ** 2)      @staticmethod     def add(a, b):         return a + b  # Using class and static methods print(MathUtils.circle_area(5)) print(MathUtils.add(10, 20)) <pre>78.53975\n30\n</pre> In\u00a0[10]: Copied! <pre>class Vector:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __str__(self):\n        return f\"Vector({self.x}, {self.y})\"\n\n    def __add__(self, other):\n        return Vector(self.x + other.x, self.y + other.y)\n\nv1 = Vector(2, 3)\nv2 = Vector(4, 1)\nv3 = v1 + v2\nprint(v3)\n</pre> class Vector:     def __init__(self, x, y):         self.x = x         self.y = y      def __str__(self):         return f\"Vector({self.x}, {self.y})\"      def __add__(self, other):         return Vector(self.x + other.x, self.y + other.y)  v1 = Vector(2, 3) v2 = Vector(4, 1) v3 = v1 + v2 print(v3) <pre>Vector(6, 4)\n</pre> In\u00a0[11]: Copied! <pre>class Engine:\n    def start(self):\n        print(\"Engine starts.\")\n    def stop(self):\n        print(\"Engine stops.\")\n\nclass Car:\n    def __init__(self):\n        self.engine = Engine()  # Composition\n\n    def drive(self):\n        self.engine.start()\n        print(\"Car is driving...\")\n        self.engine.stop()\n\nmy_car = Car()\nmy_car.drive()\n</pre> class Engine:     def start(self):         print(\"Engine starts.\")     def stop(self):         print(\"Engine stops.\")  class Car:     def __init__(self):         self.engine = Engine()  # Composition      def drive(self):         self.engine.start()         print(\"Car is driving...\")         self.engine.stop()  my_car = Car() my_car.drive() <pre>Engine starts.\nCar is driving...\nEngine stops.\n</pre> In\u00a0[13]: Copied! <pre># Your solution here\n</pre> # Your solution here In\u00a0[14]: Copied! <pre># Your solution here\nclass Vehicle:\n    pass\n\nclass Truck(Vehicle):\n    pass\n\n# Example usage:\n# car = Vehicle(\"Toyota\", \"Camry\")\n# car.drive()\n# pickup = Truck(\"Ford\", \"F-150\", 1000)\n# pickup.drive()\n</pre> # Your solution here class Vehicle:     pass  class Truck(Vehicle):     pass  # Example usage: # car = Vehicle(\"Toyota\", \"Camry\") # car.drive() # pickup = Truck(\"Ford\", \"F-150\", 1000) # pickup.drive() In\u00a0[15]: Copied! <pre># Your code here\nclass MathOperations:\n    pass\n\n# Example usage:\n# ops = MathOperations.from_list([1, 2, 3])\n# print(ops)\n# print(MathOperations.multiply(3, 5))\n</pre> # Your code here class MathOperations:     pass  # Example usage: # ops = MathOperations.from_list([1, 2, 3]) # print(ops) # print(MathOperations.multiply(3, 5))"},{"location":"chapter1/Session_1_1_Python_1o1_2/#python-classes","title":"Python Classes\u00b6","text":"<p>This notebook explores Python classes, which are crucial for object-oriented programming (OOP). Mastering classes helps structure code effectively\u2014especially important in large NLP or data science projects.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li>Basic Concepts</li> <li>Advanced Concepts</li> <li>Exercises</li> <li>Real-World Applications</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#1-basic-concepts","title":"1. Basic Concepts \u00b6","text":"<p>In Python, classes allow you to create custom data types and bundle data (attributes) with behaviors (methods).</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#11-defining-a-class","title":"1.1 Defining a Class\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_2/#12-inheritance","title":"1.2 Inheritance\u00b6","text":"<p>Inheritance enables a new class (child) to inherit attributes and methods from an existing class (parent).</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#2-advanced-concepts","title":"2. Advanced Concepts \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_2/#21-class-methods-and-static-methods","title":"2.1 Class Methods and Static Methods\u00b6","text":"<p>Class methods take a reference to the class (<code>cls</code>) instead of the instance (<code>self</code>), while static methods don\u2019t take any special first argument.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#22-dunder-methods-magic-methods","title":"2.2 Dunder Methods (Magic Methods)\u00b6","text":"<p>Dunder methods allow classes to integrate with Python\u2019s built-in operations (like <code>str()</code>, <code>len()</code>, arithmetic, etc.).</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#23-composition-vs-inheritance","title":"2.3 Composition vs Inheritance\u00b6","text":"<p>Composition is an alternative to inheritance. Instead of inheriting from a class, you hold an instance of another class as an attribute.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#3-exercises","title":"3. Exercises \u00b6","text":"<p>Work on the following exercises to consolidate your understanding of Python classes.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#exercise-1-creating-a-book-class","title":"Exercise 1: Creating a Book Class\u00b6","text":"<ol> <li>Create a <code>Book</code> class with attributes: <code>title</code>, <code>author</code>, <code>pages</code>.</li> <li>Implement a <code>__str__</code> method that returns a string in the format: <code>\"Book(title='...', author='...', pages=...)\"</code>.</li> <li>Instantiate a few <code>Book</code> objects and print them.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#exercise-2-inheritance","title":"Exercise 2: Inheritance\u00b6","text":"<ol> <li>Create a <code>Vehicle</code> parent class with attributes: <code>make</code>, <code>model</code>, and a method <code>drive()</code>.</li> <li>Create a <code>Truck</code> child class that inherits from <code>Vehicle</code>. Add an attribute <code>capacity</code> and override <code>drive()</code> to print a different message.</li> <li>Instantiate both classes and call their <code>drive()</code> methods.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#exercise-3-classstatic-methods","title":"Exercise 3: Class/Static Methods\u00b6","text":"<ol> <li>Define a class <code>MathOperations</code> with a class method <code>from_list(values)</code> that returns an instance with some aggregated result (e.g., sum of the list), and a static method <code>multiply(a, b)</code>.</li> <li>Demonstrate usage by creating an instance using <code>from_list</code> and calling <code>multiply</code>.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#4-real-world-applications","title":"4. Real-World Applications \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_2/#frameworks-using-oop","title":"Frameworks Using OOP\u00b6","text":"<ul> <li>Django: A popular web framework that relies heavily on classes (models, views) to structure large applications.</li> <li>Flask Extensions: Often define extension classes for plugin functionality.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#nlp-libraries","title":"NLP Libraries\u00b6","text":"<ul> <li>spaCy: Defines classes like <code>Doc</code>, <code>Token</code>, <code>Span</code> for text processing.</li> <li>NLTK: Many classes for parsing, tokenization, etc.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_2/#data-science","title":"Data Science\u00b6","text":"<ul> <li>scikit-learn: Almost every algorithm is an object with <code>.fit()</code> and <code>.predict()</code> methods.</li> <li>PyTorch / TensorFlow: Model classes that define neural network architectures.</li> </ul> <p>Classes are the foundation of these libraries, making it easier to organize and extend complex functionality.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/","title":"Python DataFrames (pandas)","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n\n# From a dictionary\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\n\ndf = pd.DataFrame(data)\ndf\n</pre> import pandas as pd  # From a dictionary data = {     \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],     \"Age\": [25, 30, 35],     \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"] }  df = pd.DataFrame(data) df Out[1]: Name Age City 0 Alice 25 New York 1 Bob 30 Los Angeles 2 Charlie 35 Chicago In\u00a0[2]: Copied! <pre># Example (uncomment if you have a file)\n# df_csv = pd.read_csv('data.csv')\n# df_excel = pd.read_excel('data.xlsx')\n# df_json = pd.read_json('data.json')\npass\n</pre> # Example (uncomment if you have a file) # df_csv = pd.read_csv('data.csv') # df_excel = pd.read_excel('data.xlsx') # df_json = pd.read_json('data.json') pass In\u00a0[3]: Copied! <pre>print(df.head())     # First few rows\nprint(df.tail())     # Last few rows\nprint(df.info())     # Data types and null counts\nprint(df.describe()) # Statistical summary\n</pre> print(df.head())     # First few rows print(df.tail())     # Last few rows print(df.info())     # Data types and null counts print(df.describe()) # Statistical summary <pre>      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    3 non-null      object\n 1   Age     3 non-null      int64 \n 2   City    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\nNone\n        Age\ncount   3.0\nmean   30.0\nstd     5.0\nmin    25.0\n25%    27.5\n50%    30.0\n75%    32.5\nmax    35.0\n</pre> In\u00a0[4]: Copied! <pre># Label-based indexing\nprint(\"Label-based indexing:\")\nprint(df.loc[0, \"Name\"])\nprint()\n\n# Integer-based indexing\nprint(\"Integer-based indexing:\")\nprint(df.iloc[1, 2])\n</pre> # Label-based indexing print(\"Label-based indexing:\") print(df.loc[0, \"Name\"]) print()  # Integer-based indexing print(\"Integer-based indexing:\") print(df.iloc[1, 2]) <pre>Label-based indexing:\nAlice\n\nInteger-based indexing:\nLos Angeles\n</pre> In\u00a0[5]: Copied! <pre>data_extra = {\n    \"Name\": [\"Alice\", \"Bob\"],\n    \"Salary\": [70000, 80000]\n}\ndf_extra = pd.DataFrame(data_extra)\n\nmerged_df = pd.merge(df, df_extra, on=\"Name\", how=\"left\")\nmerged_df\n</pre> data_extra = {     \"Name\": [\"Alice\", \"Bob\"],     \"Salary\": [70000, 80000] } df_extra = pd.DataFrame(data_extra)  merged_df = pd.merge(df, df_extra, on=\"Name\", how=\"left\") merged_df Out[5]: Name Age City Salary 0 Alice 25 New York 70000.0 1 Bob 30 Los Angeles 80000.0 2 Charlie 35 Chicago NaN In\u00a0[6]: Copied! <pre># Example data\ndf_sales = pd.DataFrame({\n    \"Product\": [\"A\", \"A\", \"B\", \"B\", \"B\"],\n    \"Sales\": [100, 150, 200, 120, 180],\n    \"Region\": [\"North\", \"South\", \"North\", \"South\", \"North\"]\n})\n\ngrouped = df_sales.groupby(\"Product\").agg({\"Sales\": \"sum\"})\ngrouped\n</pre> # Example data df_sales = pd.DataFrame({     \"Product\": [\"A\", \"A\", \"B\", \"B\", \"B\"],     \"Sales\": [100, 150, 200, 120, 180],     \"Region\": [\"North\", \"South\", \"North\", \"South\", \"North\"] })  grouped = df_sales.groupby(\"Product\").agg({\"Sales\": \"sum\"}) grouped Out[6]: Sales Product A 250 B 500 In\u00a0[7]: Copied! <pre>df_missing = pd.DataFrame({\n    \"Col1\": [1, None, 3],\n    \"Col2\": [None, 5, 6]\n})\nprint(df_missing)\n\ndf_dropped = df_missing.dropna()\nprint(\"\\nAfter dropna:\\n\", df_dropped)\n\ndf_filled = df_missing.fillna(0)\nprint(\"\\nAfter fillna(0):\\n\", df_filled)\n</pre> df_missing = pd.DataFrame({     \"Col1\": [1, None, 3],     \"Col2\": [None, 5, 6] }) print(df_missing)  df_dropped = df_missing.dropna() print(\"\\nAfter dropna:\\n\", df_dropped)  df_filled = df_missing.fillna(0) print(\"\\nAfter fillna(0):\\n\", df_filled) <pre>   Col1  Col2\n0   1.0   NaN\n1   NaN   5.0\n2   3.0   6.0\n\nAfter dropna:\n    Col1  Col2\n2   3.0   6.0\n\nAfter fillna(0):\n    Col1  Col2\n0   1.0   0.0\n1   0.0   5.0\n2   3.0   6.0\n</pre> In\u00a0[8]: Copied! <pre># Your code here\nimport numpy as np\n\ndf_ex = pd.DataFrame({\n    \"Name\": [\"Tom\", \"Jane\", \"Steve\", \"NaN\"],\n    \"Age\": [25, None, 30, 22],\n    \"City\": [\"Boston\", \"\", \"Seattle\", None]\n})\n# 1) Create the DataFrame\n# 2) Drop rows with missing values\n# 3) Fill missing Age with mean\n</pre> # Your code here import numpy as np  df_ex = pd.DataFrame({     \"Name\": [\"Tom\", \"Jane\", \"Steve\", \"NaN\"],     \"Age\": [25, None, 30, 22],     \"City\": [\"Boston\", \"\", \"Seattle\", None] }) # 1) Create the DataFrame # 2) Drop rows with missing values # 3) Fill missing Age with mean In\u00a0[9]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[10]: Copied! <pre># Your code here\n</pre> # Your code here"},{"location":"chapter1/Session_1_1_Python_1o1_3/#python-dataframes-pandas","title":"Python DataFrames (pandas)\u00b6","text":"<p>This notebook introduces the basics and advanced features of pandas <code>DataFrame</code>. DataFrames are central to data manipulation in Python.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li>Basic Concepts</li> <li>Advanced Concepts</li> <li>Exercises</li> <li>Real-World Applications</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#1-basic-concepts","title":"1. Basic Concepts \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_3/#11-creating-a-dataframe","title":"1.1 Creating a DataFrame\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_3/#12-reading-data-from-files","title":"1.2 Reading Data from Files\u00b6","text":"<p>Commonly, pandas is used to read CSV, Excel, or JSON files.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#13-basic-inspection","title":"1.3 Basic Inspection\u00b6","text":"<p>Methods for quickly assessing your DataFrame\u2019s shape and contents.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#2-advanced-concepts","title":"2. Advanced Concepts \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_3/#21-indexing-and-selection","title":"2.1 Indexing and Selection\u00b6","text":"<p>Pandas offers powerful indexing with <code>.loc</code> (label-based) and <code>.iloc</code> (integer-based).</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#22-merging-and-joining","title":"2.2 Merging and Joining\u00b6","text":"<p>You can combine DataFrames in various ways using <code>merge()</code>, <code>join()</code>, or <code>concat()</code>.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#23-groupby-and-aggregation","title":"2.3 GroupBy and Aggregation\u00b6","text":"<p>Grouping data by categories and applying aggregate functions like <code>sum</code>, <code>mean</code>, or <code>count</code>.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#24-handling-missing-data","title":"2.4 Handling Missing Data\u00b6","text":"<p>Missing data is common in real datasets. Pandas provides methods like <code>dropna()</code>, <code>fillna()</code>, etc.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#3-exercises","title":"3. Exercises \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_3/#exercise-1-data-cleaning","title":"Exercise 1: Data Cleaning\u00b6","text":"<ol> <li>Create a DataFrame with columns <code>Name</code>, <code>Age</code>, <code>City</code>, and some missing values.</li> <li>Drop rows with missing values.</li> <li>Fill missing values in <code>Age</code> with the mean age.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#exercise-2-groupby-and-aggregation","title":"Exercise 2: GroupBy and Aggregation\u00b6","text":"<p>Using the <code>df_sales</code> DataFrame shown earlier (or create your own):</p> <ol> <li>Group by <code>Region</code>.</li> <li>Calculate the average sales per region.</li> <li>Print the results.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#exercise-3-merging-dataframes","title":"Exercise 3: Merging DataFrames\u00b6","text":"<ol> <li>Create two DataFrames <code>df1</code> and <code>df2</code> with a common column (e.g., <code>id</code>).</li> <li>Perform a left merge on <code>id</code>.</li> <li>Perform an inner merge on <code>id</code>.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#4-real-world-applications","title":"4. Real-World Applications \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_3/#etl-extract-transform-load","title":"ETL (Extract, Transform, Load)\u00b6","text":"<ul> <li>Data scientists use pandas to extract data from various sources (databases, APIs, files), transform it (cleaning, feature engineering), and load it into analytics tools.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)\u00b6","text":"<ul> <li>Pandas is essential for quick EDA: summarizing datasets, detecting outliers, etc.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_3/#time-series-analysis","title":"Time-Series Analysis\u00b6","text":"<ul> <li>Pandas offers specialized support for time-series data, making it popular in finance and IoT data processing.</li> </ul> <p>These are just a few examples\u2014pandas is central to nearly every data-related task in Python!</p>"},{"location":"chapter1/Session_1_1_Python_1o1_4/","title":"Python NumPy","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\n# Creating arrays\narr1 = np.array([1, 2, 3])\narr2 = np.array([[1, 2], [3, 4]])\n\nprint(\"arr1:\", arr1)\nprint(\"arr2:\\n\", arr2)\nprint(\"Shape of arr2:\", arr2.shape)\n</pre> import numpy as np  # Creating arrays arr1 = np.array([1, 2, 3]) arr2 = np.array([[1, 2], [3, 4]])  print(\"arr1:\", arr1) print(\"arr2:\\n\", arr2) print(\"Shape of arr2:\", arr2.shape) <pre>arr1: [1 2 3]\narr2:\n [[1 2]\n [3 4]]\nShape of arr2: (2, 2)\n</pre> In\u00a0[2]: Copied! <pre>arr3 = np.array([10, 20, 30])\n\n# Element-wise arithmetic\nprint(\"arr1 + arr3 =\", arr1 + arr3)\n\n# Scalar operations\nprint(\"arr1 * 2 =\", arr1 * 2)\n\n# Comparison\nprint(\"arr3 &gt; 15?\", arr3 &gt; 15)\n</pre> arr3 = np.array([10, 20, 30])  # Element-wise arithmetic print(\"arr1 + arr3 =\", arr1 + arr3)  # Scalar operations print(\"arr1 * 2 =\", arr1 * 2)  # Comparison print(\"arr3 &gt; 15?\", arr3 &gt; 15) <pre>arr1 + arr3 = [11 22 33]\narr1 * 2 = [2 4 6]\narr3 &gt; 15? [False  True  True]\n</pre> In\u00a0[3]: Copied! <pre>arr4 = np.array([10, 11, 12, 13, 14, 15])\nprint(\"arr4[1:4] =\", arr4[1:4])\narr5 = np.array([[1,2,3],[4,5,6],[7,8,9]])\nprint(\"\\n\", arr5)\nprint(\"arr5[0, 1] =\", arr5[0, 1])\nprint(\"arr5[:, 1] =\", arr5[:, 1])\n</pre> arr4 = np.array([10, 11, 12, 13, 14, 15]) print(\"arr4[1:4] =\", arr4[1:4]) arr5 = np.array([[1,2,3],[4,5,6],[7,8,9]]) print(\"\\n\", arr5) print(\"arr5[0, 1] =\", arr5[0, 1]) print(\"arr5[:, 1] =\", arr5[:, 1]) <pre>arr4[1:4] = [11 12 13]\n\n [[1 2 3]\n [4 5 6]\n [7 8 9]]\narr5[0, 1] = 2\narr5[:, 1] = [2 5 8]\n</pre> In\u00a0[4]: Copied! <pre>arr6 = np.array([[1, 2, 3], [4, 5, 6]])\narr7 = np.array([10, 20, 30])\n# arr6 is (2,3) and arr7 is (3,)\n\nresult = arr6 + arr7  # Broadcasting adds arr7 to each row of arr6\nprint(result)\n</pre> arr6 = np.array([[1, 2, 3], [4, 5, 6]]) arr7 = np.array([10, 20, 30]) # arr6 is (2,3) and arr7 is (3,)  result = arr6 + arr7  # Broadcasting adds arr7 to each row of arr6 print(result) <pre>[[11 22 33]\n [14 25 36]]\n</pre> In\u00a0[5]: Copied! <pre>arr8 = np.arange(1, 7)\nprint(\"Original:\", arr8)\narr8_reshaped = arr8.reshape(2, 3)\nprint(\"\\nReshaped to (2,3):\\n\", arr8_reshaped)\n\n# Transpose\nprint(\"\\nTransposed:\\n\", arr8_reshaped.T)\n</pre> arr8 = np.arange(1, 7) print(\"Original:\", arr8) arr8_reshaped = arr8.reshape(2, 3) print(\"\\nReshaped to (2,3):\\n\", arr8_reshaped)  # Transpose print(\"\\nTransposed:\\n\", arr8_reshaped.T) <pre>Original: [1 2 3 4 5 6]\n\nReshaped to (2,3):\n [[1 2 3]\n [4 5 6]]\n\nTransposed:\n [[1 4]\n [2 5]\n [3 6]]\n</pre> In\u00a0[6]: Copied! <pre>arr9 = np.array([1, 2, 3, 4, 5])\nprint(\"Sum:\", np.sum(arr9))\nprint(\"Mean:\", np.mean(arr9))\nprint(\"Standard Deviation:\", np.std(arr9))\n</pre> arr9 = np.array([1, 2, 3, 4, 5]) print(\"Sum:\", np.sum(arr9)) print(\"Mean:\", np.mean(arr9)) print(\"Standard Deviation:\", np.std(arr9)) <pre>Sum: 15\nMean: 3.0\nStandard Deviation: 1.4142135623730951\n</pre> In\u00a0[7]: Copied! <pre>random_arr = np.random.rand(3, 3)  # uniform distribution\nprint(\"Random Array:\\n\", random_arr)\n\nrandint_arr = np.random.randint(0, 10, size=(2, 5))\nprint(\"\\nRandom Integers:\\n\", randint_arr)\n</pre> random_arr = np.random.rand(3, 3)  # uniform distribution print(\"Random Array:\\n\", random_arr)  randint_arr = np.random.randint(0, 10, size=(2, 5)) print(\"\\nRandom Integers:\\n\", randint_arr) <pre>Random Array:\n [[0.16558855 0.99591053 0.75989262]\n [0.00898257 0.12894033 0.74504858]\n [0.60349745 0.82211221 0.25542086]]\n\nRandom Integers:\n [[6 6 1 6 5]\n [3 5 8 0 6]]\n</pre> In\u00a0[8]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[9]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[10]: Copied! <pre># Your code here\n</pre> # Your code here"},{"location":"chapter1/Session_1_1_Python_1o1_4/#python-numpy","title":"Python NumPy\u00b6","text":"<p>NumPy is the fundamental package for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li>Basic Concepts</li> <li>Advanced Concepts</li> <li>Exercises</li> <li>Real-World Applications</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#1-basic-concepts","title":"1. Basic Concepts \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_4/#11-numpy-arrays","title":"1.1 NumPy Arrays\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_4/#12-array-operations","title":"1.2 Array Operations\u00b6","text":"<p>NumPy allows element-wise operations for arithmetic, comparisons, etc.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#13-slicing-and-indexing","title":"1.3 Slicing and Indexing\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_4/#2-advanced-concepts","title":"2. Advanced Concepts \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_4/#21-broadcasting","title":"2.1 Broadcasting\u00b6","text":"<p>NumPy can automatically expand array dimensions to match shapes during operations.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#22-reshaping-and-transposing","title":"2.2 Reshaping and Transposing\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_4/#23-mathematical-and-statistical-functions","title":"2.3 Mathematical and Statistical Functions\u00b6","text":"<p>NumPy provides a variety of built-in functions for computations like sum, mean, std, etc.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#24-random-module","title":"2.4 Random Module\u00b6","text":"<p>Useful for generating random numbers, random samples, etc.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#3-exercises","title":"3. Exercises \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_4/#exercise-1-basic-array-operations","title":"Exercise 1: Basic Array Operations\u00b6","text":"<ol> <li>Create a NumPy array of shape <code>(4,4)</code> with numbers from 1 to 16.</li> <li>Print the slice containing the second row.</li> <li>Multiply the entire array by 2.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#exercise-2-reshaping-and-broadcasting","title":"Exercise 2: Reshaping and Broadcasting\u00b6","text":"<ol> <li>Create a 1D array with numbers from 1 to 9.</li> <li>Reshape it into <code>(3,3)</code>.</li> <li>Add a 1D array <code>[10,10,10]</code> to it using broadcasting.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#exercise-3-statistical-functions","title":"Exercise 3: Statistical Functions\u00b6","text":"<ol> <li>Generate an array of 100 random numbers (using <code>np.random.randn()</code>).</li> <li>Compute the mean and standard deviation.</li> <li>Print the values.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#4-real-world-applications","title":"4. Real-World Applications \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_4/#machine-learning","title":"Machine Learning\u00b6","text":"<ul> <li>NumPy arrays are the foundation for libraries like scikit-learn, TensorFlow, and PyTorch.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#linear-algebra","title":"Linear Algebra\u00b6","text":"<ul> <li>Operations like matrix multiplication, decompositions, etc. are common in ML and data analysis.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_4/#signal-processing-simulations","title":"Signal Processing, Simulations\u00b6","text":"<ul> <li>Researchers often use NumPy for large-scale numerical simulations, random processes, or DSP tasks.</li> </ul> <p>NumPy is indispensable for scientific computing in Python, serving as the backbone for almost every advanced data library!</p>"},{"location":"chapter1/Session_1_1_Python_1o1_5/","title":"Python scikit-learn","text":"In\u00a0[1]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Example dataset (XOR-like)\nX = np.array([[0,0],[0,1],[1,0],[1,1]])\ny = np.array([0,1,1,0])\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\npreds = model.predict(X)\nprint(\"Predictions:\", preds)\nprint(\"Accuracy:\", accuracy_score(y, preds))\n</pre> from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np  # Example dataset (XOR-like) X = np.array([[0,0],[0,1],[1,0],[1,1]]) y = np.array([0,1,1,0])  model = LogisticRegression() model.fit(X, y)  preds = model.predict(X) print(\"Predictions:\", preds) print(\"Accuracy:\", accuracy_score(y, preds)) <pre>Predictions: [0 0 0 0]\nAccuracy: 0.5\n</pre> In\u00a0[2]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\n# Simple regression example\nX_reg = np.array([[1],[2],[3],[4],[5]])  # features\ny_reg = np.array([2,4,5,4,5])  # targets\n\nreg_model = LinearRegression()\nreg_model.fit(X_reg, y_reg)\n\nprint(\"Coefficients:\", reg_model.coef_)\nprint(\"Intercept:\", reg_model.intercept_)\n\ny_pred = reg_model.predict(X_reg)\nprint(\"Predictions:\", y_pred)\n</pre> from sklearn.linear_model import LinearRegression  # Simple regression example X_reg = np.array([[1],[2],[3],[4],[5]])  # features y_reg = np.array([2,4,5,4,5])  # targets  reg_model = LinearRegression() reg_model.fit(X_reg, y_reg)  print(\"Coefficients:\", reg_model.coef_) print(\"Intercept:\", reg_model.intercept_)  y_pred = reg_model.predict(X_reg) print(\"Predictions:\", y_pred) <pre>Coefficients: [0.6]\nIntercept: 2.2\nPredictions: [2.8 3.4 4.  4.6 5.2]\n</pre> In\u00a0[3]: Copied! <pre>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svc\", SVC(kernel=\"linear\"))\n])\n\n# Synthetic data\nX2 = np.array([[1, 200],[2, 180],[3, 240],[4, 210]])\ny2 = np.array([0,0,1,1])\n\npipeline.fit(X2, y2)\npred2 = pipeline.predict(X2)\npred2\n</pre> from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC  pipeline = Pipeline([     (\"scaler\", StandardScaler()),     (\"svc\", SVC(kernel=\"linear\")) ])  # Synthetic data X2 = np.array([[1, 200],[2, 180],[3, 240],[4, 210]]) y2 = np.array([0,0,1,1])  pipeline.fit(X2, y2) pred2 = pipeline.predict(X2) pred2 Out[3]: <pre>array([0, 0, 1, 1])</pre> In\u00a0[4]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"svc__C\": [0.1, 1, 10],\n    \"svc__kernel\": [\"linear\", \"rbf\"]\n}\n\ngrid_search = GridSearchCV(pipeline, param_grid, cv=2)\ngrid_search.fit(X2, y2)\nprint(\"Best Params:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)\n</pre> from sklearn.model_selection import GridSearchCV  param_grid = {     \"svc__C\": [0.1, 1, 10],     \"svc__kernel\": [\"linear\", \"rbf\"] }  grid_search = GridSearchCV(pipeline, param_grid, cv=2) grid_search.fit(X2, y2) print(\"Best Params:\", grid_search.best_params_) print(\"Best Score:\", grid_search.best_score_) <pre>Best Params: {'svc__C': 0.1, 'svc__kernel': 'linear'}\nBest Score: 1.0\n</pre> In\u00a0[5]: Copied! <pre>from sklearn.preprocessing import PolynomialFeatures\n\nX_poly = np.array([[2],[3],[4]])\npoly = PolynomialFeatures(degree=2)\nX_transformed = poly.fit_transform(X_poly)\nprint(\"Original:\", X_poly)\nprint(\"Polynomial Features:\\n\", X_transformed)\n</pre> from sklearn.preprocessing import PolynomialFeatures  X_poly = np.array([[2],[3],[4]]) poly = PolynomialFeatures(degree=2) X_transformed = poly.fit_transform(X_poly) print(\"Original:\", X_poly) print(\"Polynomial Features:\\n\", X_transformed) <pre>Original: [[2]\n [3]\n [4]]\nPolynomial Features:\n [[ 1.  2.  4.]\n [ 1.  3.  9.]\n [ 1.  4. 16.]]\n</pre> In\u00a0[6]: Copied! <pre>from sklearn.metrics import precision_score, recall_score\n\ny_true = [0, 1, 1, 0]\ny_pred = [0, 1, 0, 0]\n\nprint(\"Precision:\", precision_score(y_true, y_pred))\nprint(\"Recall:   \", recall_score(y_true, y_pred))\n</pre> from sklearn.metrics import precision_score, recall_score  y_true = [0, 1, 1, 0] y_pred = [0, 1, 0, 0]  print(\"Precision:\", precision_score(y_true, y_pred)) print(\"Recall:   \", recall_score(y_true, y_pred)) <pre>Precision: 1.0\nRecall:    0.5\n</pre> In\u00a0[8]: Copied! <pre># Your code here\n#X_test = np.array([[...], [...], ...])\n#y_test = np.array([...])\n# logistic_model = LogisticRegression()\n# logistic_model.fit(X_test, y_test)\n# preds_test = logistic_model.predict(X_test)\n# print(accuracy_score(y_test, preds_test))\n</pre> # Your code here #X_test = np.array([[...], [...], ...]) #y_test = np.array([...]) # logistic_model = LogisticRegression() # logistic_model.fit(X_test, y_test) # preds_test = logistic_model.predict(X_test) # print(accuracy_score(y_test, preds_test)) In\u00a0[9]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[10]: Copied! <pre># Your code here\n</pre> # Your code here"},{"location":"chapter1/Session_1_1_Python_1o1_5/#python-scikit-learn","title":"Python scikit-learn\u00b6","text":"<p>scikit-learn is a popular machine learning library offering a wide range of tools for predictive modeling: classification, regression, clustering, and more.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li>Basic Concepts</li> <li>Advanced Concepts</li> <li>Exercises</li> <li>Real-World Applications</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#1-basic-concepts","title":"1. Basic Concepts \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_5/#11-classifiers","title":"1.1 Classifiers\u00b6","text":"<p>Most scikit-learn APIs follow the <code>fit</code> and <code>predict</code> pattern.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#12-regression","title":"1.2 Regression\u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_5/#2-advanced-concepts","title":"2. Advanced Concepts \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_5/#21-pipelines","title":"2.1 Pipelines\u00b6","text":"<p>A pipeline chains multiple transformations and a final estimator.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#22-model-selection-and-cross-validation","title":"2.2 Model Selection and Cross-Validation\u00b6","text":"<p>scikit-learn provides utilities for hyperparameter tuning, like <code>GridSearchCV</code> or <code>RandomizedSearchCV</code>.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#23-feature-engineering","title":"2.3 Feature Engineering\u00b6","text":"<p>Feature engineering transforms raw data into features suitable for model training. scikit-learn has classes like <code>PolynomialFeatures</code>, <code>CountVectorizer</code> (for text), etc.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#24-common-metrics","title":"2.4 Common Metrics\u00b6","text":"<p>In addition to accuracy, scikit-learn offers <code>precision_score</code>, <code>recall_score</code>, <code>f1_score</code>, <code>r2_score</code>, etc.</p>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#3-exercises","title":"3. Exercises \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_5/#exercise-1-classification","title":"Exercise 1: Classification\u00b6","text":"<ol> <li>Use any small dataset (or generate synthetic data) for a classification task.</li> <li>Train a logistic regression model.</li> <li>Print accuracy.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#exercise-2-pipeline","title":"Exercise 2: Pipeline\u00b6","text":"<ol> <li>Create a pipeline with a <code>StandardScaler</code> and a <code>KNeighborsClassifier</code>.</li> <li>Fit it on some toy dataset.</li> <li>Predict and evaluate the performance.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#exercise-3-grid-search","title":"Exercise 3: Grid Search\u00b6","text":"<ol> <li>Use <code>GridSearchCV</code> on any model of your choice (e.g., <code>SVC</code>).</li> <li>Print the best parameters and best score.</li> </ol>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#4-real-world-applications","title":"4. Real-World Applications \u00b6","text":""},{"location":"chapter1/Session_1_1_Python_1o1_5/#classification-tasks","title":"Classification Tasks\u00b6","text":"<ul> <li>Spam Detection: Email text classification.</li> <li>Image Recognition: Digits dataset or complex images.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#regression-tasks","title":"Regression Tasks\u00b6","text":"<ul> <li>House Price Prediction: Predicting real estate prices based on features.</li> <li>Stock Forecasting: Although more advanced time-series methods exist, scikit-learn can handle simple regression or feature-based approaches.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#clustering","title":"Clustering\u00b6","text":"<ul> <li>Customer Segmentation: Using KMeans or DBSCAN to group similar customers.</li> </ul>"},{"location":"chapter1/Session_1_1_Python_1o1_5/#model-deployment","title":"Model Deployment\u00b6","text":"<ul> <li>scikit-learn models can be saved (e.g., <code>joblib</code>) and deployed within web applications for real-time inference.</li> </ul> <p>scikit-learn\u2019s consistent API and wide range of algorithms make it a go-to toolkit for ML in Python.</p>"},{"location":"chapter1/Session_1_2_baselines/","title":"Baseline with Regexes and spaCy for Spam Detection","text":"In\u00a0[1]: Copied! <pre># If you're in a local environment, uncomment the lines below:\n# !poetry run python -m spacy download en_core_web_sm\n\nimport re\nimport spacy\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.max_length = 2000000  # in case we have large texts\n</pre> # If you're in a local environment, uncomment the lines below: # !poetry run python -m spacy download en_core_web_sm  import re import spacy from datasets import load_dataset from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  nlp = spacy.load(\"en_core_web_sm\") nlp.max_length = 2000000  # in case we have large texts In\u00a0[2]: Copied! <pre>dataset = load_dataset(\"NotShrirang/email-spam-filter\")\ndataset\n</pre> dataset = load_dataset(\"NotShrirang/email-spam-filter\") dataset <pre>README.md:   0%|          | 0.00/113 [00:00&lt;?, ?B/s]</pre> <pre>train.csv:   0%|          | 0.00/5.40M [00:00&lt;?, ?B/s]</pre> <pre>Generating train split:   0%|          | 0/5171 [00:00&lt;?, ? examples/s]</pre> Out[2]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'label', 'text', 'label_num'],\n        num_rows: 5171\n    })\n})</pre> <p>We expect the dataset to have a <code>train</code> split by default, which we\u2019ll further split into train, dev, and a final test. Alternatively, we can keep the existing train as a larger pool and create dev/test from it. Some datasets also come with separate test sets. We'll check what's available after loading.</p> In\u00a0[3]: Copied! <pre># We'll see the columns: we expect something like {'text': ..., 'spam': ...}.\ndataset[\"train\"].features\n</pre> # We'll see the columns: we expect something like {'text': ..., 'spam': ...}. dataset[\"train\"].features Out[3]: <pre>{'Unnamed: 0': Value(dtype='int64', id=None),\n 'label': Value(dtype='string', id=None),\n 'text': Value(dtype='string', id=None),\n 'label_num': Value(dtype='int64', id=None)}</pre> In\u00a0[4]: Copied! <pre>df_data = dataset[\"train\"].to_pandas()\ndf_data.head()\n</pre> df_data = dataset[\"train\"].to_pandas() df_data.head() Out[4]: Unnamed: 0 label text label_num 0 605 ham Subject: enron methanol ; meter # : 988291\\nth... 0 1 2349 ham Subject: hpl nom for january 9 , 2001\\n( see a... 0 2 3624 ham Subject: neon retreat\\nho ho ho , we ' re arou... 0 3 4685 spam Subject: photoshop , windows , office . cheap ... 1 4 2030 ham Subject: re : indian springs\\nthis deal is to ... 0 In\u00a0[5]: Copied! <pre># We'll do a 60/20/20 split from the single 'train' dataset.\ndf_train, df_temp = train_test_split(df_data, test_size=0.4, stratify=df_data[\"label\"], random_state=42)\ndf_dev, df_test = train_test_split(df_temp, test_size=0.5, stratify=df_temp[\"label\"], random_state=42)\n\nprint(\"Train size:\", len(df_train))\nprint(\"Dev size:  \", len(df_dev))\nprint(\"Test size: \", len(df_test))\n</pre> # We'll do a 60/20/20 split from the single 'train' dataset. df_train, df_temp = train_test_split(df_data, test_size=0.4, stratify=df_data[\"label\"], random_state=42) df_dev, df_test = train_test_split(df_temp, test_size=0.5, stratify=df_temp[\"label\"], random_state=42)  print(\"Train size:\", len(df_train)) print(\"Dev size:  \", len(df_dev)) print(\"Test size: \", len(df_test)) <pre>Train size: 3102\nDev size:   1034\nTest size:  1035\n</pre> <p>Now we have 3 separate splits. We'll define some helper functions for evaluation.</p> In\u00a0[6]: Copied! <pre>def compute_metrics(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, pos_label=1)\n    rec = recall_score(y_true, y_pred, pos_label=1)\n    f1 = f1_score(y_true, y_pred, pos_label=1)\n    return {\n        \"accuracy\": acc,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1\": f1\n    }\n\ndef print_metrics(metrics_dict, prefix=\"\"):\n    print(f\"{prefix} Accuracy:  {metrics_dict['accuracy']*100:.2f}%\")\n    print(f\"{prefix} Precision: {metrics_dict['precision']*100:.2f}%\")\n    print(f\"{prefix} Recall:    {metrics_dict['recall']*100:.2f}%\")\n    print(f\"{prefix} F1-score:  {metrics_dict['f1']*100:.2f}%\\n\")\n</pre> def compute_metrics(y_true, y_pred):     acc = accuracy_score(y_true, y_pred)     prec = precision_score(y_true, y_pred, pos_label=1)     rec = recall_score(y_true, y_pred, pos_label=1)     f1 = f1_score(y_true, y_pred, pos_label=1)     return {         \"accuracy\": acc,         \"precision\": prec,         \"recall\": rec,         \"f1\": f1     }  def print_metrics(metrics_dict, prefix=\"\"):     print(f\"{prefix} Accuracy:  {metrics_dict['accuracy']*100:.2f}%\")     print(f\"{prefix} Precision: {metrics_dict['precision']*100:.2f}%\")     print(f\"{prefix} Recall:    {metrics_dict['recall']*100:.2f}%\")     print(f\"{prefix} F1-score:  {metrics_dict['f1']*100:.2f}%\\n\") In\u00a0[7]: Copied! <pre># Let's gather some 'spammy' tokens from the train set by naive frequency analysis.\n# We'll do a quick check of most common words in spam vs. not spam.\n\nimport collections\n\nspam_texts = df_train[df_train[\"label\"] == \"spam\"][\"text\"].values\nham_texts = df_train[df_train[\"label\"] == \"ham\"][\"text\"].values\n\ndef tokenize(text):\n    return re.findall(r\"\\w+\", text.lower())\n\nspam_words = []\nfor txt in spam_texts:\n    spam_words.extend(tokenize(txt))\n\nspam_counter = collections.Counter(spam_words)\nspam_most_common = spam_counter.most_common(20)\nspam_most_common\n</pre> # Let's gather some 'spammy' tokens from the train set by naive frequency analysis. # We'll do a quick check of most common words in spam vs. not spam.  import collections  spam_texts = df_train[df_train[\"label\"] == \"spam\"][\"text\"].values ham_texts = df_train[df_train[\"label\"] == \"ham\"][\"text\"].values  def tokenize(text):     return re.findall(r\"\\w+\", text.lower())  spam_words = [] for txt in spam_texts:     spam_words.extend(tokenize(txt))  spam_counter = collections.Counter(spam_words) spam_most_common = spam_counter.most_common(20) spam_most_common Out[7]: <pre>[('the', 4778),\n ('to', 3356),\n ('and', 3123),\n ('of', 2967),\n ('a', 2402),\n ('in', 2041),\n ('you', 1744),\n ('for', 1659),\n ('this', 1519),\n ('is', 1476),\n ('your', 1246),\n ('subject', 1000),\n ('with', 939),\n ('3', 918),\n ('that', 874),\n ('or', 869),\n ('on', 850),\n ('s', 848),\n ('be', 842),\n ('as', 766)]</pre> <p>We clearly see a lot of common words in the spam emails. \"The\", \"of\", ... stop words in English. Let's get rid of them. I imagine there are a lot of numbers and punctuation as well. Let's get rid of them too.</p> In\u00a0[8]: Copied! <pre>from spacy.lang.en.stop_words import STOP_WORDS\nimport string\n\npunctuation = string.punctuation\nnumbers = string.digits\n\nstop_words = set(STOP_WORDS)\n\nspam_words = []\nfor txt in spam_texts:\n    for word in tokenize(txt):\n        if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:\n            spam_words.append(word)\n\nspam_counter = collections.Counter(spam_words)\nspam_most_common = spam_counter.most_common(20)\nspam_most_common\n</pre> from spacy.lang.en.stop_words import STOP_WORDS import string  punctuation = string.punctuation numbers = string.digits  stop_words = set(STOP_WORDS)  spam_words = [] for txt in spam_texts:     for word in tokenize(txt):         if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:             spam_words.append(word)  spam_counter = collections.Counter(spam_words) spam_most_common = spam_counter.most_common(20) spam_most_common Out[8]: <pre>[('subject', 1000),\n ('company', 506),\n ('http', 460),\n ('information', 361),\n ('statements', 312),\n ('price', 310),\n ('email', 277),\n ('pills', 258),\n ('time', 241),\n ('font', 214),\n ('free', 211),\n ('message', 194),\n ('investment', 194),\n ('stock', 187),\n ('money', 184),\n ('business', 184),\n ('securities', 179),\n ('report', 176),\n ('2004', 174),\n ('contact', 172)]</pre> <p>We'll pick a few frequent tokens as naive spam triggers. (In reality, you'd do more thorough exploration or use a more advanced approach\u2014but let's keep it simple for demonstration.)</p> In\u00a0[9]: Copied! <pre># Let's define a basic regex pattern that flags emails containing typical spammy words.\nspam_keywords = [\"free\", \"http\", \"www\", \"money\", \n                 \"win\", \"winner\", \"congratulations\", \n                 \"urgent\", \"claim\", \"prize\", \"click\",\n                 \"price\"]\npattern = re.compile(r\"(\" + \"|\".join(spam_keywords) + r\")\", re.IGNORECASE)\n\ndef regex_spam_classifier(text):\n    if pattern.search(text):\n        return 1  # spam\n    return 0     # not spam\n</pre> # Let's define a basic regex pattern that flags emails containing typical spammy words. spam_keywords = [\"free\", \"http\", \"www\", \"money\",                   \"win\", \"winner\", \"congratulations\",                   \"urgent\", \"claim\", \"prize\", \"click\",                  \"price\"] pattern = re.compile(r\"(\" + \"|\".join(spam_keywords) + r\")\", re.IGNORECASE)  def regex_spam_classifier(text):     if pattern.search(text):         return 1  # spam     return 0     # not spam In\u00a0[10]: Copied! <pre>y_test_true = df_test[\"label_num\"].values\ny_test_pred = [regex_spam_classifier(txt) for txt in df_test[\"text\"].values]\n\ntest_metrics = compute_metrics(y_test_true, y_test_pred)\nprint_metrics(test_metrics, prefix=\"Regex Baseline (Test) \")\n</pre> y_test_true = df_test[\"label_num\"].values y_test_pred = [regex_spam_classifier(txt) for txt in df_test[\"text\"].values]  test_metrics = compute_metrics(y_test_true, y_test_pred) print_metrics(test_metrics, prefix=\"Regex Baseline (Test) \") <pre>Regex Baseline (Test)  Accuracy:  68.79%\nRegex Baseline (Test)  Precision: 47.42%\nRegex Baseline (Test)  Recall:    70.33%\nRegex Baseline (Test)  F1-score:  56.64%\n\n</pre> <p>Okay, not so bad, we get 70% of the spam emails, but we also have a lot of false positives almost 50% of our predictions are false positives !!</p> In\u00a0[11]: Copied! <pre>y_dev_true = df_dev[\"label_num\"].values\ntexts_dev = df_dev[\"text\"].values\n\ny_dev_pred = [regex_spam_classifier(txt) for txt in texts_dev]\ndev_metrics = compute_metrics(y_dev_true, y_dev_pred)\nprint_metrics(dev_metrics, prefix=\"Regex Baseline (Dev) \")\n\n# Let's identify the false positives and negatives.\nfp_indices = []  # predicted spam but actually ham\nfn_indices = []  # predicted ham but actually spam\n\nfor i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred)):\n    if gold == 0 and pred == 1:\n        fp_indices.append(i)\n    elif gold == 1 and pred == 0:\n        fn_indices.append(i)\n\nprint(\"False Positives:\", len(fp_indices), \"examples\")\nprint(\"False Negatives:\", len(fn_indices), \"examples\")\n</pre> y_dev_true = df_dev[\"label_num\"].values texts_dev = df_dev[\"text\"].values  y_dev_pred = [regex_spam_classifier(txt) for txt in texts_dev] dev_metrics = compute_metrics(y_dev_true, y_dev_pred) print_metrics(dev_metrics, prefix=\"Regex Baseline (Dev) \")  # Let's identify the false positives and negatives. fp_indices = []  # predicted spam but actually ham fn_indices = []  # predicted ham but actually spam  for i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred)):     if gold == 0 and pred == 1:         fp_indices.append(i)     elif gold == 1 and pred == 0:         fn_indices.append(i)  print(\"False Positives:\", len(fp_indices), \"examples\") print(\"False Negatives:\", len(fn_indices), \"examples\") <pre>Regex Baseline (Dev)  Accuracy:  67.41%\nRegex Baseline (Dev)  Precision: 45.77%\nRegex Baseline (Dev)  Recall:    66.67%\nRegex Baseline (Dev)  F1-score:  54.27%\n\nFalse Positives: 237 examples\nFalse Negatives: 100 examples\n</pre> <p>First thing is that the metrics are quite similar from the test set. Which means that both sets may be similar. Therefore if we find a way to improve on the dev set, we should see an improvement on the test set.</p> <p>We clearly see that we have a lot of false positives, also a significant number of false negatives. Therefore first, we may want to cover more cases and then create some other rules to reduce the number of false positives.</p> In\u00a0[12]: Copied! <pre>print(\"\\n--- Some False Negatives ---\\n\")\nfor idx in fn_indices[:20]:\n    print(\"DEV INDEX:\", idx)\n    print(texts_dev[idx][:300], \"...\")\n    print(\"---\")\n</pre> print(\"\\n--- Some False Negatives ---\\n\") for idx in fn_indices[:20]:     print(\"DEV INDEX:\", idx)     print(texts_dev[idx][:300], \"...\")     print(\"---\") <pre>\n--- Some False Negatives ---\n\nDEV INDEX: 17\nSubject: fw : old aged mmomy wants a date\nhey , man ! : )\ndovizhdane\n ...\n---\nDEV INDEX: 22\nSubject: prescription medication delivered overnight . p . n . termin , valium , + xanax + available . ki 80 hzhrb 5 if\nwe believe ordering medication should be as simple as ordering anything else on the internet : private , secure , and easy .\nalways available : \\ xana : x : # vlagr @ , | vialium ^ ...\n---\nDEV INDEX: 30\nSubject: 6 et vi - codin le 6 ally baronial fy dmabi\nhey there ,\nofore\nphacy\nspecials on :\nviin , van - ax , vi - are\ntariff\npleaove\nme\ntaunt\naccompaniment\nyjhanl pactwmtnfbiiw pl ym romjr\njco jbxdlnvtwszthg\nnjrjduhen d yfwvg lrn ...\n---\nDEV INDEX: 44\nSubject: story - my daughter isn ' t in pain anymore\nnewsweek medical : are you in pain ?\ncomparison finalists\nno more\ncrave persuasivehave penis worldbodyguard\nlackey coupeglutamine escape morphinefisherman\ncryptanalytic stokecellar algonquin bewitchcatnip\ncomplicate alkalinedalton kafkaesque gigab ...\n---\nDEV INDEX: 57\nSubject: whats the word . order your prescr - iption ' s here . . adams gettysburg\n . . . . pertinaciousd ' . . . .\nrawmnv ...\n---\nDEV INDEX: 68\nSubject: how to earn thousands writing google adwords part - time kara\ngooglecash gives you all the tools you need to turn the search engine google . com into\nan autopilot cash generating machine !\nwhat ' s your dream lifestyle ?\nphosphor disco ghoulish eardrum airplane geriatric approximant drop co ...\n---\nDEV INDEX: 72\nSubject: microsoft update warning - january 7 th\nminnesota , which can clinch a wild - card\nplayoff spot with a loss by either carolina or st . louis this weekend , appeared on\nits way to retaking the lead . but a holding penalty on birk - - the vikings were\nflagged nine times for 78 yards - - wiped ...\n---\nDEV INDEX: 77\nSubject: young pussies\ntonya could feel the glow of the hundreds of candles on her bare skin .\nher hair was plastered to her face and she thought she must have looked\nhorrible soaking wet , but she didn ' t care . gabriel thought she was beautiful\nand that was all she needed to know . tonya slid tow ...\n---\nDEV INDEX: 90\nSubject: the only smart way to control spam\nhey , i have a special _ offer for you . . .\nbetter than all other spam filters -\nonly delivers the email you want !\nthis is the ultimate solution that is guaranteed to stop all spam\nwithout\nlosing any of your important email ! this system protects you 100 ...\n---\nDEV INDEX: 108\nSubject: can jim come over and watch ?\nup to 80 %\nsavings on\nxanax ,\nvalium ,\ncodeine ,\nviagra\nand moretry us out here\nfor email\nremoval ,\ngo here .\njewelry elsinore chairperson ameslan decorticate badge foam cutler zinc shopkeep cylinder oracle alcove steppe inefficacy skeleton quartic wasp compagn ...\n---\nDEV INDEX: 114\nSubject: greatly improve your stamina\ni ' ve been using your product for 4 months now . i ' ve increased my\nlength from 2\nto nearly 6 . your product has saved my sex life . - matt , fl\nmy girlfriend loves the results , but she doesn ' t know what i do . she\nthinks\nit ' s natural - thomas , ca\npleasu ...\n---\nDEV INDEX: 116\nSubject: cheap soft viagra\nviagra soft tabs : perfect feeling of being men again .\nstarts working within just 15 minutes .\nsoft tabs :\ninfo site\nyou take a candy and get hard rock erection .\nthis is not miracle . this is just soft tabs .\nremove your email ...\n---\nDEV INDEX: 131\nSubject: penls enlarg 3 ment pllls\nenlarge your penls nowcllck h 3 re !\nno more ...\n---\nDEV INDEX: 161\nSubject: antigen downstairs dance\nstill no luck enlarging it ?\nour 2 products will work for you !\n1 . # 1 supplement available ! - works !\nfor vprx ciilck here\nand\n2 . * new * enhancement oil - get hard in 60 seconds ! amazing !\nlike no other oil you ' ve seen .\nfor vprx oil ciilck here\nthe 2 produc ...\n---\nDEV INDEX: 167\nSubject: new details id : 21195\nget your u n ive\nrsi t y d i\nplom acal 1 this number : 206\n- 424 - 1596 ( anytime )\nthere are no required tests , class e s , books , or\ninterviews !\nget a b a chelors , masters , m ba , and d\no ctorate ( phd ) d i\nploma ! receive the benefits and admiration\nthat come ...\n---\nDEV INDEX: 188\nSubject: welcome to toronto pharmac euticals , the net ' s most secure source for presc ription medicines made in the usa . . coincide confrere\nyou can finally get * real * pain medic ation that works .\nwe receive your orders and one of our 24 x 7 onboard us physicians will approve of your order ( 9 ...\n---\nDEV INDEX: 202\nSubject: inexplicable crying spells , sadness and / or irritability\n- - - - 22037566626923367\nhi varou ,\nsetting small , achievable goals will help will take you farther than you can imagine over time . it will help you reach your final destination : a happier , low - anxiety life .\nwe offer some of ...\n---\nDEV INDEX: 205\nSubject: quick , easy vicodin w lij tirb xhzcixu\nwe are the only source for vicodin online !\n- very easy ordering\n- no prior prescription needed\n- quick delivery\n- inexpensive\n ...\n---\nDEV INDEX: 213\nSubject: super cheap rates on best sexual health drug !\nthe power and effects of cialis stay in your body 9 times longer than vlagra !\nsave up to 60 % - order generic cialis today !\nnow they ' re chewable . like soft candy !\nexcalibu snuffybeautifu roy taffy daddy birdturbo abby cookies volley prope ...\n---\nDEV INDEX: 216\nSubject: lower lipids and lower risk for heart disease langley\nsome hills are never seenthe universe is\nexpanding\nalbum : good stufftitle : bad influence call it\nbad big town holds me backbig town skinns my mind\nsorry to have troubled you ; but it couldn ' t\nbe helped\nbolan kerlaugir xmo 3 reginlejf ...\n---\n</pre> <p>Okay looks interesting, maybe let's look for words that appear in the false negatives but not in the false positives.</p> In\u00a0[13]: Copied! <pre># Let's look for words that appear a lot in the false negatives but not so much in the false positives.\n# Let's use collections to count the words in the false negatives and false positives.\n# We'll get rid of stop words, punctuation and numbers.\n\nfn_words = []\nfor idx in fn_indices:\n    for word in tokenize(texts_dev[idx]):\n        if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:\n            fn_words.append(word)\n\nfp_words = []\nfor idx in fp_indices:\n    for word in tokenize(texts_dev[idx]):\n        if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:\n            fp_words.append(word)\n\n\nfn_counter = collections.Counter(fn_words)\nfp_counter = collections.Counter(fp_words)\n\n# Let's create a ratio of occurences in the false negatives over the false positives.\nfn_ratio = {word: fn_counter.get(word, 0) / (fp_counter.get(word, 0) + fn_counter.get(word, 0)) \n            for word in fn_counter if fp_counter.get(word, 0) + fn_counter.get(word, 0) &gt; 4}\n\n#Let's sort the words by the ratio.\nfn_ratio = sorted(fn_ratio.items(), key=lambda x: x[1], reverse=True)\n\n#Let's print the words that appear a lot in the false negatives but not so much in the false positives.\nfor word, ratio in fn_ratio[:50]:\n    print(word, ratio)\n</pre> # Let's look for words that appear a lot in the false negatives but not so much in the false positives. # Let's use collections to count the words in the false negatives and false positives. # We'll get rid of stop words, punctuation and numbers.  fn_words = [] for idx in fn_indices:     for word in tokenize(texts_dev[idx]):         if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:             fn_words.append(word)  fp_words = [] for idx in fp_indices:     for word in tokenize(texts_dev[idx]):         if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:             fp_words.append(word)   fn_counter = collections.Counter(fn_words) fp_counter = collections.Counter(fp_words)  # Let's create a ratio of occurences in the false negatives over the false positives. fn_ratio = {word: fn_counter.get(word, 0) / (fp_counter.get(word, 0) + fn_counter.get(word, 0))              for word in fn_counter if fp_counter.get(word, 0) + fn_counter.get(word, 0) &gt; 4}  #Let's sort the words by the ratio. fn_ratio = sorted(fn_ratio.items(), key=lambda x: x[1], reverse=True)  #Let's print the words that appear a lot in the false negatives but not so much in the false positives. for word, ratio in fn_ratio[:50]:     print(word, ratio) <pre>medications 1.0\npalestinian 1.0\nviagra 1.0\ncheap 1.0\nsoft 1.0\nminutes 1.0\nvicodin 1.0\ncialis 1.0\ndoctor 1.0\nblood 1.0\nloading 1.0\ncsgu 1.0\nprescription 0.9090909090909091\nspam 0.8888888888888888\nstop 0.8888888888888888\nsources 0.875\ngeneric 0.8\nmilitary 0.8\nrock 0.8\napproved 0.8\nsound 0.8\nmobile 0.7777777777777778\nordering 0.75\nstory 0.6666666666666666\ntabs 0.6666666666666666\nlady 0.6666666666666666\nvideo 0.625\nwaiting 0.625\nremove 0.6153846153846154\nattack 0.6\ninside 0.6\ninternational 0.6\nfriend 0.6\nstreet 0.6\ntook 0.6\nsecure 0.5714285714285714\nquick 0.5454545454545454\nturn 0.5\nclear 0.5\nhard 0.5\nreal 0.5\nquality 0.5\nsoftware 0.5\npaper 0.5\nshort 0.5\ncredit 0.46153846153846156\nenjoy 0.4444444444444444\nsaid 0.4444444444444444\ntown 0.42857142857142855\ncase 0.42857142857142855\n</pre> <p>Well looks like we have some interesting words there. Let's add them to the regex. We do it dumb way here, but in practice we should explore a bit more.</p> In\u00a0[14]: Copied! <pre>spam_keywords = [\"free\", \"http\", \"www\", \"money\", \n                 \"win\", \"winner\", \"congratulations\", \n                 \"urgent\", \"claim\", \"prize\", \"click\",\n                 \"price\", \"viagra\", \"vialium\", \"medication\",\n                 \"aged\", \"xana\", \"xanax\", \"asyc\", \"cheap\", \n                 \"palestinian\", \"blood\", \"doctor\", \"cialis\", \n                 \"minutes\", \"vicodin\", \"soft\", \"loading\", \n                 \"csgu\", \"medications\", \"prescription\", \"spam\", \"stop\"]\npattern = re.compile(r\"(\" + \"|\".join(spam_keywords) + r\")\", re.IGNORECASE)\n\ndef regex_spam_classifier_v0_2(text):\n    if pattern.search(text):\n        return 1  # spam\n    return 0     # not spam\n</pre> spam_keywords = [\"free\", \"http\", \"www\", \"money\",                   \"win\", \"winner\", \"congratulations\",                   \"urgent\", \"claim\", \"prize\", \"click\",                  \"price\", \"viagra\", \"vialium\", \"medication\",                  \"aged\", \"xana\", \"xanax\", \"asyc\", \"cheap\",                   \"palestinian\", \"blood\", \"doctor\", \"cialis\",                   \"minutes\", \"vicodin\", \"soft\", \"loading\",                   \"csgu\", \"medications\", \"prescription\", \"spam\", \"stop\"] pattern = re.compile(r\"(\" + \"|\".join(spam_keywords) + r\")\", re.IGNORECASE)  def regex_spam_classifier_v0_2(text):     if pattern.search(text):         return 1  # spam     return 0     # not spam In\u00a0[15]: Copied! <pre>y_test_true = df_test[\"label_num\"].values\ny_test_pred = [regex_spam_classifier_v0_2(txt) for txt in df_test[\"text\"].values]\n\ntest_metrics = compute_metrics(y_test_true, y_test_pred)\nprint_metrics(test_metrics, prefix=\"Regex Baseline (Test) \")\n</pre> y_test_true = df_test[\"label_num\"].values y_test_pred = [regex_spam_classifier_v0_2(txt) for txt in df_test[\"text\"].values]  test_metrics = compute_metrics(y_test_true, y_test_pred) print_metrics(test_metrics, prefix=\"Regex Baseline (Test) \") <pre>Regex Baseline (Test)  Accuracy:  70.14%\nRegex Baseline (Test)  Precision: 49.08%\nRegex Baseline (Test)  Recall:    80.33%\nRegex Baseline (Test)  F1-score:  60.94%\n\n</pre> <p>Incredible, meaning that just by adding a few words we get a huge improvement in the metrics (+10% of recall!) and the precision is still more or less the same.</p> In\u00a0[16]: Copied! <pre>y_dev_true = df_dev[\"label_num\"].values\ntexts_dev = df_dev[\"text\"].values\n\ny_dev_pred = [regex_spam_classifier_v0_2(txt) for txt in texts_dev]\ndev_metrics = compute_metrics(y_dev_true, y_dev_pred)\nprint_metrics(dev_metrics, prefix=\"Regex Baseline (Dev) \")\n\n# Let's identify the false positives and negatives.\nfp_indices = []  # predicted spam but actually ham\nfn_indices = []  # predicted ham but actually spam\n\nfor i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred)):\n    if gold == 0 and pred == 1:\n        fp_indices.append(i)\n    elif gold == 1 and pred == 0:\n        fn_indices.append(i)\n\nprint(\"False Positives:\", len(fp_indices), \"examples\")\nprint(\"False Negatives:\", len(fn_indices), \"examples\")\n</pre> y_dev_true = df_dev[\"label_num\"].values texts_dev = df_dev[\"text\"].values  y_dev_pred = [regex_spam_classifier_v0_2(txt) for txt in texts_dev] dev_metrics = compute_metrics(y_dev_true, y_dev_pred) print_metrics(dev_metrics, prefix=\"Regex Baseline (Dev) \")  # Let's identify the false positives and negatives. fp_indices = []  # predicted spam but actually ham fn_indices = []  # predicted ham but actually spam  for i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred)):     if gold == 0 and pred == 1:         fp_indices.append(i)     elif gold == 1 and pred == 0:         fn_indices.append(i)  print(\"False Positives:\", len(fp_indices), \"examples\") print(\"False Negatives:\", len(fn_indices), \"examples\") <pre>Regex Baseline (Dev)  Accuracy:  70.50%\nRegex Baseline (Dev)  Precision: 49.49%\nRegex Baseline (Dev)  Recall:    81.00%\nRegex Baseline (Dev)  F1-score:  61.44%\n\nFalse Positives: 248 examples\nFalse Negatives: 57 examples\n</pre> <p>We see that we reduced by two the number of false negatives. Let's see if we can reduce the number of false positives.</p> In\u00a0[17]: Copied! <pre>print(\"\\n--- Some False Positives ---\\n\")\nfor idx in fp_indices[:20]:\n    print(\"DEV INDEX:\", idx)\n    print(texts_dev[idx][:300], \"...\")\n    print(\"---\")\n</pre> print(\"\\n--- Some False Positives ---\\n\") for idx in fp_indices[:20]:     print(\"DEV INDEX:\", idx)     print(texts_dev[idx][:300], \"...\")     print(\"---\") <pre>\n--- Some False Positives ---\n\nDEV INDEX: 1\nSubject: playgroup pictures from houston cow parade\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ? easy unsubscribe click here : http : / / topica . com / u / ? a 84 vnf . a 9 ivhm or send an email to : brcc . yf  ...\n---\nDEV INDEX: 2\nSubject: re : united oil &amp; minerals , inc . , chapman unit # 1\nvance ,\ndeal # 357904 has been created and entered in sitara .\nbob\nvance l taylor\n08 / 04 / 2000 04 : 06 pm\nto : robert cotten / hou / ect @ ect , hillary mack / corp / enron @ enron , lisa\nhesse / hou / ect @ ect , trisha hughes / hou / ...\n---\nDEV INDEX: 9\nSubject: re : spinnaker exploration company , l . l . c . n . padre is . block 883 l\noffshore kleberg county , texas contract 96047295 , meter 098 - 9862 ( 098 - 9848\nplatform )\nthanks , bob . it now turns out that due to operational issues , the additional 10 , 000 / d may not come on next week . s ...\n---\nDEV INDEX: 12\nSubject: enronoptions update !\nenronoptions announcement\nwe have updated the enronoptions \u0001 ) your stock option program web site ! the\nweb site now contains specific details of the enronoptions program including\nthe december 29 , 2000 grant price and additional information on employee\neligibility .\n ...\n---\nDEV INDEX: 13\nSubject: panenergy marketing march 2000 production\ndeal # 157288\nper our conversation yesterday afternoon , pls . separate the centena term deal\nfrom the spot deal in sitara for march 2000 production .\nalso , i need to have the price for the east texas redelivery changed in\nsitara from hs index $ -  ...\n---\nDEV INDEX: 24\nSubject: re : april spot tickets\nthe spot deals are in and the deal numbers are added below to the original\nnotice .\nvance l taylor @ ect\n03 / 28 / 2000 01 : 40 pm\nto : tom acton / corp / enron @ enron\ncc : carlos j rodriguez / hou / ect @ ect , lisa hesse / hou / ect @ ect , susan\nsmith / hou / ect ...\n---\nDEV INDEX: 26\nSubject: good friday\nfyi - the risk team will not be in the office on friday . pat is evaluating\nthe situation currently , and will decide later this week . let me know if you\nhave any questions or concerns .\n- - - - - - - - - - - - - - - - - - - - - - forwarded by brenda f herod / hou / ect on 04 / ...\n---\nDEV INDEX: 28\nSubject: re : hpl discrepancy\nhey clem can you help us out with this one ? what are the volumes and deal\ntickets in question for those two days and what is the location ? we\ndelivered to you at centana and enerfin . didn ' t we have that famous\nhpl / tetco oba already set up to handle the small volu ...\n---\nDEV INDEX: 32\nSubject: volume feedback from unify to sitara\nfyi : the following is the unify to sitara bridge back schedule from the sitara team . unify can still send the files to sitara but sitara will not process during the \" no bridge back \" times listed . this list is in response to several inquiries to me r ...\n---\nDEV INDEX: 37\nSubject: epgt\nmike :\ni am down to the last few error messages on the epgt quick response and also looking into the external pool for who 34 in unify . about 70 % of the line items have been cleaned up .\ni need the following information from you as soon as possible .\n1 . the downstream contract numbe ...\n---\nDEV INDEX: 38\nSubject: re : tenaska iv 10 / 00\ndarren ,\nthe demand fee is probably the best solution . we can use it to create a\nrecieivable / payable with tenaska , depending on which way the calculation goes\neach month . how are pma ' s to be handled once the fee been calculated and the\ndeal put in the system ? ...\n---\nDEV INDEX: 43\nSubject: re : first delivery - cummings &amp; walker and exxon\nvance ,\ndeal # 446704 has been created and entered in sitara for cummins &amp; walker oil\ncompany inc . for the period 9 / 26 / 00 - 9 / 30 / 00 .\nbob\nvance l taylor\n10 / 20 / 2000 04 : 17 pm\nto : robert cotten / hou / ect @ ect\ncc : lisa hesse  ...\n---\nDEV INDEX: 66\nSubject: fw : txu fuel deals imbalances\ndaren ,\nthe deals listed below are related to tufco imbalances . . . let me know if you have any objections to me entering the deals . . . o ' neal 3 - 9686\n- - - - - original message - - - - -\nfrom : griffin , rebecca\nsent : thursday , june 28 , 2001 9 : 58 a ...\n---\nDEV INDEX: 67\nSubject: pat - out for jury duty\ni am out of the office on monday for jury duty . in my absence , charlotte\nhawkins will be the contact for the texas desk\nlogistics group . she will attend any meetings while i am out and is\nresponsible for our group ( we will rotate this backup\nrole among the senior ...\n---\nDEV INDEX: 69\nSubject: urgent\ned has requested that we compile a list this morning of all parties / points which we owe gas to , in the event that we need to find a home for excess volumes today . please email me a list of any meters / contracts that you are aware of . i am compiling an interim list based upon th ...\n---\nDEV INDEX: 83\nSubject: alternative work schedule status\nas you might already know we had to reschedule our second meeting that was\nscheduled for wednesday 2 / 16 to tuesday 2 / 22 in room 3013 . lunch will be\nprovided . i apologize and will avoid rescheduling our meetings in the future .\ni was encouraged by the e ...\n---\nDEV INDEX: 87\nSubject: fw : tribute to america\nregards ,\namy brock\nhbd marketing team\noffice : 281 - 988 - 2157\ncell : 713 - 702 - 6815\n- - - - - original message - - - - -\nfrom : rex waller\nsent : wednesday , september 12 , 2001 5 : 49 pm\nto : alfred webb ; allen hadaway ; allison boren ; amy brock ; barry willi ...\n---\nDEV INDEX: 89\nSubject: re : coastal o &amp; g , mtr . 4179 , goliad co .\nvance ,\njulie meyers created deal # 592122 in sitara . i have edited the ticket to\nreflect the details described below :\nbob\nvance l taylor\n02 / 01 / 2001 08 : 21 am\nto : robert cotten / hou / ect @ ect\ncc : clem cernosek / hou / ect @ ect\nsubje ...\n---\nDEV INDEX: 91\nSubject: april availabilities\n- - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 03 / 22 / 2000\n03 : 40 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\" steve holmes \" on 03 / 22 / 2000 01 : 51 : 48 pm\nto : ,\ncc : , , ,\n, ,\n, , ,\n, , ,\n, , ,\n, , ,\nsubjec ...\n---\nDEV INDEX: 94\nSubject: re : hpl delivery meter 1520\ncheryl ,\ndo you have any documentation on a gas lift deal with coastal ? engage ? at\nmeter 098 - 1520 ? thanks . george x 3 - 6992\n- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 04 / 19 / 2000\n06 : 48 pm - - - - - - - - - ...\n---\n</pre> In\u00a0[18]: Copied! <pre># Let's look for words that appear a lot in the false positives but not so much in the negatives.\n# Let's use collections to count the words in the negatives and false positives.\n# We'll get rid of stop words, punctuation and numbers.\n\npositive_indices = []\nfor i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred)):\n    if gold == 1:\n        positive_indices.append(i)\n\n\npositive_words = []\nfor idx in positive_indices:\n    for word in tokenize(texts_dev[idx]):\n        if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:\n            positive_words.append(word)\n\nfp_words = []\nfor idx in fp_indices:\n    for word in tokenize(texts_dev[idx]):\n        if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:\n            fp_words.append(word)\n\n\nfp_counter = collections.Counter(fp_words)\npositive_counter = collections.Counter(positive_words)\n\n# Let's create a ratio of occurences in the false positives over the false negatives.\nfp_ratio = {word: fp_counter.get(word, 0) / (fp_counter.get(word, 0) + positive_counter.get(word, 0)) \n            for word in fp_counter if fp_counter.get(word, 0) + positive_counter.get(word, 0) &gt; 3}\n\n#Let's sort the words by the ratio.\nfp_ratio = sorted(fp_ratio.items(), key=lambda x: x[1], reverse=True)\n\n#Let's print the words that appear a lot in the false positives but not so much in the false negatives.\nfor word, ratio in fp_ratio[:50]:\n    print(word, ratio)\n</pre> # Let's look for words that appear a lot in the false positives but not so much in the negatives. # Let's use collections to count the words in the negatives and false positives. # We'll get rid of stop words, punctuation and numbers.  positive_indices = [] for i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred)):     if gold == 1:         positive_indices.append(i)   positive_words = [] for idx in positive_indices:     for word in tokenize(texts_dev[idx]):         if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:             positive_words.append(word)  fp_words = [] for idx in fp_indices:     for word in tokenize(texts_dev[idx]):         if word not in stop_words and word not in punctuation and word not in numbers and len(word) &gt; 3:             fp_words.append(word)   fp_counter = collections.Counter(fp_words) positive_counter = collections.Counter(positive_words)  # Let's create a ratio of occurences in the false positives over the false negatives. fp_ratio = {word: fp_counter.get(word, 0) / (fp_counter.get(word, 0) + positive_counter.get(word, 0))              for word in fp_counter if fp_counter.get(word, 0) + positive_counter.get(word, 0) &gt; 3}  #Let's sort the words by the ratio. fp_ratio = sorted(fp_ratio.items(), key=lambda x: x[1], reverse=True)  #Let's print the words that appear a lot in the false positives but not so much in the false negatives. for word, ratio in fp_ratio[:50]:     print(word, ratio) <pre>topica 1.0\nivhm 1.0\nbrcc 1.0\ndfarmer 1.0\nenron 1.0\nmanage 1.0\ntago 1.0\nvance 1.0\nsitara 1.0\ncotten 1.0\nhillary 1.0\nmack 1.0\nlisa 1.0\nhesse 1.0\ntrisha 1.0\nhughes 1.0\nsusan 1.0\nreinhardt 1.0\nmelissa 1.0\ngraves 1.0\nacton 1.0\ncounterparty 1.0\nmeter 1.0\nvolumes 1.0\nmmbtu 1.0\nseptember 1.0\nadditionally 1.0\ntracked 1.0\nwellhead 1.0\n6353 1.0\nforwarded 1.0\njennifer 1.0\nblay 1.0\nchristy 1.0\nsweeney 1.0\njill 1.0\nzivley 1.0\nesther 1.0\nspinnaker 1.0\npadre 1.0\n96047295 1.0\n9862 1.0\n9848 1.0\nposted 1.0\ngeorge 1.0\nweissman 1.0\ndaren 1.0\nriley 1.0\nmike 1.0\nmorris 1.0\n</pre> <p>A bit leass easy, but we can try to create a new regex that should cover the false positives. A lot of names and surnames appear there, maybe quitting them would help. And also some coporate words such as \"following\" or \"brcc\".</p> In\u00a0[19]: Copied! <pre>spam_keywords = [\"free\", \"http\", \"www\", \"money\", \n                 \"win\", \"winner\", \"congratulations\", \n                 \"urgent\", \"claim\", \"prize\", \"click\",\n                 \"price\", \"viagra\", \"vialium\", \"medication\",\n                 \"aged\", \"xana\", \"xanax\", \"asyc\", \"cheap\", \n                 \"palestinian\", \"blood\", \"doctor\", \"cialis\", \n                 \"minutes\", \"vicodin\", \"soft\", \"loading\", \n                 \"csgu\", \"medications\", \"prescription\", \"spam\", \"stop\"]\nham_keywords = [\"hillary\", \"christy\", \"chapman\", \"susan\", \"reinhardt\",\n                \"sweeney\", \"melissa\", \"hughes\", \"lisa\", \"trisha\",\n                \"september\", \"tracked\", \"wellhead\", \"volumes\", \"meter\",\n                \"offshore\", \"county\", \"manage\", \"brcc\", \"ivmh\"]\npattern_spam_v0_3 = re.compile(r\"(\" + \"|\".join(spam_keywords) + r\")\", re.IGNORECASE)\npattern_ham_v0_3 = re.compile(r\"(\" + \"|\".join(ham_keywords) + r\")\", re.IGNORECASE)\n\ndef regex_spam_classifier_v0_3(text):\n    if len(pattern_spam_v0_3.findall(text)) &gt; len(pattern_ham_v0_3.findall(text)):\n        return 1  # spam\n    return 0     # not spam\n</pre> spam_keywords = [\"free\", \"http\", \"www\", \"money\",                   \"win\", \"winner\", \"congratulations\",                   \"urgent\", \"claim\", \"prize\", \"click\",                  \"price\", \"viagra\", \"vialium\", \"medication\",                  \"aged\", \"xana\", \"xanax\", \"asyc\", \"cheap\",                   \"palestinian\", \"blood\", \"doctor\", \"cialis\",                   \"minutes\", \"vicodin\", \"soft\", \"loading\",                   \"csgu\", \"medications\", \"prescription\", \"spam\", \"stop\"] ham_keywords = [\"hillary\", \"christy\", \"chapman\", \"susan\", \"reinhardt\",                 \"sweeney\", \"melissa\", \"hughes\", \"lisa\", \"trisha\",                 \"september\", \"tracked\", \"wellhead\", \"volumes\", \"meter\",                 \"offshore\", \"county\", \"manage\", \"brcc\", \"ivmh\"] pattern_spam_v0_3 = re.compile(r\"(\" + \"|\".join(spam_keywords) + r\")\", re.IGNORECASE) pattern_ham_v0_3 = re.compile(r\"(\" + \"|\".join(ham_keywords) + r\")\", re.IGNORECASE)  def regex_spam_classifier_v0_3(text):     if len(pattern_spam_v0_3.findall(text)) &gt; len(pattern_ham_v0_3.findall(text)):         return 1  # spam     return 0     # not spam In\u00a0[20]: Copied! <pre>y_test_true = df_test[\"label_num\"].values\ny_test_pred = [regex_spam_classifier_v0_3(txt) for txt in df_test[\"text\"].values]\n\ntest_metrics = compute_metrics(y_test_true, y_test_pred)\nprint_metrics(test_metrics, prefix=\"Regex Baseline (Test) \")\n</pre> y_test_true = df_test[\"label_num\"].values y_test_pred = [regex_spam_classifier_v0_3(txt) for txt in df_test[\"text\"].values]  test_metrics = compute_metrics(y_test_true, y_test_pred) print_metrics(test_metrics, prefix=\"Regex Baseline (Test) \") <pre>Regex Baseline (Test)  Accuracy:  80.68%\nRegex Baseline (Test)  Precision: 63.37%\nRegex Baseline (Test)  Recall:    79.00%\nRegex Baseline (Test)  F1-score:  70.33%\n\n</pre> <p>Well with we improved by 10 pts precision and 10 pts recall (almost !). Just by investigating the false positives and false negatives we can see that we are now detecting more spam and less ham. Therefore looking at the data is crucial to understanstand what the model is doing !</p> In\u00a0[21]: Copied! <pre>from spacy.matcher import Matcher\n\nmatcher = Matcher(nlp.vocab)\n\n# Example token-level patterns\npattern_free = [{\"LOWER\": \"free\"}]\npattern_click_now = [{\"LOWER\": \"click\"}, {\"LOWER\": \"now\"}]\npattern_urgent = [{\"LOWER\": \"urgent\"}]\n# etc.\n\nmatcher.add(\"FREE\", [pattern_free])\nmatcher.add(\"CLICK_NOW\", [pattern_click_now])\nmatcher.add(\"URGENT\", [pattern_urgent])\n</pre> from spacy.matcher import Matcher  matcher = Matcher(nlp.vocab)  # Example token-level patterns pattern_free = [{\"LOWER\": \"free\"}] pattern_click_now = [{\"LOWER\": \"click\"}, {\"LOWER\": \"now\"}] pattern_urgent = [{\"LOWER\": \"urgent\"}] # etc.  matcher.add(\"FREE\", [pattern_free]) matcher.add(\"CLICK_NOW\", [pattern_click_now]) matcher.add(\"URGENT\", [pattern_urgent]) In\u00a0[22]: Copied! <pre>def spacy_matcher_spam(doc):\n    matches = matcher(doc)\n    if matches:\n        return 1  # spam\n    return 0\n\ndef spacy_spam_classifier(text):\n    doc = nlp(text)\n    return spacy_matcher_spam(doc)\n</pre> def spacy_matcher_spam(doc):     matches = matcher(doc)     if matches:         return 1  # spam     return 0  def spacy_spam_classifier(text):     doc = nlp(text)     return spacy_matcher_spam(doc) In\u00a0[23]: Copied! <pre>y_test_pred_spacy = [spacy_spam_classifier(t) for t in df_test[\"text\"].values]\ntest_metrics_spacy = compute_metrics(y_test_true, y_test_pred_spacy)\nprint_metrics(test_metrics_spacy, \"spaCy Baseline (Test)\")\n</pre> y_test_pred_spacy = [spacy_spam_classifier(t) for t in df_test[\"text\"].values] test_metrics_spacy = compute_metrics(y_test_true, y_test_pred_spacy) print_metrics(test_metrics_spacy, \"spaCy Baseline (Test)\") <pre>spaCy Baseline (Test) Accuracy:  72.95%\nspaCy Baseline (Test) Precision: 64.71%\nspaCy Baseline (Test) Recall:    14.67%\nspaCy Baseline (Test) F1-score:  23.91%\n\n</pre> <p>In practice, we\u2019d repeat the false positive/negative analysis from earlier. I'll skip it as you can do it yourself :).</p> In\u00a0[24]: Copied! <pre>print(\"--- Final Comparison on Test Set ---\\n\")\nprint(\"Regex v2:\")\nprint_metrics(test_metrics)\nprint(\"spaCy v2:\")\nprint_metrics(test_metrics_spacy)\n</pre> print(\"--- Final Comparison on Test Set ---\\n\") print(\"Regex v2:\") print_metrics(test_metrics) print(\"spaCy v2:\") print_metrics(test_metrics_spacy) <pre>--- Final Comparison on Test Set ---\n\nRegex v2:\n Accuracy:  80.68%\n Precision: 63.37%\n Recall:    79.00%\n F1-score:  70.33%\n\nspaCy v2:\n Accuracy:  72.95%\n Precision: 64.71%\n Recall:    14.67%\n F1-score:  23.91%\n\n</pre> <p>We spent different amount of time on each approach, and that's why the metrics for regexes are better. With spaCy we can do more complex patterns and that's why it's more time consuming to implement. But let's imagine we use both models to see if we can improve the metrics.</p> <p>To do so let's compare the false positives and false negatives of the two models on the dev set. Maybe there are some patterns that are detected by one model but not by the other one.</p> In\u00a0[25]: Copied! <pre>y_dev_pred_spacy = [spacy_spam_classifier(t) for t in df_dev[\"text\"].values]\ny_dev_pred_regex = [regex_spam_classifier_v0_3(t) for t in df_dev[\"text\"].values]\n\nfp_indices_spacy = []\nfn_indices_spacy = []\n\nfor i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred_spacy)):\n    if gold == 0 and pred == 1:\n        fp_indices_spacy.append(i)\n    elif gold == 1 and pred == 0:\n        fn_indices_spacy.append(i)\n\nfp_indices_regex = []\nfn_indices_regex = []\n\nfor i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred_regex)):\n    if gold == 0 and pred == 1:\n        fp_indices_regex.append(i)\n    elif gold == 1 and pred == 0:\n        fn_indices_regex.append(i)\n</pre> y_dev_pred_spacy = [spacy_spam_classifier(t) for t in df_dev[\"text\"].values] y_dev_pred_regex = [regex_spam_classifier_v0_3(t) for t in df_dev[\"text\"].values]  fp_indices_spacy = [] fn_indices_spacy = []  for i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred_spacy)):     if gold == 0 and pred == 1:         fp_indices_spacy.append(i)     elif gold == 1 and pred == 0:         fn_indices_spacy.append(i)  fp_indices_regex = [] fn_indices_regex = []  for i, (gold, pred) in enumerate(zip(y_dev_true, y_dev_pred_regex)):     if gold == 0 and pred == 1:         fp_indices_regex.append(i)     elif gold == 1 and pred == 0:         fn_indices_regex.append(i) <p>Now let's look at the intersection of the two sets.</p> In\u00a0[26]: Copied! <pre>common_fp = set(fp_indices_spacy) &amp; set(fp_indices_regex)\ncommon_fn = set(fn_indices_spacy) &amp; set(fn_indices_regex)\n\nprint('Models:\\t spaCy\\t regex')\nprint(\"False Positives:\\t\", len(fp_indices_spacy), \"\\t\", len(fp_indices_regex))\nprint(\"False Negatives:\\t\", len(fn_indices_spacy), \"\\t\", len(fn_indices_regex))\nprint(\"Common False Positives:\\t\", len(common_fp))\nprint(\"Common False Negatives:\\t\", len(common_fn))\n</pre> common_fp = set(fp_indices_spacy) &amp; set(fp_indices_regex) common_fn = set(fn_indices_spacy) &amp; set(fn_indices_regex)  print('Models:\\t spaCy\\t regex') print(\"False Positives:\\t\", len(fp_indices_spacy), \"\\t\", len(fp_indices_regex)) print(\"False Negatives:\\t\", len(fn_indices_spacy), \"\\t\", len(fn_indices_regex)) print(\"Common False Positives:\\t\", len(common_fp)) print(\"Common False Negatives:\\t\", len(common_fn)) <pre>Models:\t spaCy\t regex\nFalse Positives:\t 37 \t 146\nFalse Negatives:\t 267 \t 63\nCommon False Positives:\t 28\nCommon False Negatives:\t 63\n</pre> <p>Therefore we see that the whole false neatives from regex are detected by spaCy. But there are less false positives from spaCy. Maybe adding the spaCy patterns to confirm false positives from regex would help. This is something you can test when you have optimized the spaCy patterns and even use a model that could learn how much weight to give to each model. Or just a statistical weight to avoid using Machine Learning models !</p>"},{"location":"chapter1/Session_1_2_baselines/#baseline-with-regexes-and-spacy-for-spam-detection","title":"Baseline with Regexes and spaCy for Spam Detection\u00b6","text":"<p>In this notebook, we will:</p> <ol> <li>Load a spam detection dataset from Hugging Face.</li> <li>Split our data into train, dev, and test sets, and explain why we need all three.</li> <li>Create a regex-based baseline pipeline:<ul> <li>Build naive patterns from the train set.</li> <li>Evaluate on test set.</li> <li>Check results on dev set to find false positives/negatives.</li> <li>Update regex rules.</li> <li>Final metrics on the test set.</li> </ul> </li> <li>Build a spaCy pipeline for spam detection:<ul> <li>Use token and phrase matchers.</li> <li>Repeat the same steps (train -&gt; dev -&gt; refine -&gt; test).</li> </ul> </li> <li>Compare results between the improved regex approach and spaCy approach.</li> </ol>"},{"location":"chapter1/Session_1_2_baselines/#setup-and-imports","title":"Setup and Imports\u00b6","text":"<p>We\u2019ll need:</p> <ul> <li>datasets: To load the spam dataset.</li> <li>scikit-learn: For splitting the dataset and computing metrics.</li> <li>re (built-in): For regex-based matching.</li> <li>spaCy: For token and phrase matchers.</li> </ul> <p>Make sure to look at this link to install all the dependencies.</p>"},{"location":"chapter1/Session_1_2_baselines/#1-load-the-dataset","title":"1. Load the Dataset\u00b6","text":"<p>We'll use NotShrirang/email-spam-filter. It's a dataset with email text labeled as spam or not spam.</p>"},{"location":"chapter1/Session_1_2_baselines/#2-create-traindevtest-splits","title":"2. Create Train/Dev/Test Splits\u00b6","text":"<p>Why do we need a dev set in addition to a train/test set?</p> <ul> <li>Train set: used to fit our model (or in this case, develop our regex/spaCy patterns).</li> <li>Dev (validation) set: used to tweak or refine patterns, hyperparameters, etc., without touching the final test. This prevents overfitting on the test set.</li> <li>Test set: final unbiased evaluation.</li> </ul> <p>If we only had train/test, we might continually adjust our method to do better on the test set, inadvertently tuning to that test distribution. The dev set helps keep the test set \"truly\" unseen.</p>"},{"location":"chapter1/Session_1_2_baselines/#3-regex-based-baseline","title":"3. Regex-Based Baseline\u00b6","text":""},{"location":"chapter1/Session_1_2_baselines/#3a-create-a-first-naive-pipeline","title":"3a. Create a first naive pipeline\u00b6","text":"<p>We\u2019ll look at the train set to find some potential spam indicators. Typically, spam might have words like <code>free</code>, <code>win</code>, <code>urgent</code>, <code>congratulations</code>, etc. This is just a guess. In a real scenario, you\u2019d examine the train data more carefully.</p>"},{"location":"chapter1/Session_1_2_baselines/#3b-get-metrics-on-the-test-set","title":"3b. Get metrics on the test set\u00b6","text":"<p>Even though we said we\u2019d refine on dev, let\u2019s see how it does out-of-the-box on the test set. (Sometimes it\u2019s informative to check a naive baseline right away.)</p>"},{"location":"chapter1/Session_1_2_baselines/#3c-check-dev-set-find-false-positives-negatives","title":"3c. Check dev set, find false positives &amp; negatives\u00b6","text":"<p>Let\u2019s see how many spam messages were missed (false negatives) and how many ham messages were flagged as spam (false positives) on the dev set.</p>"},{"location":"chapter1/Session_1_2_baselines/#3d-analyze-fn-to-improve-regex","title":"3d. Analyze FN to improve regex\u00b6","text":"<p>Let's first take a look at the false negatives to see if we can improve the regex.</p>"},{"location":"chapter1/Session_1_2_baselines/#3e-analyze-fp-to-improve-regex","title":"3e. Analyze FP to improve regex\u00b6","text":"<p>Let's do the same for the false positives. Meaning that we will find words that appear a lot in the false positives but not so much in the false negatives. If the message is detected as spam, we will apply another regex to check if it contains any of the words in the false positives. If it does, we will label it as ham.</p> <p>First let's check the dev set false positives.</p>"},{"location":"chapter1/Session_1_2_baselines/#3f-test-on-test-set","title":"3f. Test on test set\u00b6","text":"<p>We do the final metrics on the test set now that we have a more refined approach. (Though in practice, you might do multiple dev cycles, carefully checking you\u2019re not overfitting.)</p>"},{"location":"chapter1/Session_1_2_baselines/#3g-limitations","title":"3g. Limitations\u00b6","text":"<p>Clearly, a regex approach is limited. We\u2019ll often get false positives for edge cases or false negatives for spam that doesn\u2019t match our known keywords. Regexes can\u2019t capture synonyms or context. That\u2019s where an ML approach or more advanced text processing can help. But still we get 70% in F1 without any ML or advanced text processing !</p>"},{"location":"chapter1/Session_1_2_baselines/#4-spacy-approach","title":"4. spaCy Approach\u00b6","text":"<p>We\u2019ll create a small spaCy pipeline using the Matcher or TokenMatcher to detect spammy patterns. This is still rule-based, but spaCy makes it easier to do token-based patterns or phrase matching that\u2019s more robust than plain regex.</p>"},{"location":"chapter1/Session_1_2_baselines/#4a-token-matcher","title":"4a. Token matcher\u00b6","text":"<p>We can define token-based patterns: e.g., if a doc has <code>[{'LOWER': 'free'}]</code> or <code>[{'LOWER': 'click'}, {'LOWER': 'now'}]</code>.</p>"},{"location":"chapter1/Session_1_2_baselines/#4b-spacy-based-classifier","title":"4b. spaCy-based classifier\u00b6","text":"<p>We'll define a function that processes text with <code>nlp</code>, runs the matcher, and if any match is found, we label it spam. We'll refine similarly by analyzing dev set mistakes.</p>"},{"location":"chapter1/Session_1_2_baselines/#4c-evaluate-on-dev-set-refine-evaluate-on-test-set","title":"4c. Evaluate on dev set -&gt; refine -&gt; evaluate on test set\u00b6","text":"<p>Let\u2019s do it quickly, given we already know the general approach. We'll compute dev metrics, see if we can spot improvements, and finalize on test.</p>"},{"location":"chapter1/Session_1_2_baselines/#5-compare-regex-vs-spacy-approaches","title":"5. Compare Regex vs. spaCy Approaches\u00b6","text":"<p>We can summarize the final test metrics side by side.</p>"},{"location":"chapter1/Session_1_3_tfidf/","title":"ML with TF-IDF + Logistic Regression on IMDB Dataset","text":"In\u00a0[1]: Copied! <pre># 0. Setup &amp; Data Loading\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\n\n# Set numpy random seed for reproducibility\nnp.random.seed(42)\n\n# Load the IMDB dataset from Hugging Face\nimdb = load_dataset(\"imdb\")\n\n# Sample 8000 examples for training from the original train split\nimdb_train_full = imdb[\"train\"]\nimdb_test_full = imdb[\"test\"]\n\ntrain_df = pd.DataFrame(imdb_train_full)\ntest_df = pd.DataFrame(imdb_test_full)\n\n# Sample 8000 from train and 3000 from test\ntrain_df = train_df.sample(n=8000, random_state=42).reset_index(drop=True)\ntest_df = test_df.sample(n=3000, random_state=42).reset_index(drop=True)\n\n# Split train_df into train and dev (e.g., 80/20 split)\ntrain_df, dev_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"label\"])\n\nprint(\"Train size:\", len(train_df))\nprint(\"Dev size:\", len(dev_df))\nprint(\"Test size:\", len(test_df))\n</pre> # 0. Setup &amp; Data Loading import numpy as np import pandas as pd from datasets import load_dataset from sklearn.model_selection import train_test_split  # Set numpy random seed for reproducibility np.random.seed(42)  # Load the IMDB dataset from Hugging Face imdb = load_dataset(\"imdb\")  # Sample 8000 examples for training from the original train split imdb_train_full = imdb[\"train\"] imdb_test_full = imdb[\"test\"]  train_df = pd.DataFrame(imdb_train_full) test_df = pd.DataFrame(imdb_test_full)  # Sample 8000 from train and 3000 from test train_df = train_df.sample(n=8000, random_state=42).reset_index(drop=True) test_df = test_df.sample(n=3000, random_state=42).reset_index(drop=True)  # Split train_df into train and dev (e.g., 80/20 split) train_df, dev_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"label\"])  print(\"Train size:\", len(train_df)) print(\"Dev size:\", len(dev_df)) print(\"Test size:\", len(test_df)) <pre>Train size: 6400\nDev size: 1600\nTest size: 3000\n</pre> In\u00a0[2]: Copied! <pre># Import necessary libraries for TF-IDF and Logistic Regression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import FunctionTransformer\n\n# For demonstration, we define a very simple text preprocessor (could be extended)\nimport re\n\ndef simple_preprocessor(text):\n    # Lowercase and remove non-alphabetic characters\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    return text\n\n# Example hyperparameter settings for TfidfVectorizer\ntfidf_params = {\n    \"stop_words\": \"english\",           # Remove English stop words\n    \"tokenizer\": None,                   # Use default tokenizer; can be replaced by a custom one\n    \"analyzer\": \"word\",                # Analyze words (as opposed to characters)\n    \"min_df\": 5,                         # Ignore terms that appear in fewer than 5 documents\n    \"max_df\": 0.8,                       # Ignore terms that appear in more than 80% of the documents\n    \"ngram_range\": (1, 1),               # Unigrams by default (can change to (1,2) for bigrams, etc.)\n    \"max_features\": 10000                # Limit vocabulary size\n}\n\n# Create the pipeline\npipeline = Pipeline([\n    (\"preprocessor\", FunctionTransformer(lambda X: [simple_preprocessor(text) for text in X])),\n    (\"tfidf\", TfidfVectorizer(**tfidf_params)),\n    (\"logreg\", LogisticRegression(\n         # See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n         # Regularization: C is the inverse of regularization strength (smaller values specify stronger regularization)\n         # class_weight: can be set to 'balanced' to automatically adjust weights inversely proportional to class frequencies\n         C=1.0,\n         penalty='l2',\n         solver='lbfgs',\n         max_iter=1000,\n         class_weight='balanced'\n     ))\n])\n\n# Display the pipeline steps\npipeline\n</pre> # Import necessary libraries for TF-IDF and Logistic Regression from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.preprocessing import FunctionTransformer  # For demonstration, we define a very simple text preprocessor (could be extended) import re  def simple_preprocessor(text):     # Lowercase and remove non-alphabetic characters     text = text.lower()     text = re.sub(r'[^a-z\\s]', '', text)     return text  # Example hyperparameter settings for TfidfVectorizer tfidf_params = {     \"stop_words\": \"english\",           # Remove English stop words     \"tokenizer\": None,                   # Use default tokenizer; can be replaced by a custom one     \"analyzer\": \"word\",                # Analyze words (as opposed to characters)     \"min_df\": 5,                         # Ignore terms that appear in fewer than 5 documents     \"max_df\": 0.8,                       # Ignore terms that appear in more than 80% of the documents     \"ngram_range\": (1, 1),               # Unigrams by default (can change to (1,2) for bigrams, etc.)     \"max_features\": 10000                # Limit vocabulary size }  # Create the pipeline pipeline = Pipeline([     (\"preprocessor\", FunctionTransformer(lambda X: [simple_preprocessor(text) for text in X])),     (\"tfidf\", TfidfVectorizer(**tfidf_params)),     (\"logreg\", LogisticRegression(          # See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html          # Regularization: C is the inverse of regularization strength (smaller values specify stronger regularization)          # class_weight: can be set to 'balanced' to automatically adjust weights inversely proportional to class frequencies          C=1.0,          penalty='l2',          solver='lbfgs',          max_iter=1000,          class_weight='balanced'      )) ])  # Display the pipeline steps pipeline Out[2]: <pre>Pipeline(steps=[('preprocessor',\n                 FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x137e4d760&gt;)),\n                ('tfidf',\n                 TfidfVectorizer(max_df=0.8, max_features=10000, min_df=5,\n                                 stop_words='english')),\n                ('logreg',\n                 LogisticRegression(class_weight='balanced', max_iter=1000))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('preprocessor',\n                 FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x137e4d760&gt;)),\n                ('tfidf',\n                 TfidfVectorizer(max_df=0.8, max_features=10000, min_df=5,\n                                 stop_words='english')),\n                ('logreg',\n                 LogisticRegression(class_weight='balanced', max_iter=1000))])</pre> &lt;lambda&gt;FunctionTransformer?Documentation for FunctionTransformer<pre>FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x137e4d760&gt;)</pre> TfidfVectorizer?Documentation for TfidfVectorizer<pre>TfidfVectorizer(max_df=0.8, max_features=10000, min_df=5, stop_words='english')</pre> LogisticRegression?Documentation for LogisticRegression<pre>LogisticRegression(class_weight='balanced', max_iter=1000)</pre> In\u00a0[3]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Using built-in stop words for English\nvectorizer1 = TfidfVectorizer(stop_words=\"english\")\nvectorizer2 = TfidfVectorizer(stop_words=None)\n\ndocuments = [\"This is a sample document.\", \"Another document with more text.\"]\n\ntfidf_matrix1 = vectorizer1.fit_transform(documents)\ntfidf_matrix2 = vectorizer2.fit_transform(documents)\n\nprint(\"Features with stop_words='english':\", vectorizer1.get_feature_names_out())\nprint(\"Features without stop_words:\", vectorizer2.get_feature_names_out())\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer  # Using built-in stop words for English vectorizer1 = TfidfVectorizer(stop_words=\"english\") vectorizer2 = TfidfVectorizer(stop_words=None)  documents = [\"This is a sample document.\", \"Another document with more text.\"]  tfidf_matrix1 = vectorizer1.fit_transform(documents) tfidf_matrix2 = vectorizer2.fit_transform(documents)  print(\"Features with stop_words='english':\", vectorizer1.get_feature_names_out()) print(\"Features without stop_words:\", vectorizer2.get_feature_names_out()) <pre>Features with stop_words='english': ['document' 'sample' 'text']\nFeatures without stop_words: ['another' 'document' 'is' 'more' 'sample' 'text' 'this' 'with']\n</pre> In\u00a0[4]: Copied! <pre>def custom_tokenizer(text):\n    # Remove punctuation and split by whitespace\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text.lower().split()\n\nvectorizer1 = TfidfVectorizer(tokenizer=custom_tokenizer)\nvectorizer2 = TfidfVectorizer(tokenizer=None)\n\ndocuments = [\"Hello, world! This is an example. I'm happy to see you.\", \"Custom tokenizer works well.\"]\n\ntfidf_matrix1 = vectorizer1.fit_transform(documents)\ntfidf_matrix2 = vectorizer2.fit_transform(documents)\n\nprint(\"Tokens using custom tokenizer:\", vectorizer1.get_feature_names_out())\nprint(\"Tokens without custom tokenizer:\", vectorizer2.get_feature_names_out())\n</pre> def custom_tokenizer(text):     # Remove punctuation and split by whitespace     text = re.sub(r'[^\\w\\s]', '', text)     return text.lower().split()  vectorizer1 = TfidfVectorizer(tokenizer=custom_tokenizer) vectorizer2 = TfidfVectorizer(tokenizer=None)  documents = [\"Hello, world! This is an example. I'm happy to see you.\", \"Custom tokenizer works well.\"]  tfidf_matrix1 = vectorizer1.fit_transform(documents) tfidf_matrix2 = vectorizer2.fit_transform(documents)  print(\"Tokens using custom tokenizer:\", vectorizer1.get_feature_names_out()) print(\"Tokens without custom tokenizer:\", vectorizer2.get_feature_names_out()) <pre>Tokens using custom tokenizer: ['an' 'custom' 'example' 'happy' 'hello' 'im' 'is' 'see' 'this' 'to'\n 'tokenizer' 'well' 'works' 'world' 'you']\nTokens without custom tokenizer: ['an' 'custom' 'example' 'happy' 'hello' 'is' 'see' 'this' 'to'\n 'tokenizer' 'well' 'works' 'world' 'you']\n</pre> <pre>/Users/agomberto/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n</pre> In\u00a0[5]: Copied! <pre># Use character analyzer (each character n-gram will be a feature)\nvectorizer1 = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4))\nvectorizer2 = TfidfVectorizer(analyzer=\"word\", ngram_range=(2, 4))\nvectorizer3 = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2, 4))\n\ndocuments = [\"This is a sample document.\", \"Another document with more text.\"]\n\ntfidf_matrix1 = vectorizer1.fit_transform(documents)\ntfidf_matrix2 = vectorizer2.fit_transform(documents)\ntfidf_matrix3 = vectorizer3.fit_transform(documents)\n\nprint(\"Character n-grams as features:\", vectorizer1.get_feature_names_out())\nprint(\"Character-word boundary n-grams as features:\", vectorizer3.get_feature_names_out())\nprint(\"Word n-grams as features:\", vectorizer2.get_feature_names_out())\n</pre> # Use character analyzer (each character n-gram will be a feature) vectorizer1 = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4)) vectorizer2 = TfidfVectorizer(analyzer=\"word\", ngram_range=(2, 4)) vectorizer3 = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2, 4))  documents = [\"This is a sample document.\", \"Another document with more text.\"]  tfidf_matrix1 = vectorizer1.fit_transform(documents) tfidf_matrix2 = vectorizer2.fit_transform(documents) tfidf_matrix3 = vectorizer3.fit_transform(documents)  print(\"Character n-grams as features:\", vectorizer1.get_feature_names_out()) print(\"Character-word boundary n-grams as features:\", vectorizer3.get_feature_names_out()) print(\"Word n-grams as features:\", vectorizer2.get_feature_names_out())  <pre>Character n-grams as features: [' a' ' a ' ' a s' ' d' ' do' ' doc' ' i' ' is' ' is ' ' m' ' mo' ' mor'\n ' s' ' sa' ' sam' ' t' ' te' ' tex' ' w' ' wi' ' wit' 'a ' 'a s' 'a sa'\n 'am' 'amp' 'ampl' 'an' 'ano' 'anot' 'cu' 'cum' 'cume' 'do' 'doc' 'docu'\n 'e ' 'e d' 'e do' 'e t' 'e te' 'en' 'ent' 'ent ' 'ent.' 'er' 'er ' 'er d'\n 'ex' 'ext' 'ext.' 'h ' 'h m' 'h mo' 'he' 'her' 'her ' 'hi' 'his' 'his '\n 'is' 'is ' 'is a' 'is i' 'it' 'ith' 'ith ' 'le' 'le ' 'le d' 'me' 'men'\n 'ment' 'mo' 'mor' 'more' 'mp' 'mpl' 'mple' 'no' 'not' 'noth' 'nt' 'nt '\n 'nt w' 'nt.' 'oc' 'ocu' 'ocum' 'or' 'ore' 'ore ' 'ot' 'oth' 'othe' 'pl'\n 'ple' 'ple ' 'r ' 'r d' 'r do' 're' 're ' 're t' 's ' 's a' 's a ' 's i'\n 's is' 'sa' 'sam' 'samp' 't ' 't w' 't wi' 't.' 'te' 'tex' 'text' 'th'\n 'th ' 'th m' 'the' 'ther' 'thi' 'this' 'um' 'ume' 'umen' 'wi' 'wit'\n 'with' 'xt' 'xt.']\nCharacter-word boundary n-grams as features: [' a' ' a ' ' an' ' ano' ' d' ' do' ' doc' ' i' ' is' ' is ' ' m' ' mo'\n ' mor' ' s' ' sa' ' sam' ' t' ' te' ' tex' ' th' ' thi' ' w' ' wi' ' wit'\n '. ' 'a ' 'am' 'amp' 'ampl' 'an' 'ano' 'anot' 'cu' 'cum' 'cume' 'do'\n 'doc' 'docu' 'e ' 'en' 'ent' 'ent ' 'ent.' 'er' 'er ' 'ex' 'ext' 'ext.'\n 'h ' 'he' 'her' 'her ' 'hi' 'his' 'his ' 'is' 'is ' 'it' 'ith' 'ith '\n 'le' 'le ' 'me' 'men' 'ment' 'mo' 'mor' 'more' 'mp' 'mpl' 'mple' 'no'\n 'not' 'noth' 'nt' 'nt ' 'nt.' 'nt. ' 'oc' 'ocu' 'ocum' 'or' 'ore' 'ore '\n 'ot' 'oth' 'othe' 'pl' 'ple' 'ple ' 'r ' 're' 're ' 's ' 'sa' 'sam'\n 'samp' 't ' 't.' 't. ' 'te' 'tex' 'text' 'th' 'th ' 'the' 'ther' 'thi'\n 'this' 'um' 'ume' 'umen' 'wi' 'wit' 'with' 'xt' 'xt.' 'xt. ']\nWord n-grams as features: ['another document' 'another document with' 'another document with more'\n 'document with' 'document with more' 'document with more text'\n 'is sample' 'is sample document' 'more text' 'sample document' 'this is'\n 'this is sample' 'this is sample document' 'with more' 'with more text']\n</pre> In\u00a0[6]: Copied! <pre>vectorizer0 = TfidfVectorizer()\nvectorizer1 = TfidfVectorizer(min_df=2)\nvectorizer2 = TfidfVectorizer(max_df=0.8)\n\ndocuments = [\"This is a sample document.\", \"Another document with more text.\", \"This is another document.\"]\n\n\ntfidf_matrix0 = vectorizer0.fit_transform(documents)\ntfidf_matrix1 = vectorizer1.fit_transform(documents)\ntfidf_matrix2 = vectorizer2.fit_transform(documents)\n\nprint(\"Features without min_df and max_df:\", vectorizer0.get_feature_names_out())\nprint(\"Features with min_df=2:\", vectorizer1.get_feature_names_out())\nprint(\"Features with max_df=0.8:\", vectorizer2.get_feature_names_out())\n</pre> vectorizer0 = TfidfVectorizer() vectorizer1 = TfidfVectorizer(min_df=2) vectorizer2 = TfidfVectorizer(max_df=0.8)  documents = [\"This is a sample document.\", \"Another document with more text.\", \"This is another document.\"]   tfidf_matrix0 = vectorizer0.fit_transform(documents) tfidf_matrix1 = vectorizer1.fit_transform(documents) tfidf_matrix2 = vectorizer2.fit_transform(documents)  print(\"Features without min_df and max_df:\", vectorizer0.get_feature_names_out()) print(\"Features with min_df=2:\", vectorizer1.get_feature_names_out()) print(\"Features with max_df=0.8:\", vectorizer2.get_feature_names_out()) <pre>Features without min_df and max_df: ['another' 'document' 'is' 'more' 'sample' 'text' 'this' 'with']\nFeatures with min_df=2: ['another' 'document' 'is' 'this']\nFeatures with max_df=0.8: ['another' 'is' 'more' 'sample' 'text' 'this' 'with']\n</pre> In\u00a0[7]: Copied! <pre>vectorizer1 = TfidfVectorizer(ngram_range=(1, 2))\nvectorizer2 = TfidfVectorizer(ngram_range=(1, 3))\n\ndocuments = [\"this is a test\", \"another test example\"]\n\ntfidf_matrix1 = vectorizer1.fit_transform(documents)\ntfidf_matrix2 = vectorizer2.fit_transform(documents)\n\nprint(\"Features with ngram_range=(1,2):\", vectorizer1.get_feature_names_out())\nprint(\"Features with ngram_range=(1,3):\", vectorizer2.get_feature_names_out())\n</pre> vectorizer1 = TfidfVectorizer(ngram_range=(1, 2)) vectorizer2 = TfidfVectorizer(ngram_range=(1, 3))  documents = [\"this is a test\", \"another test example\"]  tfidf_matrix1 = vectorizer1.fit_transform(documents) tfidf_matrix2 = vectorizer2.fit_transform(documents)  print(\"Features with ngram_range=(1,2):\", vectorizer1.get_feature_names_out()) print(\"Features with ngram_range=(1,3):\", vectorizer2.get_feature_names_out()) <pre>Features with ngram_range=(1,2): ['another' 'another test' 'example' 'is' 'is test' 'test' 'test example'\n 'this' 'this is']\nFeatures with ngram_range=(1,3): ['another' 'another test' 'another test example' 'example' 'is' 'is test'\n 'test' 'test example' 'this' 'this is' 'this is test']\n</pre> In\u00a0[8]: Copied! <pre>vectorizer = TfidfVectorizer(max_features=3)\ndocuments = [\"This is a document with a lot of words\", \"This is another document with a lot of words\"]\ntfidf_matrix = vectorizer.fit_transform(documents)\nprint(\"Features with max_features=3:\", vectorizer.get_feature_names_out())\n</pre> vectorizer = TfidfVectorizer(max_features=3) documents = [\"This is a document with a lot of words\", \"This is another document with a lot of words\"] tfidf_matrix = vectorizer.fit_transform(documents) print(\"Features with max_features=3:\", vectorizer.get_feature_names_out()) <pre>Features with max_features=3: ['document' 'is' 'lot']\n</pre> In\u00a0[9]: Copied! <pre>custom_vocab1 = {\"data\": 0, \"science\": 1, \"machine\": 2, \"learning\": 3}\ncustom_vocab2 = {\"data\": 0, \"science\": 1, \"machine\": 2, \"is\": 3, \"amazing\": 4}\n\nvectorizer1 = TfidfVectorizer(vocabulary=custom_vocab1)\nvectorizer2 = TfidfVectorizer(vocabulary=custom_vocab2)\n\ndocuments = [\"data science is amazing\", \"machine learning is part of data science\"]\n\ntfidf_matrix1 = vectorizer1.fit_transform(documents)\ntfidf_matrix2 = vectorizer2.fit_transform(documents)\n\nprint(\"Features using custom vocabulary:\", vectorizer1.get_feature_names_out())\nprint(\"Features using custom vocabulary:\", vectorizer2.get_feature_names_out())\n</pre> custom_vocab1 = {\"data\": 0, \"science\": 1, \"machine\": 2, \"learning\": 3} custom_vocab2 = {\"data\": 0, \"science\": 1, \"machine\": 2, \"is\": 3, \"amazing\": 4}  vectorizer1 = TfidfVectorizer(vocabulary=custom_vocab1) vectorizer2 = TfidfVectorizer(vocabulary=custom_vocab2)  documents = [\"data science is amazing\", \"machine learning is part of data science\"]  tfidf_matrix1 = vectorizer1.fit_transform(documents) tfidf_matrix2 = vectorizer2.fit_transform(documents)  print(\"Features using custom vocabulary:\", vectorizer1.get_feature_names_out()) print(\"Features using custom vocabulary:\", vectorizer2.get_feature_names_out()) <pre>Features using custom vocabulary: ['data' 'science' 'machine' 'learning']\nFeatures using custom vocabulary: ['data' 'science' 'machine' 'is' 'amazing']\n</pre> In\u00a0[10]: Copied! <pre># Train the model\npipeline.fit(train_df[\"text\"], train_df[\"label\"])\n\n# Evaluate on dev set\ndev_preds = pipeline.predict(dev_df[\"text\"])\ndev_accuracy = accuracy_score(dev_df[\"label\"], dev_preds)\ndev_precision = precision_score(dev_df[\"label\"], dev_preds)\ndev_recall = recall_score(dev_df[\"label\"], dev_preds)\ndev_f1 = f1_score(dev_df[\"label\"], dev_preds)\n\nprint(\"Dev Set Metrics:\")\nprint(f\"Accuracy: {dev_accuracy*100:.2f}%\")\nprint(f\"Precision: {dev_precision*100:.2f}%\")\nprint(f\"Recall: {dev_recall*100:.2f}%\")\nprint(f\"F1-score: {dev_f1*100:.2f}%\")\n\n# Evaluate on test set\ntest_preds = pipeline.predict(test_df[\"text\"])\ntest_accuracy = accuracy_score(test_df[\"label\"], test_preds)\ntest_precision = precision_score(test_df[\"label\"], test_preds)\ntest_recall = recall_score(test_df[\"label\"], test_preds)\ntest_f1 = f1_score(test_df[\"label\"], test_preds)\n\nprint(\"\\nTest Set Metrics:\")\nprint(f\"Accuracy: {test_accuracy*100:.2f}%\")\nprint(f\"Precision: {test_precision*100:.2f}%\")\nprint(f\"Recall: {test_recall*100:.2f}%\")\nprint(f\"F1-score: {test_f1*100:.2f}%\")\n</pre> # Train the model pipeline.fit(train_df[\"text\"], train_df[\"label\"])  # Evaluate on dev set dev_preds = pipeline.predict(dev_df[\"text\"]) dev_accuracy = accuracy_score(dev_df[\"label\"], dev_preds) dev_precision = precision_score(dev_df[\"label\"], dev_preds) dev_recall = recall_score(dev_df[\"label\"], dev_preds) dev_f1 = f1_score(dev_df[\"label\"], dev_preds)  print(\"Dev Set Metrics:\") print(f\"Accuracy: {dev_accuracy*100:.2f}%\") print(f\"Precision: {dev_precision*100:.2f}%\") print(f\"Recall: {dev_recall*100:.2f}%\") print(f\"F1-score: {dev_f1*100:.2f}%\")  # Evaluate on test set test_preds = pipeline.predict(test_df[\"text\"]) test_accuracy = accuracy_score(test_df[\"label\"], test_preds) test_precision = precision_score(test_df[\"label\"], test_preds) test_recall = recall_score(test_df[\"label\"], test_preds) test_f1 = f1_score(test_df[\"label\"], test_preds)  print(\"\\nTest Set Metrics:\") print(f\"Accuracy: {test_accuracy*100:.2f}%\") print(f\"Precision: {test_precision*100:.2f}%\") print(f\"Recall: {test_recall*100:.2f}%\") print(f\"F1-score: {test_f1*100:.2f}%\") <pre>Dev Set Metrics:\nAccuracy: 85.31%\nPrecision: 84.16%\nRecall: 87.00%\nF1-score: 85.56%\n\nTest Set Metrics:\nAccuracy: 85.23%\nPrecision: 83.31%\nRecall: 86.71%\nF1-score: 84.98%\n</pre> <p>Results are quite good if we compare to the random baseline (50% F1). We outperform the baseline by a large margin. We also see that the results of precision and recall are quite good around 85% without a clear decrepancy between the two.</p> <p>Let's look at the feature importance analysis to understand which tokens drive the predictions and maybe find some interesting patterns that will help us improve the pipeline.</p> In\u00a0[11]: Copied! <pre>from IPython.display import display, HTML\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\n# Get the trained Logistic Regression model from the pipeline\nlogreg = pipeline.named_steps['logreg']\ntfidf = pipeline.named_steps['tfidf']\n\n# Get feature names (tokens)\nfeature_names = np.array(tfidf.get_feature_names_out())\n\n# Logistic Regression coefficients\ncoefficients = logreg.coef_[0]\n\n# a. Contribution of each token\n# Create a DataFrame to display tokens and their coefficients\ncoef_df = pd.DataFrame({\n    'token': feature_names,\n    'coefficient': coefficients\n})\n\n# b. Top tokens for positive (assumed label 1) and negative (assumed label 0) sentiment\ntop_positive = coef_df.sort_values(by='coefficient', ascending=False).head(20)\ntop_negative = coef_df.sort_values(by='coefficient').head(20)\n\n#Make an horizontal plot of the top tokens for positive and negative sentiment\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\nax[0].barh(top_positive['token'], top_positive['coefficient'], color='green')\nax[1].barh(top_negative['token'], top_negative['coefficient'], color='red')\nax[0].set_xlabel('Coefficient')\nax[0].set_ylabel('Tokens')\nax[0].set_title('Top Tokens for Positive Sentiment')\nax[1].set_xlabel('Coefficient')\nax[1].set_ylabel('Tokens')\nax[1].set_title('Top Tokens for Negative Sentiment')\n\nplt.tight_layout()\nplt.show()\n</pre> from IPython.display import display, HTML from matplotlib import pyplot as plt import pandas as pd  # Get the trained Logistic Regression model from the pipeline logreg = pipeline.named_steps['logreg'] tfidf = pipeline.named_steps['tfidf']  # Get feature names (tokens) feature_names = np.array(tfidf.get_feature_names_out())  # Logistic Regression coefficients coefficients = logreg.coef_[0]  # a. Contribution of each token # Create a DataFrame to display tokens and their coefficients coef_df = pd.DataFrame({     'token': feature_names,     'coefficient': coefficients })  # b. Top tokens for positive (assumed label 1) and negative (assumed label 0) sentiment top_positive = coef_df.sort_values(by='coefficient', ascending=False).head(20) top_negative = coef_df.sort_values(by='coefficient').head(20)  #Make an horizontal plot of the top tokens for positive and negative sentiment fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))  ax[0].barh(top_positive['token'], top_positive['coefficient'], color='green') ax[1].barh(top_negative['token'], top_negative['coefficient'], color='red') ax[0].set_xlabel('Coefficient') ax[0].set_ylabel('Tokens') ax[0].set_title('Top Tokens for Positive Sentiment') ax[1].set_xlabel('Coefficient') ax[1].set_ylabel('Tokens') ax[1].set_title('Top Tokens for Negative Sentiment')  plt.tight_layout() plt.show()  <p>We see interesting patterns here. For positive label, the top tokens are mostly related to sentiment. Words like \"great\", \"wonderful\", \"excellent\", \"fantastic\" are all positive words. For negative label, the top tokens are mostly related to sentiment. Words like \"bad\", \"terrible\", \"horrible\", \"awful\" are all negative words.</p> <p>Nevertheless, we see some tokens that are not related to sentiment but are still important for the classification. For example, \"today\" or \"script\" are important for the classification but they are not related to sentiment. It looks like the model overfits on those tokens because they may be present too many times in the corpus for positive or negative reviews.</p> <p>Let's check that hypothesis by looking at the probability to have a positive or negative review depending on the presence of a token.</p> In\u00a0[12]: Copied! <pre># Get the TF-IDF vectorizer from the pipeline\ntfidf = pipeline.named_steps['tfidf']\n\n# Get the vocabulary and feature names\nvocabulary = tfidf.vocabulary_\nfeature_names = tfidf.get_feature_names_out()\n\n# We just want to know if a word appears or not\n# Transform the training data\nX_count = tfidf.transform(train_df[\"text\"])\n\n# Convert to binary (1 if word appears, 0 if not)\nX_binary = (X_count &gt; 0).astype(int)\n\n# Get the labels\ny = train_df[\"label\"].values\n\n# Calculate probabilities\nword_sentiment_probs = {}\n\n# Total counts for each sentiment\npositive_count = np.sum(y == 1)\nnegative_count = np.sum(y == 0)\ntotal_count = len(y)\n\n# For each word in the vocabulary\nfor i, word in enumerate(feature_names):\n    # Get documents containing this word\n    docs_with_word = X_binary[:, i].toarray().flatten()\n    \n    # Count documents with this word for each sentiment\n    positive_with_word = np.sum(docs_with_word &amp; (y == 1))\n    negative_with_word = np.sum(docs_with_word &amp; (y == 0))\n    total_with_word = np.sum(docs_with_word)\n    \n    if total_with_word &gt; 0:  # Avoid division by zero\n        # P(positive | word)\n        p_positive_given_word = positive_with_word / total_with_word\n        \n        # P(negative | word)\n        p_negative_given_word = negative_with_word / total_with_word\n        \n        # Store the probabilities\n        word_sentiment_probs[word] = {\n            'P(positive|word)': p_positive_given_word,\n            'P(negative|word)': p_negative_given_word,\n            'count': total_with_word,\n            'positive_count': positive_with_word,\n            'negative_count': negative_with_word\n        }\n\n# Convert to DataFrame for easier analysis\nprobs_df = pd.DataFrame.from_dict(word_sentiment_probs, orient='index')\n\n# Sort by probability of positive sentiment\nmost_positive_words = probs_df.sort_values(by='P(positive|word)', ascending=False).head(20)\nmost_negative_words = probs_df.sort_values(by='P(negative|word)', ascending=False).head(20)\n\n# Display results\nprint(\"Words most associated with positive sentiment:\")\nprint(most_positive_words[['P(positive|word)', 'count']])\n\nprint(\"\\nWords most associated with negative sentiment:\")\nprint(most_negative_words[['P(negative|word)', 'count']])\n</pre> # Get the TF-IDF vectorizer from the pipeline tfidf = pipeline.named_steps['tfidf']  # Get the vocabulary and feature names vocabulary = tfidf.vocabulary_ feature_names = tfidf.get_feature_names_out()  # We just want to know if a word appears or not # Transform the training data X_count = tfidf.transform(train_df[\"text\"])  # Convert to binary (1 if word appears, 0 if not) X_binary = (X_count &gt; 0).astype(int)  # Get the labels y = train_df[\"label\"].values  # Calculate probabilities word_sentiment_probs = {}  # Total counts for each sentiment positive_count = np.sum(y == 1) negative_count = np.sum(y == 0) total_count = len(y)  # For each word in the vocabulary for i, word in enumerate(feature_names):     # Get documents containing this word     docs_with_word = X_binary[:, i].toarray().flatten()          # Count documents with this word for each sentiment     positive_with_word = np.sum(docs_with_word &amp; (y == 1))     negative_with_word = np.sum(docs_with_word &amp; (y == 0))     total_with_word = np.sum(docs_with_word)          if total_with_word &gt; 0:  # Avoid division by zero         # P(positive | word)         p_positive_given_word = positive_with_word / total_with_word                  # P(negative | word)         p_negative_given_word = negative_with_word / total_with_word                  # Store the probabilities         word_sentiment_probs[word] = {             'P(positive|word)': p_positive_given_word,             'P(negative|word)': p_negative_given_word,             'count': total_with_word,             'positive_count': positive_with_word,             'negative_count': negative_with_word         }  # Convert to DataFrame for easier analysis probs_df = pd.DataFrame.from_dict(word_sentiment_probs, orient='index')  # Sort by probability of positive sentiment most_positive_words = probs_df.sort_values(by='P(positive|word)', ascending=False).head(20) most_negative_words = probs_df.sort_values(by='P(negative|word)', ascending=False).head(20)  # Display results print(\"Words most associated with positive sentiment:\") print(most_positive_words[['P(positive|word)', 'count']])  print(\"\\nWords most associated with negative sentiment:\") print(most_negative_words[['P(negative|word)', 'count']]) <pre>Words most associated with positive sentiment:\n              P(positive|word)  count\nheshe                      1.0      1\nebert                      1.0     11\njonestown                  1.0      6\nelectrifying               1.0      8\nundying                    1.0      6\nshintaro                   1.0      6\nextravagant                1.0      7\naskey                      1.0      6\ngovernments                1.0      7\nsorrow                     1.0     11\ncurr                       1.0      5\njohansson                  1.0      9\nverhoevens                 1.0      1\njoes                       1.0      1\nceleste                    1.0      6\nstewarts                   1.0      1\nelviras                    1.0      1\nmyrna                      1.0      6\nstevenson                  1.0      7\ntucker                     1.0      6\n\nWords most associated with negative sentiment:\n            P(negative|word)  count\noneliners                1.0      1\nhavent                   1.0      1\nheros                    1.0      1\ngiamatti                 1.0      7\nta                       1.0      3\nohara                    1.0      1\nheather                  1.0      5\nschindlers               1.0      1\ncorleone                 1.0      5\nhg                       1.0      3\nwandered                 1.0      7\ngiggle                   1.0      9\nkareena                  1.0      6\nstandup                  1.0      2\nmishmash                 1.0      3\ngodawful                 1.0      4\nds                       1.0      2\nbiehn                    1.0      6\nretirement               1.0      7\nweaker                   1.0     11\n</pre> In\u00a0[13]: Copied! <pre>word_sentiment_probs[\"today\"]\n</pre> word_sentiment_probs[\"today\"] Out[13]: <pre>{'P(positive|word)': np.float64(0.7734375),\n 'P(negative|word)': np.float64(0.2265625),\n 'count': np.int64(256),\n 'positive_count': np.int64(198),\n 'negative_count': np.int64(58)}</pre> In\u00a0[14]: Copied! <pre>word_sentiment_probs[\"movie\"]\n</pre> word_sentiment_probs[\"movie\"] Out[14]: <pre>{'P(positive|word)': np.float64(0.4669042769857434),\n 'P(negative|word)': np.float64(0.5330957230142567),\n 'count': np.int64(3928),\n 'positive_count': np.int64(1834),\n 'negative_count': np.int64(2094)}</pre> In\u00a0[15]: Copied! <pre>word_sentiment_probs[\"film\"]\n</pre> word_sentiment_probs[\"film\"] Out[15]: <pre>{'P(positive|word)': np.float64(0.49778024417314093),\n 'P(negative|word)': np.float64(0.5022197558268591),\n 'count': np.int64(3604),\n 'positive_count': np.int64(1794),\n 'negative_count': np.int64(1810)}</pre> <p>Here we see a lot of words that may not be related to sentiment but are still important for the classification. For example, \"tate\" or \"joss\". And we look at the exemple of \"today\" that we saw before, the probability to have a positive or negative review depending on the presence of \"today\" is clearly skewed towards positive reviews. There may be a reason, but this word without context, is not a good feature for the classification. Therefore maybe we should remove those words from the vocabulary or use them only with more context ie with higher ngrams.</p> <p>Let's see another way of looking at the feature importance by highlighting the words in some example reviews.</p> In\u00a0[16]: Copied! <pre># c. Example: Highlight words in a review (for illustration, using HTML styling)\n# Let's put intensity of the color to the coefficient of the token\n# For instance, if the coefficient is high, the color should be more intense\ndef highlight_review(review, threshold=0.5):\n    tokens = review.split()\n    highlighted = []\n    for token in tokens:\n        token_clean = token.lower()\n        if token_clean in feature_names:\n            # Find index of token in the vocabulary\n            idx = np.where(feature_names == token_clean)[0][0]\n            coef = coefficients[idx]\n            # Color positive words in green and negative in red\n            if coef &gt; threshold:\n                token = f'&lt;span style=\"color:green\"&gt;{token}&lt;/span&gt;'\n            elif coef &lt; -threshold:\n                token = f'&lt;span style=\"color:red\"&gt;{token}&lt;/span&gt;'\n        highlighted.append(token)\n    return ' '.join(highlighted)\n\n# Show highlighted review example (use IPython.display to render HTML)\nfrom IPython.display import display, HTML\n\n\nfor i in range(10):\n    sample_review = test_df['text'].iloc[i]\n    display(HTML(highlight_review(sample_review, threshold=0.1)))\n    print('--------------------------------')\n</pre> # c. Example: Highlight words in a review (for illustration, using HTML styling) # Let's put intensity of the color to the coefficient of the token # For instance, if the coefficient is high, the color should be more intense def highlight_review(review, threshold=0.5):     tokens = review.split()     highlighted = []     for token in tokens:         token_clean = token.lower()         if token_clean in feature_names:             # Find index of token in the vocabulary             idx = np.where(feature_names == token_clean)[0][0]             coef = coefficients[idx]             # Color positive words in green and negative in red             if coef &gt; threshold:                 token = f'{token}'             elif coef &lt; -threshold:                 token = f'{token}'         highlighted.append(token)     return ' '.join(highlighted)  # Show highlighted review example (use IPython.display to render HTML) from IPython.display import display, HTML   for i in range(10):     sample_review = test_df['text'].iloc[i]     display(HTML(highlight_review(sample_review, threshold=0.1)))     print('--------------------------------')  I could not believe how terrible and boring this Hollywood remake was.It's so dreadful. It easily lands a place in my top 10 worst films of 1998.About the only thing it had going for it was Bruce Willis,who should stick to action films,as a completely emotionless killer who'd kill his own mother for the right price.But I'd rather listen to Robbie Coltraine talk American for a week than listen to Richard Gere's nauseating Irish accent again.But this film is also implausible,unconvincing,uneven,unexciting,unimpressive and lands Sidney Poiter in a rubbish role to make a possible career comeback.One for filmroll-footie purposes entirely.  <pre>--------------------------------\n</pre>  I rented Boogie Nights last week and I could tell you, when I watched the film I had a blast. If you think that when you watch the film you will get sicked by the porn. I mean yes, if your not a porn person who can't bother being by it, than this isn't the film to see. But the thing is, the whole film isn't really about porn. Well halfway through the film is about the porn industry but the other half is about the character development and the bad situations these characters go through. The actors played there roles perfect, especially Mark Wahlberg, John C. Reilly, and William H. Macy. The sex scenes, of course are terrific but mainly focus on the character's hype in porn films until there struggles. Excellent film, one of the best! Hedeen's Outlook: 10/10 **** A+  <pre>--------------------------------\n</pre>  First off, this movie is not near complete, my guess is that someone actually bothered to steal every other page of the script.The movie contains bizarre time-travels without notice, inconsistent dialogs, misplaced details all over, the music isn't very bad at all, other then misplaced tracks, and besides the fact that the volume goes up and down between the different tracks. The cutting-room did a descent job actually, and that says a lot. Missplaced sound effects ruin the tension, though.Luke Perry does what he does best, just looking worried, and occasionally coughing up punchlines from hell.I seriously rate this movie as the worst of 2007, and i've seen a few bad ones. Do not spend money on this one, it's not so bad it's a laugh, it's worse. Ratings above 1 star, should render a blacklist at IMDb, because it's a damn lie.  <pre>--------------------------------\n</pre>  I watched this mini in the early eighties. Sam Waterson proved himself to be a great actor. In fact when he began Law and Order I was disappointed in him as it was not as powerful a role. Unfortunately the good roles do not pay the bills. I wish I could find a copy of this rare series and review it. It is both factual and entertaining. Everyone should see it to know what really happened. I was so moved I purchased and read the book \"Pppenheimer-Shatterer of Worlds\". And saw how this man became an unlikely hero who was never rewarded for his insight. If you get a chance be sure to watch this movie and see what a performance Mr. Waterston can really provide an audience. Enjoy the movies!  <pre>--------------------------------\n</pre>  This movie was never intended as a big-budget film but was a cute little picture that pretty much anyone could enjoy. It probably won't change your life, but it is certainly charming and engaging.Clifton Webb plays a curmudgeon (that's certainly not new) who has a TV. However, his ratings are failing and he is worried about cancellation. So he decides maybe he is too out of touch with kids--as he and his wife have none of their own. So, he volunteers as a scoutmaster and regrets doing this almost immediately! Remember, he IS a curmudgeon and doesn't particularly like kids. To make things worse, one of the kids really likes him and follows him like a lost puppy. No matter how indifferently he acts towards the kid, the child just wants to spend time with him! The kid is cute and nearly steals the show all by himself! What happens next and the twists and turns of the movie are something you'll just have to find out for yourself. Understand that this is a light, cute and yet not cloying movie you'll probably enjoy.  <pre>--------------------------------\n</pre>  I thought this was an extremely bad movie. The whole time I was watching this movie I couldn't help but think over and over how bad it is, and how that was $3.69 down the drain. The plot was so jumpy. They did an excellent job at the beginning of explaining who dated who in high school, but they never really explained anything after that. Was it a supernatural thriller? Was it a regular thriller? Apparently you can decide for yourself, because they didn't see the need to explain. I understood basically what happened, I think. What I got confused about was all of it prior, what was the deal with the bloody noses, phone calls, etc.? Was this guy coming back? Was the wife channeling \"Carrie\" or something? Who knows? You certainly won't after watching this movie.  <pre>--------------------------------\n</pre>  This was one of the biggest pieces of crap I have ever had to watch. I mean, seriously. How would anybody else feel if they were in Woody Harrelson's shoes and your wife was even CONSIDERING it would be a good idea to sleep with the other guy even for a million bucks. After all, she was the one talking about it in bed and saying how it would be good for them since he can build his house or whatever with that money. Woody never fully agreed to it until she talked him into it. How CAN you trust her? Who the hell would actually even consider that if they were married? I don't care how desperate they were. That's the most ridiculous thing I have ever heard in my life. Then, he flips out on her. Apparently, he had no right to mistrust her, other than the fact that his wife just slept with another dude who is extremely rich and handsome. Oh and wait, then he's supposed to apologize to HER after she files for the divorce so she can be with the guy she slept with. Of course Woody has no right to say anything to her or mistrust her especially after she still has Roy Hobb's card in her wallet. Then, at the end of the movie, she's apparently so in love with Woody still and misses him so much, that she was not going to leave Hobbs until he made some ridiculously stupid story up to try to hint to her to leave, and she bleeping thanks Hobbs???? Are you bleeping kidding me? Was she under contract as his sex slave or something?? I mean what the bleep?? Oh and wait it gets better. She bleeping kisses him passionately before she gets out of the car. Yea, she's not a whore. Oh, thank you for letting me go, let me go make out with you one last time for good ole' sake. Smooch smooch, smooch even though I'm still married to a guy I left for a rich guy. I have never seen such a piece of crap in my life. How the hell are we supposed to feel good after that horrible ending? What was this movie supposed to represent? NOTHING CAME OUT OF THIS! This was the most pointless movie I have ever seen in my life. Two pathetic desperate people. If I were Woody, I would tell her to go drown herself in that body of water they were near. Apparently, he had no self respect. What the hell was Roy Hobbs thinking by taking this horrible role. I feel like puking after watching this. This movie was so bad, it was seriously laughable. I want those two hours of my life back that I wasted watching this piece of ****.  <pre>--------------------------------\n</pre>  I just watched it for the second time today and I must say with all my heart it is about damn time they made a movie about us as people not as spiritual beings. Such a waste of human life as this story was maybe some good will come out of it. And Eric is hotter than ever. To often in the movies First Nations people are seen as other than everyday people. We are always portrayed as chiefs or medicine people. Hey we are just like everyone else. And this movie showed just that. We hurt when an injustice is done and we can win in quest for justice. It is really to bad that the big movie companies cant see that. I cant wait till this comes out on DVD. Thanks to those who chose to show this story as it really was.  <pre>--------------------------------\n</pre>  I've read through a lot of the comments here about how this movie sticks to the book.. I don't think any of them have actually read it. Edgar Rice wrote about a dangerous African Jungle and Apes were killers and hunters. We know differently now and this movie portrays Apes in a more modern view. I've never seen a Tarzan movie that even comes close to Edgar's vision. Maybe one day Hollywood with trust talented and respected authors to tell the story. So, if you've never read the book and enjoy a good story about feelings and a fluffy bunny view of wild animals, maybe a good cry, see the movie. I hope John Carter of Mars get's more respect than Tarzan has. We miss ya, Edgar!  <pre>--------------------------------\n</pre>  I usually much prefer French movies over American ones, with explosions and car chases, but this movie was very disappointing. There is no way to write a spoiler because nothing really happens. This French couple has been living in Lisbon for years, and they return to Paris for a friend's wedding. They announce to another friend they are having dinner with that they are going to split. Then nothing much happens, they don't seem to know whether they want to separate or not. I don't necessarily think that their hesitations make for a bad movie, it is very human to hesitate before making such a decision for good, but this could be treated in an interesting manner, giving some flesh to their desires and their relationship, but that does not happen. One gets out of the theater unsure of why these two got together or want to split. The only piece I enjoyed was the conversation with the drunk. That was true to life.  <pre>--------------------------------\n</pre> <p>This is quit interesting to see the results as  we see words like \"American\", \"think\" or \"right\" which are highlighted in red and words like \"maybe\", \"tell\" or \"film\", that are highlighted in green. WE see also a lot of names.</p> <p>But it looks weird as those words are not related to sentiment. It looks like the model overfits on those tokens.</p> <p>One potential solution for this problem would be to consider only bigrams or trigrams as features just to increase the context of the tokens. One way would be to use the <code>ngram_range</code> parameter in the TF-IDF vectorizer. We could also increase the <code>min_df</code> parameter to remove words that are not present in enough documents and reduce words that are present in too many documents such as \"film\" or \"movie\".</p> <p>Let's try to see if this hypothesis is correct by running the pipeline with different stop words. Also we see that the word \"like\" is in red generally, maybe we should use it with more context ? because like and don't like would dramatically change the sentiment of the review.</p> In\u00a0[17]: Copied! <pre>from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nnew_stop_words = [\"film\", \"movie\", \"american\", \"think\", \"right\", \"maybe\", \"tell\", \n                  \"couple\", \"want\", \"gets\", \"get\", \"john\", \"carter\", \"rice\", \"day\", \"apes\",\n                  \"say\"]\n\nfor stop_words in [ENGLISH_STOP_WORDS, ENGLISH_STOP_WORDS.union(new_stop_words)]:\n    print(f\"\\nEvaluating with stop_words = {stop_words}\")\n    tfidf_params[\"stop_words\"] = list(stop_words)\n    pipeline.set_params(tfidf=TfidfVectorizer(**tfidf_params))\n    pipeline.fit(train_df[\"text\"], train_df[\"label\"])\n    preds = pipeline.predict(dev_df[\"text\"])\n    acc = accuracy_score(dev_df[\"label\"], preds)\n    precision = precision_score(dev_df[\"label\"], preds)\n    recall = recall_score(dev_df[\"label\"], preds)\n    f1 = f1_score(dev_df[\"label\"], preds)\n    print(f\"Dev Accuracy: {acc*100:.2f}%\")\n    print(f\"Dev Precision: {precision*100:.2f}%\")\n    print(f\"Dev Recall: {recall*100:.2f}%\")\n    print(f\"Dev F1-score: {f1*100:.2f}%\")\n</pre> from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS  new_stop_words = [\"film\", \"movie\", \"american\", \"think\", \"right\", \"maybe\", \"tell\",                    \"couple\", \"want\", \"gets\", \"get\", \"john\", \"carter\", \"rice\", \"day\", \"apes\",                   \"say\"]  for stop_words in [ENGLISH_STOP_WORDS, ENGLISH_STOP_WORDS.union(new_stop_words)]:     print(f\"\\nEvaluating with stop_words = {stop_words}\")     tfidf_params[\"stop_words\"] = list(stop_words)     pipeline.set_params(tfidf=TfidfVectorizer(**tfidf_params))     pipeline.fit(train_df[\"text\"], train_df[\"label\"])     preds = pipeline.predict(dev_df[\"text\"])     acc = accuracy_score(dev_df[\"label\"], preds)     precision = precision_score(dev_df[\"label\"], preds)     recall = recall_score(dev_df[\"label\"], preds)     f1 = f1_score(dev_df[\"label\"], preds)     print(f\"Dev Accuracy: {acc*100:.2f}%\")     print(f\"Dev Precision: {precision*100:.2f}%\")     print(f\"Dev Recall: {recall*100:.2f}%\")     print(f\"Dev F1-score: {f1*100:.2f}%\")  <pre>\nEvaluating with stop_words = frozenset({'ever', 'show', 'themselves', 'done', 'who', 'all', 'still', 'will', 'etc', 'seemed', 'put', 'had', 'cant', 'something', 'hereby', 'has', 'or', 'nothing', 'anywhere', 'become', 'third', 'any', 'a', 'she', 'once', 'whatever', 'is', 'somewhere', 'over', 'from', 'already', 'amount', 'been', 'throughout', 'only', 'onto', 'they', 'being', 'con', 'her', 'hasnt', 'find', 'but', 'why', 'it', 'how', 'describe', 'everything', 'mostly', 'ours', 'forty', 're', 'can', 'less', 'wherever', 'some', 'when', 'such', 'now', 'might', 'below', 'although', 'seems', 'nowhere', 'hereafter', 'if', 'sixty', 'hers', 'up', 'during', 'perhaps', 'amongst', 'except', 'ie', 'more', 'most', 'bill', 'yet', 'latter', 'whither', 'eight', 'same', 'do', 'this', 'becomes', 'herself', 'beforehand', 'sometime', 'under', 'your', 'he', 'together', 'go', 'enough', 'nor', 'thence', 'therefore', 'between', 'anyone', 'namely', 'not', 'former', 'besides', 'that', 'nine', 'those', 'thus', 'yourselves', 'too', 'hereupon', 'cannot', 'either', 'us', 'what', 'often', 'much', 'fire', 'empty', 'beside', 'i', 'ltd', 'his', 'part', 'amoungst', 'around', 'seeming', 'name', 'take', 'down', 'were', 'their', 'please', 'several', 'about', 'six', 'one', 'interest', 'very', 'sometimes', 'me', 'was', 'further', 'never', 'alone', 'becoming', 'others', 'my', 'thru', 'made', 'whether', 'formerly', 'eg', 'call', 'have', 'ourselves', 'while', 'noone', 'himself', 'there', 'latterly', 'whence', 'seem', 'here', 'whom', 'nevertheless', 'then', 'next', 'before', 'few', 'hence', 'top', 'almost', 'became', 'bottom', 'twenty', 'whenever', 'even', 'whose', 'therein', 'myself', 'full', 'through', 'back', 'whoever', 'whereafter', 'thereupon', 'meanwhile', 'again', 'side', 'every', 'none', 'these', 'for', 'them', 'yourself', 'least', 'wherein', 'anything', 'should', 'whole', 'after', 'couldnt', 'two', 'beyond', 'move', 'you', 'because', 'among', 'sincere', 'keep', 'behind', 'herein', 'inc', 'across', 'moreover', 'so', 'elsewhere', 'yours', 'no', 'mine', 'get', 'in', 'thin', 'where', 'him', 'fifty', 'as', 'the', 'on', 'must', 'an', 'nobody', 'un', 'always', 'than', 'co', 'within', 'with', 'upon', 'ten', 'due', 'since', 'against', 'toward', 'see', 'five', 'thereby', 'of', 'whereas', 'at', 'someone', 'be', 'without', 'last', 'mill', 'above', 'serious', 'however', 'well', 'anyhow', 'many', 'thick', 'along', 'four', 'also', 'detail', 'its', 'neither', 'off', 'found', 'we', 'fifteen', 'into', 'via', 'else', 'our', 'other', 'and', 'anyway', 'whereby', 'itself', 'first', 'each', 'both', 'per', 'by', 'rather', 'front', 'eleven', 'fill', 'thereafter', 'everywhere', 'though', 'are', 'afterwards', 'cry', 'could', 'twelve', 'to', 'whereupon', 'towards', 'system', 'otherwise', 'am', 'which', 'until', 'somehow', 'another', 'indeed', 'everyone', 'give', 'hundred', 'three', 'would', 'own', 'may', 'de', 'out'})\nDev Accuracy: 85.31%\nDev Precision: 84.16%\nDev Recall: 87.00%\nDev F1-score: 85.56%\n\nEvaluating with stop_words = frozenset({'right', 'eg', 'call', 'tell', 'ever', 'show', 'themselves', 'done', 'who', 'have', 'all', 'ourselves', 'while', 'noone', 'still', 'himself', 'will', 'etc', 'there', 'seemed', 'latterly', 'whence', 'seem', 'put', 'here', 'whom', 'had', 'cant', 'nevertheless', 'then', 'next', 'before', 'few', 'hence', 'top', 'something', 'hereby', 'has', 'almost', 'or', 'became', 'nothing', 'bottom', 'twenty', 'anywhere', 'whenever', 'become', 'third', 'even', 'any', 'a', 'she', 'whose', 'therein', 'couple', 'american', 'myself', 'carter', 'film', 'full', 'through', 'back', 'once', 'whatever', 'is', 'somewhere', 'over', 'from', 'already', 'amount', 'been', 'movie', 'whoever', 'throughout', 'whereafter', 'thereupon', 'meanwhile', 'again', 'only', 'side', 'every', 'rice', 'onto', 'they', 'none', 'being', 'these', 'con', 'for', 'them', 'her', 'hasnt', 'find', 'yourself', 'least', 'wherein', 'but', 'why', 'anything', 'should', 'whole', 'after', 'it', 'couldnt', 'how', 'describe', 'everything', 'two', 'beyond', 'mostly', 'move', 'you', 'ours', 'because', 'forty', 're', 'among', 'can', 'sincere', 'keep', 'behind', 'herein', 'inc', 'less', 'across', 'moreover', 'wherever', 'some', 'when', 'so', 'such', 'elsewhere', 'yours', 'no', 'mine', 'get', 'in', 'thin', 'now', 'might', 'where', 'below', 'him', 'although', 'seems', 'nowhere', 'hereafter', 'if', 'sixty', 'fifty', 'hers', 'as', 'the', 'on', 'up', 'during', 'perhaps', 'amongst', 'except', 'must', 'an', 'nobody', 'un', 'always', 'ie', 'more', 'than', 'want', 'co', 'most', 'within', 'bill', 'with', 'upon', 'yet', 'latter', 'whither', 'ten', 'due', 'eight', 'same', 'since', 'against', 'do', 'this', 'toward', 'becomes', 'see', 'five', 'say', 'herself', 'beforehand', 'thereby', 'of', 'sometime', 'whereas', 'at', 'someone', 'be', 'under', 'your', 'without', 'he', 'last', 'together', 'go', 'mill', 'enough', 'apes', 'nor', 'above', 'serious', 'however', 'thence', 'therefore', 'well', 'between', 'anyhow', 'anyone', 'namely', 'not', 'former', 'many', 'thick', 'along', 'four', 'also', 'besides', 'that', 'detail', 'nine', 'its', 'those', 'neither', 'off', 'maybe', 'found', 'we', 'gets', 'thus', 'fifteen', 'yourselves', 'too', 'hereupon', 'into', 'cannot', 'via', 'else', 'our', 'other', 'either', 'and', 'anyway', 'us', 'what', 'whereby', 'often', 'itself', 'much', 'fire', 'first', 'each', 'empty', 'beside', 'both', 'think', 'i', 'ltd', 'per', 'his', 'by', 'part', 'rather', 'front', 'amoungst', 'around', 'eleven', 'fill', 'seeming', 'thereafter', 'everywhere', 'name', 'though', 'are', 'afterwards', 'take', 'down', 'day', 'were', 'their', 'please', 'several', 'about', 'six', 'one', 'cry', 'john', 'interest', 'could', 'twelve', 'to', 'whereupon', 'towards', 'system', 'very', 'otherwise', 'am', 'which', 'sometimes', 'me', 'was', 'further', 'until', 'never', 'somehow', 'alone', 'another', 'indeed', 'everyone', 'becoming', 'others', 'my', 'give', 'hundred', 'thru', 'made', 'whether', 'three', 'would', 'own', 'formerly', 'may', 'de', 'out'})\nDev Accuracy: 85.56%\nDev Precision: 84.40%\nDev Recall: 87.25%\nDev F1-score: 85.80%\n</pre> In\u00a0[18]: Copied! <pre>tfidf = pipeline.named_steps['tfidf']\nlogreg = pipeline.named_steps['logreg']\n\nvocabulary = tfidf.vocabulary_\nfeature_names = tfidf.get_feature_names_out()\ncoefficients = logreg.coef_[0]\n\ndef highlight_review(review, threshold=0.5):\n    tokens = review.split()\n    highlighted = []\n    for token in tokens:\n        token_clean = token.lower()\n        if token_clean in feature_names:\n            # Find index of token in the vocabulary\n            idx = np.where(feature_names == token_clean)[0][0]\n            coef = coefficients[idx]\n            # Color positive words in green and negative in red\n            if coef &gt; threshold:\n                token = f'&lt;span style=\"color:green\"&gt;{token}&lt;/span&gt;'\n            elif coef &lt; -threshold:\n                token = f'&lt;span style=\"color:red\"&gt;{token}&lt;/span&gt;'\n        highlighted.append(token)\n    return ' '.join(highlighted)\n\n\n\nfor i in range(10):\n    sample_review = test_df['text'].iloc[i]\n    display(HTML(highlight_review(sample_review, threshold=0.1)))\n    print('--------------------------------')\n</pre> tfidf = pipeline.named_steps['tfidf'] logreg = pipeline.named_steps['logreg']  vocabulary = tfidf.vocabulary_ feature_names = tfidf.get_feature_names_out() coefficients = logreg.coef_[0]  def highlight_review(review, threshold=0.5):     tokens = review.split()     highlighted = []     for token in tokens:         token_clean = token.lower()         if token_clean in feature_names:             # Find index of token in the vocabulary             idx = np.where(feature_names == token_clean)[0][0]             coef = coefficients[idx]             # Color positive words in green and negative in red             if coef &gt; threshold:                 token = f'{token}'             elif coef &lt; -threshold:                 token = f'{token}'         highlighted.append(token)     return ' '.join(highlighted)    for i in range(10):     sample_review = test_df['text'].iloc[i]     display(HTML(highlight_review(sample_review, threshold=0.1)))     print('--------------------------------')  I could not believe how terrible and boring this Hollywood remake was.It's so dreadful. It easily lands a place in my top 10 worst films of 1998.About the only thing it had going for it was Bruce Willis,who should stick to action films,as a completely emotionless killer who'd kill his own mother for the right price.But I'd rather listen to Robbie Coltraine talk American for a week than listen to Richard Gere's nauseating Irish accent again.But this film is also implausible,unconvincing,uneven,unexciting,unimpressive and lands Sidney Poiter in a rubbish role to make a possible career comeback.One for filmroll-footie purposes entirely.  <pre>--------------------------------\n</pre>  I rented Boogie Nights last week and I could tell you, when I watched the film I had a blast. If you think that when you watch the film you will get sicked by the porn. I mean yes, if your not a porn person who can't bother being by it, than this isn't the film to see. But the thing is, the whole film isn't really about porn. Well halfway through the film is about the porn industry but the other half is about the character development and the bad situations these characters go through. The actors played there roles perfect, especially Mark Wahlberg, John C. Reilly, and William H. Macy. The sex scenes, of course are terrific but mainly focus on the character's hype in porn films until there struggles. Excellent film, one of the best! Hedeen's Outlook: 10/10 **** A+  <pre>--------------------------------\n</pre>  First off, this movie is not near complete, my guess is that someone actually bothered to steal every other page of the script.The movie contains bizarre time-travels without notice, inconsistent dialogs, misplaced details all over, the music isn't very bad at all, other then misplaced tracks, and besides the fact that the volume goes up and down between the different tracks. The cutting-room did a descent job actually, and that says a lot. Missplaced sound effects ruin the tension, though.Luke Perry does what he does best, just looking worried, and occasionally coughing up punchlines from hell.I seriously rate this movie as the worst of 2007, and i've seen a few bad ones. Do not spend money on this one, it's not so bad it's a laugh, it's worse. Ratings above 1 star, should render a blacklist at IMDb, because it's a damn lie.  <pre>--------------------------------\n</pre>  I watched this mini in the early eighties. Sam Waterson proved himself to be a great actor. In fact when he began Law and Order I was disappointed in him as it was not as powerful a role. Unfortunately the good roles do not pay the bills. I wish I could find a copy of this rare series and review it. It is both factual and entertaining. Everyone should see it to know what really happened. I was so moved I purchased and read the book \"Pppenheimer-Shatterer of Worlds\". And saw how this man became an unlikely hero who was never rewarded for his insight. If you get a chance be sure to watch this movie and see what a performance Mr. Waterston can really provide an audience. Enjoy the movies!  <pre>--------------------------------\n</pre>  This movie was never intended as a big-budget film but was a cute little picture that pretty much anyone could enjoy. It probably won't change your life, but it is certainly charming and engaging.Clifton Webb plays a curmudgeon (that's certainly not new) who has a TV. However, his ratings are failing and he is worried about cancellation. So he decides maybe he is too out of touch with kids--as he and his wife have none of their own. So, he volunteers as a scoutmaster and regrets doing this almost immediately! Remember, he IS a curmudgeon and doesn't particularly like kids. To make things worse, one of the kids really likes him and follows him like a lost puppy. No matter how indifferently he acts towards the kid, the child just wants to spend time with him! The kid is cute and nearly steals the show all by himself! What happens next and the twists and turns of the movie are something you'll just have to find out for yourself. Understand that this is a light, cute and yet not cloying movie you'll probably enjoy.  <pre>--------------------------------\n</pre>  I thought this was an extremely bad movie. The whole time I was watching this movie I couldn't help but think over and over how bad it is, and how that was $3.69 down the drain. The plot was so jumpy. They did an excellent job at the beginning of explaining who dated who in high school, but they never really explained anything after that. Was it a supernatural thriller? Was it a regular thriller? Apparently you can decide for yourself, because they didn't see the need to explain. I understood basically what happened, I think. What I got confused about was all of it prior, what was the deal with the bloody noses, phone calls, etc.? Was this guy coming back? Was the wife channeling \"Carrie\" or something? Who knows? You certainly won't after watching this movie.  <pre>--------------------------------\n</pre>  This was one of the biggest pieces of crap I have ever had to watch. I mean, seriously. How would anybody else feel if they were in Woody Harrelson's shoes and your wife was even CONSIDERING it would be a good idea to sleep with the other guy even for a million bucks. After all, she was the one talking about it in bed and saying how it would be good for them since he can build his house or whatever with that money. Woody never fully agreed to it until she talked him into it. How CAN you trust her? Who the hell would actually even consider that if they were married? I don't care how desperate they were. That's the most ridiculous thing I have ever heard in my life. Then, he flips out on her. Apparently, he had no right to mistrust her, other than the fact that his wife just slept with another dude who is extremely rich and handsome. Oh and wait, then he's supposed to apologize to HER after she files for the divorce so she can be with the guy she slept with. Of course Woody has no right to say anything to her or mistrust her especially after she still has Roy Hobb's card in her wallet. Then, at the end of the movie, she's apparently so in love with Woody still and misses him so much, that she was not going to leave Hobbs until he made some ridiculously stupid story up to try to hint to her to leave, and she bleeping thanks Hobbs???? Are you bleeping kidding me? Was she under contract as his sex slave or something?? I mean what the bleep?? Oh and wait it gets better. She bleeping kisses him passionately before she gets out of the car. Yea, she's not a whore. Oh, thank you for letting me go, let me go make out with you one last time for good ole' sake. Smooch smooch, smooch even though I'm still married to a guy I left for a rich guy. I have never seen such a piece of crap in my life. How the hell are we supposed to feel good after that horrible ending? What was this movie supposed to represent? NOTHING CAME OUT OF THIS! This was the most pointless movie I have ever seen in my life. Two pathetic desperate people. If I were Woody, I would tell her to go drown herself in that body of water they were near. Apparently, he had no self respect. What the hell was Roy Hobbs thinking by taking this horrible role. I feel like puking after watching this. This movie was so bad, it was seriously laughable. I want those two hours of my life back that I wasted watching this piece of ****.  <pre>--------------------------------\n</pre>  I just watched it for the second time today and I must say with all my heart it is about damn time they made a movie about us as people not as spiritual beings. Such a waste of human life as this story was maybe some good will come out of it. And Eric is hotter than ever. To often in the movies First Nations people are seen as other than everyday people. We are always portrayed as chiefs or medicine people. Hey we are just like everyone else. And this movie showed just that. We hurt when an injustice is done and we can win in quest for justice. It is really to bad that the big movie companies cant see that. I cant wait till this comes out on DVD. Thanks to those who chose to show this story as it really was.  <pre>--------------------------------\n</pre>  I've read through a lot of the comments here about how this movie sticks to the book.. I don't think any of them have actually read it. Edgar Rice wrote about a dangerous African Jungle and Apes were killers and hunters. We know differently now and this movie portrays Apes in a more modern view. I've never seen a Tarzan movie that even comes close to Edgar's vision. Maybe one day Hollywood with trust talented and respected authors to tell the story. So, if you've never read the book and enjoy a good story about feelings and a fluffy bunny view of wild animals, maybe a good cry, see the movie. I hope John Carter of Mars get's more respect than Tarzan has. We miss ya, Edgar!  <pre>--------------------------------\n</pre>  I usually much prefer French movies over American ones, with explosions and car chases, but this movie was very disappointing. There is no way to write a spoiler because nothing really happens. This French couple has been living in Lisbon for years, and they return to Paris for a friend's wedding. They announce to another friend they are having dinner with that they are going to split. Then nothing much happens, they don't seem to know whether they want to separate or not. I don't necessarily think that their hesitations make for a bad movie, it is very human to hesitate before making such a decision for good, but this could be treated in an interesting manner, giving some flesh to their desires and their relationship, but that does not happen. One gets out of the theater unsure of why these two got together or want to split. The only piece I enjoyed was the conversation with the drunk. That was true to life.  <pre>--------------------------------\n</pre> <p>We see that we have removed the words that are not related to sentiment and the model is more focused on the sentiment of the review. But we see others. Maybe we should just limit the vocabulary size to the most important words\u2014the ones that are really important for the classification. This should be investigated.</p> <p>We see that the model stays with equivalent results. It means that the stop words we removed are not so important for the classification as the model can handle the classification without them. We would need to investigate further to see if we can improve the results by removing other stop words and feeling confident enough to put such models in production. We don't want the user looking at results and seeing that a word like \"film\" or \"movie\" drove the results the user will lose confidence in the model.</p>"},{"location":"chapter1/Session_1_3_tfidf/#ml-with-tf-idf-logistic-regression-on-imdb-dataset","title":"ML with TF-IDF + Logistic Regression on IMDB Dataset\u00b6","text":"<p>In this notebook, we will:</p> <ol> <li>Set a NumPy random seed and load the IMDB dataset from Hugging Face, sampling 8k examples for training and 3k for testing. We will further split the training set into train and dev.</li> <li>Tackle sentiment classification with TF-IDF features and Logistic Regression.</li> <li>Explore key hyperparameters: <code>stop_words</code>, <code>tokenizer</code>, <code>analyzer</code>, <code>min_df</code>, <code>max_df</code>, <code>ngram_range</code>, <code>max_features</code>, and <code>vocabulary</code> with clear examples.</li> <li>Create a scikit-learn Pipeline with a pre-processing class (if needed), TF-IDF vectorizer, and Logistic Regression (with comments on regularization parameters and class weights) and evaluate the results on the dev and test sets.</li> <li>Analyze feature importance:<ul> <li>a. Look at the contribution of each token.</li> <li>b. Show (with color highlights) which words drive the predictions.</li> <li>c. Rank the top words for positive and negative classes.</li> </ul> </li> <li>Update the pre-processing pipelines based on this analysis.</li> </ol>"},{"location":"chapter1/Session_1_3_tfidf/#0-load-the-imdb-dataset","title":"0. Load the IMDB dataset\u00b6","text":""},{"location":"chapter1/Session_1_3_tfidf/#what-is-the-imdb-dataset","title":"What is the IMDB Dataset?\u00b6","text":"<p>The IMDB dataset is a widely-used benchmark in sentiment analysis. It consists of movie reviews collected from the Internet Movie Database (IMDB), each labeled with a binary sentiment indicating whether the review is positive or negative.</p>"},{"location":"chapter1/Session_1_3_tfidf/#key-features","title":"Key Features:\u00b6","text":"<ul> <li>Text Reviews: Each record is a movie review written in natural language.</li> <li>Binary Sentiment Labels: Reviews are marked as <code>0</code> (negative) or <code>1</code> (positive).</li> <li>Benchmark Dataset: It is extensively used for training and evaluating models in natural language processing, particularly for sentiment classification tasks.</li> </ul> <p>I set a random seed for reproducibility. I sample 8000 examples for training and 3000 for testing. I further split the training set into train and dev (80/20 split).</p>"},{"location":"chapter1/Session_1_3_tfidf/#1-tf-idf-logistic-regression-for-sentiment-classification","title":"1. TF-IDF + Logistic Regression for Sentiment Classification\u00b6","text":"<p>We will build a machine learning model using TF-IDF features combined with Logistic Regression to predict sentiment (positive/negative) from IMDB reviews.</p> <p>As mentioned in the slides, we can use the <code>TfidfVectorizer</code> to convert the text data into a matrix of TF-IDF features.</p> <p>We can then use a <code>LogisticRegression</code> model to predict the sentiment of the reviews. It's a simple model that is easy to understand and interpret.</p> <p>We can also use a <code>Pipeline</code> to streamline the process of converting the text data into TF-IDF features and then using the Logistic Regression model to predict the sentiment. <code>Pipeline</code> is a powerful tool that allows us to chain together multiple steps of the process into a single object.</p>"},{"location":"chapter1/Session_1_3_tfidf/#2-hyperparameter-examples-for-tf-idf","title":"2. Hyperparameter Examples for TF-IDF\u00b6","text":"<p>Below are some key parameters for the TF-IDF vectorizer with brief explanations:</p> <ul> <li>stop_words: Words to remove (e.g., \"english\" removes common English stop words).</li> <li>tokenizer: Function to split text into tokens. You can provide your own function.</li> <li>analyzer: Determines whether the analyzer operates on word or character level.</li> <li>min_df: Minimum document frequency; ignore terms that appear in fewer documents.</li> <li>max_df: Maximum document frequency; ignore terms that appear in a large proportion of documents.</li> <li>ngram_range: The range of n-values for different n-grams to be extracted (e.g., (1,2) for unigrams and bigrams).</li> <li>max_features: Maximum number of features (vocabulary size) to consider.</li> <li>vocabulary: Optionally, you can pass a custom vocabulary.</li> </ul> <p>Below are examples of how those parameters affect the feature matrix.</p>"},{"location":"chapter1/Session_1_3_tfidf/#stop_words","title":"Stop_words\u00b6","text":"<p>Description: Removes common words from the text. Example: Remove common English stop words.</p>"},{"location":"chapter1/Session_1_3_tfidf/#tokenizer","title":"Tokenizer\u00b6","text":"<p>Description: Function to split text into tokens. Example: Use a custom tokenizer.</p> <p>You'll see a warning when you use a custom tokenizer as you won't be using the default <code>token_pattern</code> anymore.</p>"},{"location":"chapter1/Session_1_3_tfidf/#analyzer","title":"Analyzer\u00b6","text":"<p>Description: Determines whether the analyzer operates on word or character level. Example: Analyze words, characters or character-word boundary.</p> <p>\u2018char_wb\u2019 creates character n-grams only from text inside word boundaries.</p>"},{"location":"chapter1/Session_1_3_tfidf/#min_df-and-max_df","title":"Min_df and Max_df\u00b6","text":"<p>Description: Minimum and maximum document frequencies for terms to be included in the vocabulary. Example: Include terms that appear in at least 5 documents and at most 80% of the documents.</p> <p>In the following example, we see that the features map is changing when we change the <code>min_df</code> and <code>max_df</code> parameters.</p>"},{"location":"chapter1/Session_1_3_tfidf/#n-gram-range","title":"N-gram Range\u00b6","text":"<p>Description: The range of n-values for different n-grams to be extracted (e.g., (1,2) for unigrams and bigrams). Example: Extract unigrams and bigrams.</p>"},{"location":"chapter1/Session_1_3_tfidf/#max_features","title":"Max_features\u00b6","text":"<p>Description: Maximum number of features (vocabulary size) to consider. Example: Limit the vocabulary size to 2.</p> <p>Only consider the top <code>max_features</code> ordered by term frequency across the corpus.</p>"},{"location":"chapter1/Session_1_3_tfidf/#vocabulary","title":"Vocabulary\u00b6","text":"<p>Description: Optionally, you can pass a custom vocabulary. Example: Use a custom vocabulary.</p> <p>The feature map will be the intersection of the vocabulary and the features in the corpus.</p>"},{"location":"chapter1/Session_1_3_tfidf/#3-train-the-model-and-evaluate","title":"3. Train the Model and Evaluate\u00b6","text":"<p>We now train our pipeline on the training set, evaluate on the dev set, and finally check performance on the test set.</p> <p>You'll see below that this is straightforward as we just need to call the <code>fit</code> method on the training set and then the <code>predict</code> method on the dev and test sets.</p>"},{"location":"chapter1/Session_1_3_tfidf/#4-feature-importance-analysis","title":"4. Feature Importance Analysis\u00b6","text":"<p>After training, we analyze the learned logistic regression coefficients to understand which tokens drive the predictions.</p> <p>a. Contribution of Each Token: We extract the coefficient for each feature (token).</p> <p>b. Visual Examples: We'll highlight tokens in some example reviews (this example uses HTML formatting for color).</p> <p>c. Ranking Top Tokens: We rank tokens for each class (positive and negative).</p>"},{"location":"chapter1/Session_1_4_BM25/","title":"BM25: A Better TF-IDF for Text Retrieval","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datasets import load_dataset\nimport time\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Scikit-learn imports\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, _document_frequency\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\n\n# Set random seed for reproducibility\nnp.random.seed(42)\nimport random\nrandom.seed(42)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from datasets import load_dataset import time from tqdm.notebook import tqdm import warnings warnings.filterwarnings('ignore')  # Scikit-learn imports from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, _document_frequency from sklearn.metrics import precision_recall_curve, average_precision_score from sklearn.metrics import precision_score, recall_score, f1_score from sklearn.utils.validation import check_is_fitted from sklearn.pipeline import Pipeline from sklearn.metrics.pairwise import cosine_similarity from sklearn.model_selection import train_test_split from sklearn.preprocessing import normalize  # Set random seed for reproducibility np.random.seed(42) import random random.seed(42) In\u00a0[2]: Copied! <pre># Load the imdb\ndataset = load_dataset('imdb')\nprint(dataset)\n</pre> # Load the imdb dataset = load_dataset('imdb') print(dataset) <pre>DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n</pre> In\u00a0[3]: Copied! <pre># Convert to pandas DataFrame for easier manipulation\ntrain_df = dataset['train'].to_pandas()\ntest_df = dataset['test'].to_pandas()\n# Clean the memory\ndel dataset\n</pre> # Convert to pandas DataFrame for easier manipulation train_df = dataset['train'].to_pandas() test_df = dataset['test'].to_pandas() # Clean the memory del dataset In\u00a0[4]: Copied! <pre>#Distribution of label\nsns.histplot(train_df['label'], bins=2)\nplt.title('Distribution of Label')\nplt.xticks([0.25, 0.75], ['Not Positive', 'Positive'])\nplt.legend()\nplt.show()\n\n#Distribution of label\nsns.histplot(test_df['label'], bins=2)\nplt.title('Distribution of Label')\nplt.xticks([0.25, 0.75], ['Not Positive', 'Positive'])\nplt.legend()\nplt.show()\n\n# Display a sample\ntest_df.head()\n</pre> #Distribution of label sns.histplot(train_df['label'], bins=2) plt.title('Distribution of Label') plt.xticks([0.25, 0.75], ['Not Positive', 'Positive']) plt.legend() plt.show()  #Distribution of label sns.histplot(test_df['label'], bins=2) plt.title('Distribution of Label') plt.xticks([0.25, 0.75], ['Not Positive', 'Positive']) plt.legend() plt.show()  # Display a sample test_df.head() Out[4]: text label 0 I love sci-fi and am willing to put up with a ... 0 1 Worth the entertainment value of a rental, esp... 0 2 its a totally average film with a few semi-alr... 0 3 STAR RATING: ***** Saturday Night **** Friday ... 0 4 First off let me say, If you haven't enjoyed a... 0 In\u00a0[5]: Copied! <pre># Look at the distribution of paragraph lengths\ntrain_df['review_length'] = train_df['text'].apply(len)\ntest_df['review_length'] = test_df['text'].apply(len)\n\nplt.figure(figsize=(10, 6))\n#normalized histograms\nsns.histplot(train_df['review_length'], bins=100, label='Train Reviews', stat='density', color='blue', alpha=0.5)\nsns.histplot(test_df['review_length'], bins=100, label='Test Reviews', stat='density', color='orange', alpha=0.5)\n\n\nplt.title('Distribution of Paragraph Lengths')\nplt.xlabel('Length (characters)')\nplt.ylabel('Count')\nplt.axvline(x=train_df['review_length'].mean(), color='r', linestyle='--', label=f'Mean: {train_df[\"review_length\"].mean():.0f}')\nplt.axvline(x=train_df['review_length'].median(), color='g', linestyle='--', label=f'Median: {train_df[\"review_length\"].median():.0f}')\nplt.axvline(x=test_df['review_length'].mean(), color='b', linestyle='--', label=f'Mean: {test_df[\"review_length\"].mean():.0f}')\nplt.axvline(x=test_df['review_length'].median(), color='y', linestyle='--', label=f'Median: {test_df[\"review_length\"].median():.0f}')\nplt.legend()\nplt.show()\n</pre> # Look at the distribution of paragraph lengths train_df['review_length'] = train_df['text'].apply(len) test_df['review_length'] = test_df['text'].apply(len)  plt.figure(figsize=(10, 6)) #normalized histograms sns.histplot(train_df['review_length'], bins=100, label='Train Reviews', stat='density', color='blue', alpha=0.5) sns.histplot(test_df['review_length'], bins=100, label='Test Reviews', stat='density', color='orange', alpha=0.5)   plt.title('Distribution of Paragraph Lengths') plt.xlabel('Length (characters)') plt.ylabel('Count') plt.axvline(x=train_df['review_length'].mean(), color='r', linestyle='--', label=f'Mean: {train_df[\"review_length\"].mean():.0f}') plt.axvline(x=train_df['review_length'].median(), color='g', linestyle='--', label=f'Median: {train_df[\"review_length\"].median():.0f}') plt.axvline(x=test_df['review_length'].mean(), color='b', linestyle='--', label=f'Mean: {test_df[\"review_length\"].mean():.0f}') plt.axvline(x=test_df['review_length'].median(), color='y', linestyle='--', label=f'Median: {test_df[\"review_length\"].median():.0f}') plt.legend() plt.show() <p>The distributions are quite similar with equal labels in both train and test. We don't need to do any balancing here.</p> In\u00a0[6]: Copied! <pre>from scipy import sparse\n\nclass BM25Vectorizer(CountVectorizer):\n    \"\"\"Convert a collection of text documents to a matrix of BM25 scores.\n    \n    This implementation follows the Okapi BM25 formula and extends CountVectorizer\n    to allow for seamless integration with scikit-learn pipelines.\n    \n    Parameters\n    ----------\n    k1 : float, default=1.5\n        Controls term frequency saturation\n    b : float, default=0.75\n        Controls how much document length normalization affects the score\n    norm : {'l1', 'l2'}, default=None\n        Optional normalization\n    use_idf : bool, default=True\n        Whether to use inverse document frequency weighting\n    smooth_idf : bool, default=True\n        Smooth IDF weights by adding 1 to document frequencies\n    sublinear_tf : bool, default=False\n        Apply sublinear scaling to term frequencies (1 + log(tf))\n    \"\"\"\n    \n    def __init__(self, input='content', encoding='utf-8', decode_error='strict',\n                 strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,\n                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\", ngram_range=(1, 1),\n                 analyzer='word', max_df=1.0, min_df=1, max_features=None,\n                 vocabulary=None, binary=False, dtype=np.int64,\n                 k1=1.5, b=0.75, norm=None, use_idf=True, \n                 smooth_idf=True, sublinear_tf=False):\n        \n        # Initialize CountVectorizer with all its parameters\n        super().__init__(\n            input=input, encoding=encoding, decode_error=decode_error,\n            strip_accents=strip_accents, lowercase=lowercase,\n            preprocessor=preprocessor, tokenizer=tokenizer,\n            stop_words=stop_words, token_pattern=token_pattern,\n            ngram_range=ngram_range, analyzer=analyzer,\n            max_df=max_df, min_df=min_df, max_features=max_features,\n            vocabulary=vocabulary, binary=binary, dtype=dtype\n        )\n        \n        # BM25-specific parameters\n        self.k1 = k1\n        self.b = b\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        \n    def fit(self, X, y=None):\n        \"\"\"Learn vocabulary and document frequencies from training set.\"\"\"\n        X = super().fit_transform(X)\n        \n        if self.use_idf:\n            n_samples, n_features = X.shape\n            df = _document_frequency(X)\n            \n            # Calculate IDF scores\n            idf = np.log((n_samples - df + 0.5) / (df + 0.5) + 1.0)\n            self._idf_diag = sparse.diags(idf, offsets=0,\n                                         shape=(n_features, n_features),\n                                         format=\"csr\")\n        \n        # Calculate average document length for BM25\n        self._avgdl = X.sum(axis=1).mean()\n        \n        return self\n        \n    def transform(self, X):\n        \"\"\"Transform documents to BM25 vectors.\"\"\"\n        # Transform documents to term frequency vectors\n        X = super().transform(X)\n        \n        if self.sublinear_tf:\n            # Apply sublinear term frequency scaling: tf -&gt; 1 + log(tf)\n            np.log1p(X.data, out=X.data)\n        \n        # Apply BM25 term frequency transformation\n        doc_lengths = X.sum(axis=1).A1\n        \n        # Calculate document length normalization factors\n        len_norm = (1 - self.b) + self.b * (doc_lengths / self._avgdl)\n        \n        # Implement BM25 term frequency transformation in a vectorized way\n        # For each non-zero element in the sparse matrix:\n        # tf_scaled = tf * (k1 + 1) / (tf + k1 * len_norm)\n        denominator = np.repeat(len_norm, np.diff(X.indptr))\n        \n        # Calculate BM25 term frequency score\n        X.data = X.data * (self.k1 + 1) / (X.data + self.k1 * denominator)\n        \n        if self.use_idf:\n            # Apply IDF weighting\n            check_is_fitted(self, '_idf_diag')\n            X = X * self._idf_diag\n            \n        if self.norm:\n            # Apply normalization\n            X = normalize(X, norm=self.norm, copy=False)\n            \n        return X\n    \n    def fit_transform(self, X, y=None):\n        \"\"\"Learn vocabulary and document frequencies, and transform documents to BM25 vectors.\"\"\"\n        # Call fit and then transform\n        return self.fit(X).transform(X)\n</pre> from scipy import sparse  class BM25Vectorizer(CountVectorizer):     \"\"\"Convert a collection of text documents to a matrix of BM25 scores.          This implementation follows the Okapi BM25 formula and extends CountVectorizer     to allow for seamless integration with scikit-learn pipelines.          Parameters     ----------     k1 : float, default=1.5         Controls term frequency saturation     b : float, default=0.75         Controls how much document length normalization affects the score     norm : {'l1', 'l2'}, default=None         Optional normalization     use_idf : bool, default=True         Whether to use inverse document frequency weighting     smooth_idf : bool, default=True         Smooth IDF weights by adding 1 to document frequencies     sublinear_tf : bool, default=False         Apply sublinear scaling to term frequencies (1 + log(tf))     \"\"\"          def __init__(self, input='content', encoding='utf-8', decode_error='strict',                  strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None,                  stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\", ngram_range=(1, 1),                  analyzer='word', max_df=1.0, min_df=1, max_features=None,                  vocabulary=None, binary=False, dtype=np.int64,                  k1=1.5, b=0.75, norm=None, use_idf=True,                   smooth_idf=True, sublinear_tf=False):                  # Initialize CountVectorizer with all its parameters         super().__init__(             input=input, encoding=encoding, decode_error=decode_error,             strip_accents=strip_accents, lowercase=lowercase,             preprocessor=preprocessor, tokenizer=tokenizer,             stop_words=stop_words, token_pattern=token_pattern,             ngram_range=ngram_range, analyzer=analyzer,             max_df=max_df, min_df=min_df, max_features=max_features,             vocabulary=vocabulary, binary=binary, dtype=dtype         )                  # BM25-specific parameters         self.k1 = k1         self.b = b         self.norm = norm         self.use_idf = use_idf         self.smooth_idf = smooth_idf         self.sublinear_tf = sublinear_tf              def fit(self, X, y=None):         \"\"\"Learn vocabulary and document frequencies from training set.\"\"\"         X = super().fit_transform(X)                  if self.use_idf:             n_samples, n_features = X.shape             df = _document_frequency(X)                          # Calculate IDF scores             idf = np.log((n_samples - df + 0.5) / (df + 0.5) + 1.0)             self._idf_diag = sparse.diags(idf, offsets=0,                                          shape=(n_features, n_features),                                          format=\"csr\")                  # Calculate average document length for BM25         self._avgdl = X.sum(axis=1).mean()                  return self              def transform(self, X):         \"\"\"Transform documents to BM25 vectors.\"\"\"         # Transform documents to term frequency vectors         X = super().transform(X)                  if self.sublinear_tf:             # Apply sublinear term frequency scaling: tf -&gt; 1 + log(tf)             np.log1p(X.data, out=X.data)                  # Apply BM25 term frequency transformation         doc_lengths = X.sum(axis=1).A1                  # Calculate document length normalization factors         len_norm = (1 - self.b) + self.b * (doc_lengths / self._avgdl)                  # Implement BM25 term frequency transformation in a vectorized way         # For each non-zero element in the sparse matrix:         # tf_scaled = tf * (k1 + 1) / (tf + k1 * len_norm)         denominator = np.repeat(len_norm, np.diff(X.indptr))                  # Calculate BM25 term frequency score         X.data = X.data * (self.k1 + 1) / (X.data + self.k1 * denominator)                  if self.use_idf:             # Apply IDF weighting             check_is_fitted(self, '_idf_diag')             X = X * self._idf_diag                      if self.norm:             # Apply normalization             X = normalize(X, norm=self.norm, copy=False)                      return X          def fit_transform(self, X, y=None):         \"\"\"Learn vocabulary and document frequencies, and transform documents to BM25 vectors.\"\"\"         # Call fit and then transform         return self.fit(X).transform(X) In\u00a0[7]: Copied! <pre>reviews = train_df['text'].tolist()\n\n# Initialize vectorizers\ntfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.85, ngram_range=(1, 2), \n                                   sublinear_tf=True, norm='l2', stop_words=\"english\")\nbm25_vectorizer = BM25Vectorizer(min_df=3, max_df=0.85, ngram_range=(1, 2), \n                                 k1=1.5, b=0.75, norm='l2', stop_words=\"english\")\n\n# Fit the vectorizers\nprint(\"Fitting TF-IDF vectorizer...\")\nstart_time = time.time()\ntfidf_vectorizer.fit(reviews)\nprint(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\nprint(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n\nprint(\"\\nFitting BM25 vectorizer...\")\nstart_time = time.time()\nbm25_vectorizer.fit(reviews)\nprint(f\"BM25 vocabulary size: {len(bm25_vectorizer.vocabulary_)}\")\nprint(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n</pre> reviews = train_df['text'].tolist()  # Initialize vectorizers tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.85, ngram_range=(1, 2),                                     sublinear_tf=True, norm='l2', stop_words=\"english\") bm25_vectorizer = BM25Vectorizer(min_df=3, max_df=0.85, ngram_range=(1, 2),                                   k1=1.5, b=0.75, norm='l2', stop_words=\"english\")  # Fit the vectorizers print(\"Fitting TF-IDF vectorizer...\") start_time = time.time() tfidf_vectorizer.fit(reviews) print(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\") print(f\"Time taken: {time.time() - start_time:.2f} seconds\")  print(\"\\nFitting BM25 vectorizer...\") start_time = time.time() bm25_vectorizer.fit(reviews) print(f\"BM25 vocabulary size: {len(bm25_vectorizer.vocabulary_)}\") print(f\"Time taken: {time.time() - start_time:.2f} seconds\") <pre>Fitting TF-IDF vectorizer...\nTF-IDF vocabulary size: 159213\nTime taken: 4.74 seconds\n\nFitting BM25 vectorizer...\nBM25 vocabulary size: 159213\nTime taken: 4.78 seconds\n</pre> <p>We see that we have the same vocabulary size for both vectorizers. the main changes will be in the term weighting. And in term of fitting time both are quite similar too.</p> In\u00a0[8]: Copied! <pre>start_time = time.time()\nreviews_tfidf = tfidf_vectorizer.transform(reviews)\nprint(f\"TF-IDF Vectorization: {time.time() - start_time:.2f} seconds\")\n\nstart_time = time.time()\nreviews_bm25 = bm25_vectorizer.transform(reviews)\nprint(f\"BM25 Vectorization: {time.time() - start_time:.2f} seconds\")\n</pre> start_time = time.time() reviews_tfidf = tfidf_vectorizer.transform(reviews) print(f\"TF-IDF Vectorization: {time.time() - start_time:.2f} seconds\")  start_time = time.time() reviews_bm25 = bm25_vectorizer.transform(reviews) print(f\"BM25 Vectorization: {time.time() - start_time:.2f} seconds\") <pre>TF-IDF Vectorization: 3.59 seconds\nBM25 Vectorization: 3.62 seconds\n</pre> <p>Again we see that the transformation time is quite similar.</p> In\u00a0[9]: Copied! <pre>def get_top_terms(vectorizer, matrix, doc_idx, top_n=10):\n    \"\"\"Get the top weighted terms for a document from a vectorizer matrix.\"\"\"\n    feature_names = np.array(vectorizer.get_feature_names_out())\n    doc_vector = matrix[doc_idx].toarray().flatten()\n    \n    # Get indices of top weighted terms\n    top_indices = doc_vector.argsort()[-top_n:][::-1]\n    \n    # Get terms and weights\n    top_terms = feature_names[top_indices]\n    weights = doc_vector[top_indices]\n    \n    return list(zip(top_terms, weights))\n\n# Get a few example paragraphs\nexample_indices = np.random.randint(0, len(train_df), 5)\nexample_paragraphs = [reviews[idx] for idx in example_indices]\n\n# Compare TF-IDF and BM25 representations\nfor i, idx in enumerate(example_indices):\n    print(f\"\\nExample {i+1}: {example_paragraphs[i]}...\")\n    \n    print(\"\\nTop TF-IDF terms:\")\n    tfidf_terms = get_top_terms(tfidf_vectorizer, reviews_tfidf, idx)\n    for term, weight in tfidf_terms:\n        print(f\"  {term}: {weight:.4f}\")\n    \n    print(\"\\nTop BM25 terms:\")\n    bm25_terms = get_top_terms(bm25_vectorizer, reviews_bm25, idx)\n    for term, weight in bm25_terms:\n        print(f\"  {term}: {weight:.4f}\")\n    \n    print(\"\\n\" + \"-\"*80)\n</pre> def get_top_terms(vectorizer, matrix, doc_idx, top_n=10):     \"\"\"Get the top weighted terms for a document from a vectorizer matrix.\"\"\"     feature_names = np.array(vectorizer.get_feature_names_out())     doc_vector = matrix[doc_idx].toarray().flatten()          # Get indices of top weighted terms     top_indices = doc_vector.argsort()[-top_n:][::-1]          # Get terms and weights     top_terms = feature_names[top_indices]     weights = doc_vector[top_indices]          return list(zip(top_terms, weights))  # Get a few example paragraphs example_indices = np.random.randint(0, len(train_df), 5) example_paragraphs = [reviews[idx] for idx in example_indices]  # Compare TF-IDF and BM25 representations for i, idx in enumerate(example_indices):     print(f\"\\nExample {i+1}: {example_paragraphs[i]}...\")          print(\"\\nTop TF-IDF terms:\")     tfidf_terms = get_top_terms(tfidf_vectorizer, reviews_tfidf, idx)     for term, weight in tfidf_terms:         print(f\"  {term}: {weight:.4f}\")          print(\"\\nTop BM25 terms:\")     bm25_terms = get_top_terms(bm25_vectorizer, reviews_bm25, idx)     for term, weight in bm25_terms:         print(f\"  {term}: {weight:.4f}\")          print(\"\\n\" + \"-\"*80) <pre>\nExample 1: I first saw \"Breaking Glass\" in 1980, and thought that it would be one of the \"Movie Classics\". This film is a great look into the music industry with a great cast of performers. This is one film that should be in the collection of everyone and any one that wants to get into the music industry. I can't wait for it to be available on DVD....\n\nTop TF-IDF terms:\n  music industry: 0.3558\n  industry great: 0.2431\n  classics film: 0.2431\n  cast performers: 0.2431\n  look music: 0.2431\n  breaking glass: 0.2330\n  movie classics: 0.2330\n  industry: 0.2275\n  performers film: 0.2258\n  film collection: 0.2101\n\nTop BM25 terms:\n  music industry: 0.2824\n  look music: 0.2696\n  industry great: 0.2696\n  classics film: 0.2696\n  cast performers: 0.2696\n  breaking glass: 0.2559\n  movie classics: 0.2559\n  performers film: 0.2465\n  film collection: 0.2265\n  great look: 0.2191\n\n--------------------------------------------------------------------------------\n\nExample 2: Went to see this finnish film and I've got to say that it is one of the better films I've seen this year. The intrigue is made up of 5-6 different stories, all taking place the very same day in a small finnish town. The stories come together very nicely in the end, reminding, perhaps, a bit of the way Tarantino's movies are made. Most of the actors performed very well, which most certainly is needed in realistic dramas of this type. I especially enjoyed the acting by Sanna Hietala, the lead actress, and Juha Kukkonen. I noticed btw that IMDB has got the wrong information about Sanna. Her name, as you might have noticed in my review ;), is NOT Heikkil\u00e4, but Hietala....\n\nTop TF-IDF terms:\n  finnish: 0.2735\n  nicely end: 0.1969\n  finnish film: 0.1969\n  come nicely: 0.1969\n  actors performed: 0.1969\n  place day: 0.1924\n  noticed: 0.1899\n  enjoyed acting: 0.1829\n  got wrong: 0.1805\n  stories come: 0.1784\n\nTop BM25 terms:\n  finnish: 0.2258\n  nicely end: 0.2183\n  finnish film: 0.2183\n  come nicely: 0.2183\n  actors performed: 0.2183\n  place day: 0.2122\n  enjoyed acting: 0.1996\n  got wrong: 0.1965\n  stories come: 0.1938\n  movies actors: 0.1802\n\n--------------------------------------------------------------------------------\n\nExample 3: I have read the novel Reaper of Ben Mezrich a fews years ago and last night I accidentally came to see this adaption.&lt;br /&gt;&lt;br /&gt;Although it's been years since I read the story the first time, the differences between the novel and the movie are humongous. Very important elements, which made the whole thing plausible are just written out or changed to bad.&lt;br /&gt;&lt;br /&gt;If the plot sounds interesting to you: go and get the novel. Its much, much, much better.&lt;br /&gt;&lt;br /&gt;Still 4 out of 10 since it was hard to stop watching because of the great basic plot by Ben Mezrich....\n\nTop TF-IDF terms:\n  accidentally came: 0.2119\n  years read: 0.2119\n  fews: 0.2119\n  novel: 0.2109\n  sounds interesting: 0.2070\n  adaption br: 0.2030\n  plot sounds: 0.1942\n  just written: 0.1942\n  humongous: 0.1942\n  ben: 0.1915\n\nTop BM25 terms:\n  years read: 0.2344\n  accidentally came: 0.2344\n  fews: 0.2344\n  sounds interesting: 0.2277\n  adaption br: 0.2224\n  humongous: 0.2109\n  plot sounds: 0.2109\n  just written: 0.2109\n  read story: 0.1951\n  watching great: 0.1951\n\n--------------------------------------------------------------------------------\n\nExample 4: The story is similar to ET: an extraterrestrial run around on earth and tries to come back home. While its stay on our planet, it will create friendly ties with humans.&lt;br /&gt;&lt;br /&gt;But, unlike ET which exudes drama, comedy, poetry, this movie is only fun. It is indeed a pure Dysney production: its core audience are children &amp; the movie is more more in the visual than in the message.&lt;br /&gt;&lt;br /&gt;Thus, you will find some funny scenes (the first sighting of the town, a \"cosmic\" stray toaster) and the casting is experimented, with special mentions to \"Doc\", who rejuvenates in a \"Mac Fly\" character, and to Hurley, who seems open to auto-derision.&lt;br /&gt;&lt;br /&gt;Ice on the cake: the main title is scored by Danny Elfman, and like every other great composer, you recognize his \"voice\" before he is even credited....\n\nTop TF-IDF terms:\n  et: 0.1915\n  special mentions: 0.1630\n  rejuvenates: 0.1630\n  core audience: 0.1630\n  audience children: 0.1593\n  story similar: 0.1563\n  danny elfman: 0.1563\n  tries come: 0.1537\n  toaster: 0.1537\n  sighting: 0.1514\n\nTop BM25 terms:\n  special mentions: 0.1743\n  rejuvenates: 0.1743\n  core audience: 0.1743\n  audience children: 0.1693\n  danny elfman: 0.1654\n  story similar: 0.1654\n  toaster: 0.1621\n  tries come: 0.1621\n  sighting: 0.1593\n  elfman: 0.1593\n\n--------------------------------------------------------------------------------\n\nExample 5: My discovery of the cinema of Jan Svankmajer opened My eyes to a whole tradition of Czech animation, of which Jir\u00ed Trnka was a pioneer. His Ruka is one of the finest, most technically-impressive animated movies I've ever seen.&lt;br /&gt;&lt;br /&gt;A potter wakes up and waters his plant. Then he goes about making a pot. But in comes the huge hand which crashes the pot and demands that the potter make a statue of itself. He casts the hand out, but soon it returns and imprisons him in a bird cage where he's forced to sculpt a stone hand. He sets about it, fainting from exhaustion, but eventually completes the task.&lt;br /&gt;&lt;br /&gt;In a marvellous sequence of metacinema, the potter uses a candle to burn his visible puppet strings, which keep him in thrall, and he escapes back home. He shuts himself in and is accidentally killed by his own beloved plant when it falls on his head.&lt;br /&gt;&lt;br /&gt;This movie doesn't hide the fact it's pure animation, unlike modern movies that strive to be realistic (why?). The hand, for instance, is clearly someone's hand in a glove. Everything else is clay. Strings are visible and are part of the narrative, making it a precursor of the movie Strings. The atmosphere is eerie: that hand going after the little potter managed to instill more dread in me than many horror movies combined.&lt;br /&gt;&lt;br /&gt;The movie is obvious but it avoids being totally manipulative for its simplicity. it's a fable about artistic freedom and tyranny which can't help winning the heart and mind of anyone who holds freedom as a natural right....\n\nTop TF-IDF terms:\n  potter: 0.2099\n  strings: 0.1788\n  hand: 0.1409\n  plant: 0.1399\n  visible: 0.1391\n  pot: 0.1361\n  freedom: 0.1207\n  movies combined: 0.1191\n  soon returns: 0.1191\n  jir\u00ed: 0.1191\n\nTop BM25 terms:\n  potter: 0.1699\n  strings: 0.1494\n  soon returns: 0.1310\n  technically impressive: 0.1310\n  fact pure: 0.1310\n  jir\u00ed: 0.1310\n  huge hand: 0.1310\n  movies combined: 0.1310\n  winning heart: 0.1273\n  bird cage: 0.1273\n\n--------------------------------------------------------------------------------\n</pre> <p>Let's visualize the weight differences between TF-IDF and BM25 for one document.</p> In\u00a0[10]: Copied! <pre>for doc_idx in np.random.randint(0, len(train_df), 5):\n    # Get TF-IDF and BM25 term weights for the selected document\n    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n\n    # Get the non-zero terms from both vectors\n    tfidf_vector = reviews_tfidf[doc_idx].toarray().flatten()\n    bm25_vector = reviews_bm25[doc_idx].toarray().flatten()\n\n    non_zero_idx = np.logical_or(tfidf_vector &gt; 0, bm25_vector &gt; 0)\n    terms = feature_names[non_zero_idx]\n    tfidf_weights = tfidf_vector[non_zero_idx]\n    bm25_weights = bm25_vector[non_zero_idx]\n\n    # Find the top 15 terms by the sum of both weights\n    top_idx = np.argsort(tfidf_weights + bm25_weights)[-15:]\n\n    # Create a DataFrame for easier visualization\n    weights_df = pd.DataFrame({\n        'term': terms[top_idx],\n        'TF-IDF': tfidf_weights[top_idx],\n        'BM25': bm25_weights[top_idx]\n    }).sort_values('term')\n\n    # Prepare data for plotting\n    terms_plot = weights_df['term']\n    tfidf_plot = weights_df['TF-IDF']\n    bm25_plot = weights_df['BM25']\n\n    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\n    x = np.arange(len(terms_plot))\n    width = 0.35\n\n    ax[0].bar(x - width/2, tfidf_plot, width, label='TF-IDF')\n    ax[0].bar(x + width/2, bm25_plot, width, label='BM25')\n\n    ax[0].set_xlabel('Terms')\n    ax[0].set_ylabel('Weight')\n    ax[0].set_title('TF-IDF vs BM25 Term Weights')\n    ax[0].set_xticks(x, terms_plot, rotation=45, ha='right')\n    ax[0].legend()\n\n\n    # Calculate and plot the weight differences\n    weights_df['Difference'] = weights_df['BM25'] - weights_df['TF-IDF']\n    weights_df = weights_df.sort_values('Difference')\n\n    ax[1].barh(weights_df['term'], weights_df['Difference'])\n    ax[1].set_xlabel('BM25 weight - TF-IDF weight')\n    ax[1].set_ylabel('Term')\n    ax[1].set_title('Difference in Term Weighting (BM25 - TF-IDF)')\n    ax[1].grid(axis='x', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n</pre> for doc_idx in np.random.randint(0, len(train_df), 5):     # Get TF-IDF and BM25 term weights for the selected document     feature_names = np.array(tfidf_vectorizer.get_feature_names_out())      # Get the non-zero terms from both vectors     tfidf_vector = reviews_tfidf[doc_idx].toarray().flatten()     bm25_vector = reviews_bm25[doc_idx].toarray().flatten()      non_zero_idx = np.logical_or(tfidf_vector &gt; 0, bm25_vector &gt; 0)     terms = feature_names[non_zero_idx]     tfidf_weights = tfidf_vector[non_zero_idx]     bm25_weights = bm25_vector[non_zero_idx]      # Find the top 15 terms by the sum of both weights     top_idx = np.argsort(tfidf_weights + bm25_weights)[-15:]      # Create a DataFrame for easier visualization     weights_df = pd.DataFrame({         'term': terms[top_idx],         'TF-IDF': tfidf_weights[top_idx],         'BM25': bm25_weights[top_idx]     }).sort_values('term')      # Prepare data for plotting     terms_plot = weights_df['term']     tfidf_plot = weights_df['TF-IDF']     bm25_plot = weights_df['BM25']      fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))     x = np.arange(len(terms_plot))     width = 0.35      ax[0].bar(x - width/2, tfidf_plot, width, label='TF-IDF')     ax[0].bar(x + width/2, bm25_plot, width, label='BM25')      ax[0].set_xlabel('Terms')     ax[0].set_ylabel('Weight')     ax[0].set_title('TF-IDF vs BM25 Term Weights')     ax[0].set_xticks(x, terms_plot, rotation=45, ha='right')     ax[0].legend()       # Calculate and plot the weight differences     weights_df['Difference'] = weights_df['BM25'] - weights_df['TF-IDF']     weights_df = weights_df.sort_values('Difference')      ax[1].barh(weights_df['term'], weights_df['Difference'])     ax[1].set_xlabel('BM25 weight - TF-IDF weight')     ax[1].set_ylabel('Term')     ax[1].set_title('Difference in Term Weighting (BM25 - TF-IDF)')     ax[1].grid(axis='x', linestyle='--', alpha=0.7)     plt.tight_layout()     plt.show() <p>What we see is that the BM25 generally limit the weight of the terms that may be overweighted in TF-IDF (the ones with higher time frequencies).  And generally what happens is that it's compensate this lower weight with a slightly higher weight for the terms that have lower weights in TF-IDF.</p> <p>Therefore it will be more robust as it wants to give less weights to one term or the other and focuses more on contextual terms. You can see it as a regularization of the TF-IDF.</p> In\u00a0[11]: Copied! <pre>from typing import List, Tuple, Dict, Union\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass RetrievalService:\n    \"\"\"Service for retrieving and ranking documents based on query similarity.\"\"\"\n    \n    def __init__(self, documents: List[str], document_ids: List = None, method: str = \"tfidf\"):\n        \"\"\"\n        Initialize the retrieval service with a corpus of documents.\n        \n        Parameters:\n        -----------\n        documents : List[str]\n            List of document texts to index\n        document_ids : List, optional\n            List of identifiers for the documents (if None, indices will be used)\n        method : str, default=\"tfidf\"\n            Vectorization method to use, either \"tfidf\" or \"bm25\"\n        \"\"\"\n        self.documents = documents\n        self.document_ids = document_ids if document_ids is not None else list(range(len(documents)))\n        \n        if len(self.documents) != len(self.document_ids):\n            raise ValueError(\"Number of documents and document IDs must match\")\n        \n        # Create a mapping from document ID to index\n        self.id_to_index = {doc_id: idx for idx, doc_id in enumerate(self.document_ids)}\n        \n        # Initialize the appropriate vectorizer\n        if method.lower() == \"tfidf\":\n            self.vectorizer = TfidfVectorizer(\n                min_df=5, max_df=0.7, ngram_range=(1, 2),\n                sublinear_tf=True, norm='l2', stop_words='english'\n            )\n            self.method = \"TF-IDF\"\n        elif method.lower() == \"bm25\":\n            self.vectorizer = BM25Vectorizer(\n                min_df=5, max_df=0.7, ngram_range=(1, 2),\n                k1=1.5, b=0.75, norm='l2', stop_words='english'\n            )\n            self.method = \"BM25\"\n        else:\n            raise ValueError(\"Method must be either 'tfidf' or 'bm25'\")\n        \n        # Fit the vectorizer on the document corpus\n        self.document_vectors = self.vectorizer.fit_transform(self.documents)\n        \n        print(f\"RetrievalService initialized with {len(self.documents)} documents using {self.method}\")\n        print(f\"Vocabulary size: {len(self.vectorizer.vocabulary_)}\")\n    \n    def search(self, query: str, top_k: int = 10, return_scores: bool = True) -&gt; Union[List, List[Tuple]]:\n        \"\"\"\n        Search for documents similar to the query.\n        \n        Parameters:\n        -----------\n        query : str\n            The search query\n        top_k : int, default=10\n            Number of top results to return\n        return_scores : bool, default=True\n            Whether to return similarity scores along with document IDs\n            \n        Returns:\n        --------\n        List or List[Tuple]\n            If return_scores is False: List of document IDs\n            If return_scores is True: List of tuples (document_id, similarity_score)\n        \"\"\"\n        # Transform the query using the same vectorizer\n        query_vector = self.vectorizer.transform([query])\n        \n        # Calculate cosine similarities between query and all documents\n        similarities = cosine_similarity(query_vector, self.document_vectors)[0]\n        \n        # Get the indices of the top-k most similar documents\n        top_indices = similarities.argsort()[-top_k:][::-1]\n        \n        # Convert indices to document IDs\n        top_ids = [self.document_ids[idx] for idx in top_indices]\n        \n        if return_scores:\n            # Return document IDs with their similarity scores\n            top_scores = [similarities[idx] for idx in top_indices]\n            return list(zip(top_ids, top_scores))\n        else:\n            # Return only document IDs\n            return top_ids\n    \n    def evaluate_performance(self, queries: List[str]) -&gt; Dict:\n        \"\"\"\n        Evaluate the performance of the retrieval service on a set of queries.\n        \n        Parameters:\n        -----------\n        queries : List[str]\n            List of queries to evaluate\n            \n        Returns:\n        --------\n        Dict\n            Dictionary containing performance metrics\n        \"\"\"\n        start_time = time.time()\n        results = []\n        \n        for query in queries:\n            results.append(self.search(query, top_k=10))\n        \n        end_time = time.time()\n        \n        return {\n            \"method\": self.method,\n            \"num_queries\": len(queries),\n            \"total_time\": end_time - start_time,\n            \"avg_time_per_query\": (end_time - start_time) / len(queries),\n            \"vocabulary_size\": len(self.vectorizer.vocabulary_)\n        }\n    \n    def compare_methods(self, query: str, other_service, top_k: int = 10) -&gt; pd.DataFrame:\n        \"\"\"\n        Compare retrieval results between this service and another one.\n        \n        Parameters:\n        -----------\n        query : str\n            The search query\n        other_service : RetrievalService\n            Another retrieval service to compare with\n        top_k : int, default=10\n            Number of top results to return\n            \n        Returns:\n        --------\n        pd.DataFrame\n            DataFrame showing the comparison of results\n        \"\"\"\n        # Get results from both services\n        results_this = self.search(query, top_k=top_k)\n        results_other = other_service.search(query, top_k=top_k)\n        \n        # Create a DataFrame for comparison\n        df_this = pd.DataFrame(results_this, columns=['document_id', f'{self.method}_score'])\n        df_other = pd.DataFrame(results_other, columns=['document_id', f'{other_service.method}_score'])\n        \n        # Find documents in both result sets\n        common_ids = set(df_this['document_id']).intersection(set(df_other['document_id']))\n        \n        # Calculate overlap percentage\n        overlap_pct = len(common_ids) / top_k * 100\n        \n        print(f\"Query: '{query}'\")\n        print(f\"Results overlap: {len(common_ids)}/{top_k} documents ({overlap_pct:.1f}%)\")\n        \n        # Create a comparison DataFrame\n        comparison = pd.DataFrame({\n            'Rank_' + self.method: range(1, top_k + 1),\n            'ID_' + self.method: df_this['document_id'],\n            'Score_' + self.method: df_this[f'{self.method}_score'],\n            'Rank_' + other_service.method: range(1, top_k + 1),\n            'ID_' + other_service.method: df_other['document_id'],\n            'Score_' + other_service.method: df_other[f'{other_service.method}_score']\n        })\n        \n        return comparison\n</pre> from typing import List, Tuple, Dict, Union from sklearn.metrics.pairwise import cosine_similarity  class RetrievalService:     \"\"\"Service for retrieving and ranking documents based on query similarity.\"\"\"          def __init__(self, documents: List[str], document_ids: List = None, method: str = \"tfidf\"):         \"\"\"         Initialize the retrieval service with a corpus of documents.                  Parameters:         -----------         documents : List[str]             List of document texts to index         document_ids : List, optional             List of identifiers for the documents (if None, indices will be used)         method : str, default=\"tfidf\"             Vectorization method to use, either \"tfidf\" or \"bm25\"         \"\"\"         self.documents = documents         self.document_ids = document_ids if document_ids is not None else list(range(len(documents)))                  if len(self.documents) != len(self.document_ids):             raise ValueError(\"Number of documents and document IDs must match\")                  # Create a mapping from document ID to index         self.id_to_index = {doc_id: idx for idx, doc_id in enumerate(self.document_ids)}                  # Initialize the appropriate vectorizer         if method.lower() == \"tfidf\":             self.vectorizer = TfidfVectorizer(                 min_df=5, max_df=0.7, ngram_range=(1, 2),                 sublinear_tf=True, norm='l2', stop_words='english'             )             self.method = \"TF-IDF\"         elif method.lower() == \"bm25\":             self.vectorizer = BM25Vectorizer(                 min_df=5, max_df=0.7, ngram_range=(1, 2),                 k1=1.5, b=0.75, norm='l2', stop_words='english'             )             self.method = \"BM25\"         else:             raise ValueError(\"Method must be either 'tfidf' or 'bm25'\")                  # Fit the vectorizer on the document corpus         self.document_vectors = self.vectorizer.fit_transform(self.documents)                  print(f\"RetrievalService initialized with {len(self.documents)} documents using {self.method}\")         print(f\"Vocabulary size: {len(self.vectorizer.vocabulary_)}\")          def search(self, query: str, top_k: int = 10, return_scores: bool = True) -&gt; Union[List, List[Tuple]]:         \"\"\"         Search for documents similar to the query.                  Parameters:         -----------         query : str             The search query         top_k : int, default=10             Number of top results to return         return_scores : bool, default=True             Whether to return similarity scores along with document IDs                      Returns:         --------         List or List[Tuple]             If return_scores is False: List of document IDs             If return_scores is True: List of tuples (document_id, similarity_score)         \"\"\"         # Transform the query using the same vectorizer         query_vector = self.vectorizer.transform([query])                  # Calculate cosine similarities between query and all documents         similarities = cosine_similarity(query_vector, self.document_vectors)[0]                  # Get the indices of the top-k most similar documents         top_indices = similarities.argsort()[-top_k:][::-1]                  # Convert indices to document IDs         top_ids = [self.document_ids[idx] for idx in top_indices]                  if return_scores:             # Return document IDs with their similarity scores             top_scores = [similarities[idx] for idx in top_indices]             return list(zip(top_ids, top_scores))         else:             # Return only document IDs             return top_ids          def evaluate_performance(self, queries: List[str]) -&gt; Dict:         \"\"\"         Evaluate the performance of the retrieval service on a set of queries.                  Parameters:         -----------         queries : List[str]             List of queries to evaluate                      Returns:         --------         Dict             Dictionary containing performance metrics         \"\"\"         start_time = time.time()         results = []                  for query in queries:             results.append(self.search(query, top_k=10))                  end_time = time.time()                  return {             \"method\": self.method,             \"num_queries\": len(queries),             \"total_time\": end_time - start_time,             \"avg_time_per_query\": (end_time - start_time) / len(queries),             \"vocabulary_size\": len(self.vectorizer.vocabulary_)         }          def compare_methods(self, query: str, other_service, top_k: int = 10) -&gt; pd.DataFrame:         \"\"\"         Compare retrieval results between this service and another one.                  Parameters:         -----------         query : str             The search query         other_service : RetrievalService             Another retrieval service to compare with         top_k : int, default=10             Number of top results to return                      Returns:         --------         pd.DataFrame             DataFrame showing the comparison of results         \"\"\"         # Get results from both services         results_this = self.search(query, top_k=top_k)         results_other = other_service.search(query, top_k=top_k)                  # Create a DataFrame for comparison         df_this = pd.DataFrame(results_this, columns=['document_id', f'{self.method}_score'])         df_other = pd.DataFrame(results_other, columns=['document_id', f'{other_service.method}_score'])                  # Find documents in both result sets         common_ids = set(df_this['document_id']).intersection(set(df_other['document_id']))                  # Calculate overlap percentage         overlap_pct = len(common_ids) / top_k * 100                  print(f\"Query: '{query}'\")         print(f\"Results overlap: {len(common_ids)}/{top_k} documents ({overlap_pct:.1f}%)\")                  # Create a comparison DataFrame         comparison = pd.DataFrame({             'Rank_' + self.method: range(1, top_k + 1),             'ID_' + self.method: df_this['document_id'],             'Score_' + self.method: df_this[f'{self.method}_score'],             'Rank_' + other_service.method: range(1, top_k + 1),             'ID_' + other_service.method: df_other['document_id'],             'Score_' + other_service.method: df_other[f'{other_service.method}_score']         })                  return comparison In\u00a0[12]: Copied! <pre>#Let's initialize the retrieval services\ntfidf_service = RetrievalService(train_df['text'].tolist(), train_df.index.tolist(), method=\"tfidf\")\nbm25_service = RetrievalService(train_df['text'].tolist(), train_df.index.tolist(), method=\"bm25\")\n</pre> #Let's initialize the retrieval services tfidf_service = RetrievalService(train_df['text'].tolist(), train_df.index.tolist(), method=\"tfidf\") bm25_service = RetrievalService(train_df['text'].tolist(), train_df.index.tolist(), method=\"bm25\") <pre>RetrievalService initialized with 25000 documents using TF-IDF\nVocabulary size: 81097\nRetrievalService initialized with 25000 documents using BM25\nVocabulary size: 81097\n</pre> In\u00a0[13]: Copied! <pre>#Here are our queries:\n\nqueries = [\n    \"Which recent horror films are considered the scariest according to IMDB reviews?\",\n    \"What do reviewers say about the comedic elements in popular action movies?\",\n    \"How do IMDB users feel about the performances of child actors in family films?\",\n    \"Are there any standout romantic comedies with high user ratings?\",\n    \"What criticisms do reviewers have about the pacing in thriller movies?\",\n    \"Do IMDB users consider classic black-and-white films overrated or underrated?\",\n    \"How do audiences respond to major plot twists in mystery or crime dramas?\",\n    \"What are the most common complaints about special effects in sci-fi movies?\",\n    \"How do reviewers compare the film adaptation to the original book?\",\n    \"Are there any animated features that appeal strongly to both kids and adults?\"\n]\n\nfor query in queries[:2]:\n    print(f\"\\nQuery: '{query}'\")\n    print(\"\\n--- TF-IDF Results ---\")\n    tfidf_results = tfidf_service.search(query)\n    for doc_id, score in tfidf_results:\n        idx = tfidf_service.id_to_index[doc_id]\n        print(f\"Score: {score:.4f} - {doc_id}: {train_df['text'][idx][:80]}...\")\n    \n    print(\"\\n--- BM25 Results ---\")\n    bm25_results = bm25_service.search(query)\n    for doc_id, score in bm25_results:\n        idx = bm25_service.id_to_index[doc_id]\n        print(f\"Score: {score:.4f} - {doc_id}: {train_df['text'][idx][:80]}...\")\n</pre> #Here are our queries:  queries = [     \"Which recent horror films are considered the scariest according to IMDB reviews?\",     \"What do reviewers say about the comedic elements in popular action movies?\",     \"How do IMDB users feel about the performances of child actors in family films?\",     \"Are there any standout romantic comedies with high user ratings?\",     \"What criticisms do reviewers have about the pacing in thriller movies?\",     \"Do IMDB users consider classic black-and-white films overrated or underrated?\",     \"How do audiences respond to major plot twists in mystery or crime dramas?\",     \"What are the most common complaints about special effects in sci-fi movies?\",     \"How do reviewers compare the film adaptation to the original book?\",     \"Are there any animated features that appeal strongly to both kids and adults?\" ]  for query in queries[:2]:     print(f\"\\nQuery: '{query}'\")     print(\"\\n--- TF-IDF Results ---\")     tfidf_results = tfidf_service.search(query)     for doc_id, score in tfidf_results:         idx = tfidf_service.id_to_index[doc_id]         print(f\"Score: {score:.4f} - {doc_id}: {train_df['text'][idx][:80]}...\")          print(\"\\n--- BM25 Results ---\")     bm25_results = bm25_service.search(query)     for doc_id, score in bm25_results:         idx = bm25_service.id_to_index[doc_id]         print(f\"Score: {score:.4f} - {doc_id}: {train_df['text'][idx][:80]}...\") <pre>\nQuery: 'Which recent horror films are considered the scariest according to IMDB reviews?'\n\n--- TF-IDF Results ---\nScore: 0.1441 - 12656: Robert Duvall is a direct descendent of Confederate General Robert E. Lee, accor...\nScore: 0.1335 - 3330: It seems like more consideration has gone into the IMDb reviews of this film tha...\nScore: 0.1281 - 6303: This is the last time I rent a video without checking in at the IMDB reviews. Th...\nScore: 0.1218 - 23136: Amazing documentary. Saw it on original airdate and on DVD a few times in the la...\nScore: 0.1196 - 24640: This film gave me nightmares for months and I'm 17. This is the scariest movie e...\nScore: 0.1110 - 22350: 'How to Lose Friends and Alienate People' is an entertaining, if loose, adaptati...\nScore: 0.1045 - 6645: Reading my review of THE HOUSE THAT SCREAMED, many may assume that I'm some 14 y...\nScore: 0.0996 - 9117: You know that feeling of hilarity you get when you watch a film that's trying so...\nScore: 0.0980 - 2856: Oh dear. This movie could have been sub-titled \"When writers go on strike!\" What...\nScore: 0.0956 - 24837: Wonderful film, one of the best horror films of the 70s. She is realistic settin...\n\n--- BM25 Results ---\nScore: 0.1458 - 12656: Robert Duvall is a direct descendent of Confederate General Robert E. Lee, accor...\nScore: 0.1378 - 3330: It seems like more consideration has gone into the IMDb reviews of this film tha...\nScore: 0.1326 - 6303: This is the last time I rent a video without checking in at the IMDB reviews. Th...\nScore: 0.1271 - 23136: Amazing documentary. Saw it on original airdate and on DVD a few times in the la...\nScore: 0.1177 - 24640: This film gave me nightmares for months and I'm 17. This is the scariest movie e...\nScore: 0.1132 - 22350: 'How to Lose Friends and Alienate People' is an entertaining, if loose, adaptati...\nScore: 0.0996 - 9117: You know that feeling of hilarity you get when you watch a film that's trying so...\nScore: 0.0968 - 2856: Oh dear. This movie could have been sub-titled \"When writers go on strike!\" What...\nScore: 0.0956 - 10208: FLIGHT OF FURY takes the mantle of being the very WORST Steven Seagal flick I've...\nScore: 0.0941 - 11712: An okay film, with a fine leading lady, but a terrible leading man. Stephan Jenk...\n\nQuery: 'What do reviewers say about the comedic elements in popular action movies?'\n\n--- TF-IDF Results ---\nScore: 0.1898 - 21667: This is NOT as bad a movie as some reviewers, and as the summary at the IMDB pag...\nScore: 0.1363 - 10660: What happened? What we have here is basically a solid and plausible premise and ...\nScore: 0.1307 - 9699: This has to be one of the worst films I have ever seen. The DVD was given to me ...\nScore: 0.1277 - 5197: Wow, could have been such a good movie,Starts of with Brittany Daniels tied up, ...\nScore: 0.1260 - 14880: ... but I enjoyed this show anyway. I've been reading some of the comments prior...\nScore: 0.1250 - 21681: Purple Rain is a great film if you are a 40-something music lover who partied to...\nScore: 0.1096 - 24987: I enjoyed this movie. Unlike like some of the pumped up, steroid trash that is p...\nScore: 0.1066 - 10062: I was very displeased with this move. Everything was terrible from the start. Th...\nScore: 0.1026 - 17012: I'm a HUGE fan of the twin sisters. Although this was one of their \"not soo good...\nScore: 0.1024 - 4861: I loved watching the original Azumi with its mix of live action manga, compellin...\n\n--- BM25 Results ---\nScore: 0.1661 - 21667: This is NOT as bad a movie as some reviewers, and as the summary at the IMDB pag...\nScore: 0.1455 - 10660: What happened? What we have here is basically a solid and plausible premise and ...\nScore: 0.1396 - 9699: This has to be one of the worst films I have ever seen. The DVD was given to me ...\nScore: 0.1350 - 21681: Purple Rain is a great film if you are a 40-something music lover who partied to...\nScore: 0.1336 - 14880: ... but I enjoyed this show anyway. I've been reading some of the comments prior...\nScore: 0.1322 - 5197: Wow, could have been such a good movie,Starts of with Brittany Daniels tied up, ...\nScore: 0.1024 - 23257: &lt;br /&gt;&lt;br /&gt;I must admit, I was expecting something quite different from my firs...\nScore: 0.1021 - 4861: I loved watching the original Azumi with its mix of live action manga, compellin...\nScore: 0.0918 - 24987: I enjoyed this movie. Unlike like some of the pumped up, steroid trash that is p...\nScore: 0.0876 - 8161: As someone who has both read the novel and seen the film, I have a different tak...\n</pre> In\u00a0[14]: Copied! <pre>print(\"\\n--- Performance Evaluation ---\")\ncomparison = tfidf_service.compare_methods(queries[8], bm25_service)\n\ncomparison = pd.DataFrame(comparison)\ncomparison\n</pre> print(\"\\n--- Performance Evaluation ---\") comparison = tfidf_service.compare_methods(queries[8], bm25_service)  comparison = pd.DataFrame(comparison) comparison <pre>\n--- Performance Evaluation ---\nQuery: 'How do reviewers compare the film adaptation to the original book?'\nResults overlap: 8/10 documents (80.0%)\n</pre> Out[14]: Rank_TF-IDF ID_TF-IDF Score_TF-IDF Rank_BM25 ID_BM25 Score_BM25 0 1 16056 0.154307 1 16056 0.148786 1 2 8154 0.136172 2 14123 0.138861 2 3 14123 0.132292 3 14699 0.123456 3 4 15152 0.130862 4 16295 0.120684 4 5 14699 0.129979 5 8154 0.119105 5 6 24402 0.129372 6 15151 0.117781 6 7 623 0.125617 7 15152 0.117431 7 8 9344 0.123964 8 15503 0.115723 8 9 15151 0.121106 9 24402 0.111202 9 10 16295 0.119595 10 3402 0.110743 <p>We see a few things:</p> <ul> <li>Generally we have at least 70% of overlap between the two methods</li> <li>Generally they agree on the top results</li> <li>The main differences start to appear after 5-6 results</li> <li>This could lead to clear changes in the ranking of your web search results therefore in your user experience</li> <li>Indeed if we had to retrieve documents to feed a LLM to answer a question, the final answer could be very different depending on the method we use as we would feed with different documents.</li> </ul> In\u00a0[15]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport time\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set up the classifier pipelines\n\nvectorizer_params = {\n    'min_df': 3,\n    'max_df': 0.85,\n    'ngram_range': (1, 2),\n    'stop_words': 'english'\n}\n\n# Define the vectorizers\nvectorizers = {\n    'tfidf': TfidfVectorizer(**vectorizer_params),\n    'bm25': BM25Vectorizer(**vectorizer_params)\n}\n\n# Define the classifiers\nclassifiers = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n    'MultinomialNB': MultinomialNB(),\n    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42)\n}\n\npipelines = {}\n\n# Create pipeline for each vectorizer-classifier combination\nfor vec_name in ['tfidf', 'bm25']:\n    for clf_name, clf in classifiers.items():\n        pipe_name = f\"{vec_name}_{clf_name}\"\n        pipelines[pipe_name] = Pipeline([\n            ('vectorizer', vectorizers[vec_name]),\n            ('classifier', clf)\n        ])\n\n\npipelines \n</pre> from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import MultinomialNB from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score import time from tqdm.notebook import tqdm import warnings warnings.filterwarnings('ignore')  # Set up the classifier pipelines  vectorizer_params = {     'min_df': 3,     'max_df': 0.85,     'ngram_range': (1, 2),     'stop_words': 'english' }  # Define the vectorizers vectorizers = {     'tfidf': TfidfVectorizer(**vectorizer_params),     'bm25': BM25Vectorizer(**vectorizer_params) }  # Define the classifiers classifiers = {     'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),     'MultinomialNB': MultinomialNB(),     'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42) }  pipelines = {}  # Create pipeline for each vectorizer-classifier combination for vec_name in ['tfidf', 'bm25']:     for clf_name, clf in classifiers.items():         pipe_name = f\"{vec_name}_{clf_name}\"         pipelines[pipe_name] = Pipeline([             ('vectorizer', vectorizers[vec_name]),             ('classifier', clf)         ])   pipelines  Out[15]: <pre>{'tfidf_LogisticRegression': Pipeline(steps=[('vectorizer',\n                  TfidfVectorizer(max_df=0.85, min_df=3, ngram_range=(1, 2),\n                                  stop_words='english')),\n                 ('classifier',\n                  LogisticRegression(max_iter=1000, random_state=42))]),\n 'tfidf_MultinomialNB': Pipeline(steps=[('vectorizer',\n                  TfidfVectorizer(max_df=0.85, min_df=3, ngram_range=(1, 2),\n                                  stop_words='english')),\n                 ('classifier', MultinomialNB())]),\n 'tfidf_RandomForest': Pipeline(steps=[('vectorizer',\n                  TfidfVectorizer(max_df=0.85, min_df=3, ngram_range=(1, 2),\n                                  stop_words='english')),\n                 ('classifier', RandomForestClassifier(random_state=42))]),\n 'bm25_LogisticRegression': Pipeline(steps=[('vectorizer',\n                  BM25Vectorizer(max_df=0.85, min_df=3, ngram_range=(1, 2),\n                                 stop_words='english')),\n                 ('classifier',\n                  LogisticRegression(max_iter=1000, random_state=42))]),\n 'bm25_MultinomialNB': Pipeline(steps=[('vectorizer',\n                  BM25Vectorizer(max_df=0.85, min_df=3, ngram_range=(1, 2),\n                                 stop_words='english')),\n                 ('classifier', MultinomialNB())]),\n 'bm25_RandomForest': Pipeline(steps=[('vectorizer',\n                  BM25Vectorizer(max_df=0.85, min_df=3, ngram_range=(1, 2),\n                                 stop_words='english')),\n                 ('classifier', RandomForestClassifier(random_state=42))])}</pre> In\u00a0[16]: Copied! <pre>results = {\n        'Pipeline': [],\n        'Vectorizer': [],\n        'Classifier': [],\n        'Accuracy': [],\n        'Precision': [],\n        'Recall': [],\n        'F1': [],\n        'Training Time': [],\n        'Prediction Time': []\n    }\n    \n# Dictionary to store all predictions for visualizations\nall_predictions = {}\n\n# Process each pipeline\nfor pipe_name, pipeline in pipelines.items():\n    print(f\"Training pipeline: {pipe_name}\")\n    \n    # Extract features\n    start_time = time.time()\n    X_train = train_df['text']  \n    X_test = test_df['text']\n    \n    y_train = train_df['label'].values\n    y_test = test_df['label'].values\n    \n    # Train the classifier\n    pipeline.fit(X_train, y_train)\n    training_time = time.time() - start_time\n    \n    # Make predictions\n    start_time = time.time()\n    y_pred = pipeline.predict(X_test)\n    \n    prediction_time = time.time() - start_time\n    \n    # Store predictions\n    all_predictions[pipe_name] = y_pred\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, zero_division=0)\n    recall = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n    \n    # Store results\n    results['Pipeline'].append(pipe_name)\n    results['Vectorizer'].append(pipeline['vectorizer'].__class__.__name__)\n    results['Classifier'].append(pipeline['classifier'].__class__.__name__)\n    results['Accuracy'].append(accuracy)\n    results['Precision'].append(precision)\n    results['Recall'].append(recall)\n    results['F1'].append(f1)\n    results['Training Time'].append(training_time)\n    results['Prediction Time'].append(prediction_time)\n    \npd.DataFrame(results)\n</pre> results = {         'Pipeline': [],         'Vectorizer': [],         'Classifier': [],         'Accuracy': [],         'Precision': [],         'Recall': [],         'F1': [],         'Training Time': [],         'Prediction Time': []     }      # Dictionary to store all predictions for visualizations all_predictions = {}  # Process each pipeline for pipe_name, pipeline in pipelines.items():     print(f\"Training pipeline: {pipe_name}\")          # Extract features     start_time = time.time()     X_train = train_df['text']       X_test = test_df['text']          y_train = train_df['label'].values     y_test = test_df['label'].values          # Train the classifier     pipeline.fit(X_train, y_train)     training_time = time.time() - start_time          # Make predictions     start_time = time.time()     y_pred = pipeline.predict(X_test)          prediction_time = time.time() - start_time          # Store predictions     all_predictions[pipe_name] = y_pred          # Calculate metrics     accuracy = accuracy_score(y_test, y_pred)     precision = precision_score(y_test, y_pred, zero_division=0)     recall = recall_score(y_test, y_pred, zero_division=0)     f1 = f1_score(y_test, y_pred, zero_division=0)          # Store results     results['Pipeline'].append(pipe_name)     results['Vectorizer'].append(pipeline['vectorizer'].__class__.__name__)     results['Classifier'].append(pipeline['classifier'].__class__.__name__)     results['Accuracy'].append(accuracy)     results['Precision'].append(precision)     results['Recall'].append(recall)     results['F1'].append(f1)     results['Training Time'].append(training_time)     results['Prediction Time'].append(prediction_time)      pd.DataFrame(results) <pre>Training pipeline: tfidf_LogisticRegression\nTraining pipeline: tfidf_MultinomialNB\nTraining pipeline: tfidf_RandomForest\nTraining pipeline: bm25_LogisticRegression\nTraining pipeline: bm25_MultinomialNB\nTraining pipeline: bm25_RandomForest\n</pre> Out[16]: Pipeline Vectorizer Classifier Accuracy Precision Recall F1 Training Time Prediction Time 0 tfidf_LogisticRegression TfidfVectorizer LogisticRegression 0.88168 0.881009 0.88256 0.881784 5.012910 3.684035 1 tfidf_MultinomialNB TfidfVectorizer MultinomialNB 0.85532 0.879388 0.82360 0.850580 4.462019 3.450452 2 tfidf_RandomForest TfidfVectorizer RandomForestClassifier 0.85464 0.864017 0.84176 0.852743 37.430788 4.870121 3 bm25_LogisticRegression BM25Vectorizer LogisticRegression 0.89084 0.896583 0.88360 0.890044 9.400657 3.878614 4 bm25_MultinomialNB BM25Vectorizer MultinomialNB 0.84132 0.869362 0.80336 0.835059 10.483180 3.824785 5 bm25_RandomForest BM25Vectorizer RandomForestClassifier 0.85620 0.860965 0.84960 0.855245 36.839044 4.850694 In\u00a0[17]: Copied! <pre>results_df = pd.DataFrame(results)\nplt.figure(figsize=(15, 10))\nmetrics = ['Precision', 'Recall', 'F1']\n\nfor i, metric in enumerate(metrics, 1):\n    plt.subplot(2, 2, i)\n    \n    # Group by vectorizer and classifier\n    grouped = results_df.pivot(index='Classifier', columns='Vectorizer', values=metric)\n    \n    # Plot grouped bar chart\n    ax = grouped.plot(kind='bar', ax=plt.gca())\n    plt.title(f'{metric} by Classifier and Vectorizer')\n    plt.ylabel(metric)\n    plt.ylim(0, 1)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    \n    # Add value labels\n    for container in ax.containers:\n        ax.bar_label(container, fmt='%.3f', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n</pre> results_df = pd.DataFrame(results) plt.figure(figsize=(15, 10)) metrics = ['Precision', 'Recall', 'F1']  for i, metric in enumerate(metrics, 1):     plt.subplot(2, 2, i)          # Group by vectorizer and classifier     grouped = results_df.pivot(index='Classifier', columns='Vectorizer', values=metric)          # Plot grouped bar chart     ax = grouped.plot(kind='bar', ax=plt.gca())     plt.title(f'{metric} by Classifier and Vectorizer')     plt.ylabel(metric)     plt.ylim(0, 1)     plt.grid(axis='y', linestyle='--', alpha=0.7)          # Add value labels     for container in ax.containers:         ax.bar_label(container, fmt='%.3f', fontsize=8)  plt.tight_layout() plt.show() <p>That's interesting. we see that the reuslts are not that different (less than 2 points generally). And the logistic regression and random forest shows that the BM25 is slightly better than the TF-IDF. The multinomial naive bayes shows the opposite.</p> <p>The results follows the observation we made on the retrieval services.</p> In\u00a0[18]: Copied! <pre>plt.figure(figsize=(12, 6))\n    \n# Create a time comparison plot\ntime_df = results_df.pivot(index='Classifier', columns='Vectorizer', values='Training Time')\nax = time_df.plot(kind='bar', color=['blue', 'red'], ax=plt.gca())\n\n# Combine legends\nhandles, labels = plt.gca().get_legend_handles_labels()\nplt.legend(handles[:2] + handles[2:4], \n            ['TF-IDF Training', 'BM25 Training'],\n            loc='upper left')\n\nplt.title('Training Time Comparison')\nplt.ylabel('Time (seconds)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(12, 6))      # Create a time comparison plot time_df = results_df.pivot(index='Classifier', columns='Vectorizer', values='Training Time') ax = time_df.plot(kind='bar', color=['blue', 'red'], ax=plt.gca())  # Combine legends handles, labels = plt.gca().get_legend_handles_labels() plt.legend(handles[:2] + handles[2:4],              ['TF-IDF Training', 'BM25 Training'],             loc='upper left')  plt.title('Training Time Comparison') plt.ylabel('Time (seconds)') plt.grid(axis='y', linestyle='--', alpha=0.7) plt.tight_layout() plt.show() <p>On the training time it looks like the BM25 is slightly faster than the TF-IDF.</p> In\u00a0[19]: Copied! <pre>pred_df = pd.DataFrame({k: v for k, v in all_predictions.items()})\n\nplt.figure(figsize=(10, 8))\ncorr_matrix = pred_df.corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\nplt.title('Prediction Correlation Between Pipelines')\nplt.tight_layout()\nplt.show()\n</pre> pred_df = pd.DataFrame({k: v for k, v in all_predictions.items()})  plt.figure(figsize=(10, 8)) corr_matrix = pred_df.corr() sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0) plt.title('Prediction Correlation Between Pipelines') plt.tight_layout() plt.show() <p>We see again that we have high correlation between outputs when we look at the same model. Like 0.89 for logistic regression 0.85 for multinomial and 0.84 for Random Forest.</p> <p>There are differences obviously, let's look maybe at the differences in the interpretation of the results for logistic regression for instance.</p> In\u00a0[20]: Copied! <pre>from IPython.display import display, HTML\n\n# Function to extract token weights from vectorizer\ndef get_token_weights(text, vectorizer):\n    \"\"\"Extract weights for each token in a text using the given vectorizer.\"\"\"\n    # Transform the text to a vector\n    vector = vectorizer.transform([text])[0]\n    \n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Extract non-zero weights and their corresponding tokens\n    token_weights = {}\n    \n    # Split the text into tokens (considering the tokenization used by vectorizer)\n    if hasattr(vectorizer, 'preprocessor') and vectorizer.preprocessor is not None:\n        preprocessed_text = vectorizer.preprocessor(text)\n    else:\n        preprocessed_text = text\n    \n    if hasattr(vectorizer, 'tokenizer') and vectorizer.tokenizer is not None:\n        tokens = vectorizer.tokenizer(preprocessed_text)\n    else:\n        # Default token pattern\n        import re\n        tokens = re.findall(vectorizer.token_pattern, preprocessed_text)\n    \n    # Handle ngrams by creating all possible ngrams from tokens\n    ngrams = []\n    for n in range(vectorizer.ngram_range[0], vectorizer.ngram_range[1] + 1):\n        for i in range(len(tokens) - n + 1):\n            ngrams.append(' '.join(tokens[i:i+n]))\n    \n    # Create a mapping from token indices to weights\n    indices = vector.indices\n    data = vector.data\n    \n    # Map feature indices to weights\n    idx_to_weight = {idx: weight for idx, weight in zip(indices, data)}\n    \n    # Map tokens to weights\n    for token in set(ngrams):\n        if token.lower() in feature_names:\n            idx = np.where(feature_names == token.lower())[0][0]\n            if idx in idx_to_weight:\n                token_weights[token] = idx_to_weight[idx]\n    \n    return token_weights\n\n# Function to highlight text with both TF-IDF and BM25 weights\ndef highlight_text_comparison(text, tfidf_vectorizer, bm25_vectorizer, threshold=0.01, background=False):\n    \"\"\"\n    Highlight tokens in text with different colors based on TF-IDF and BM25 weights.\n    \n    Parameters:\n    -----------\n    text : str\n        Text to highlight\n    tfidf_vectorizer : TfidfVectorizer\n        Fitted TF-IDF vectorizer\n    bm25_vectorizer : BM25Vectorizer\n        Fitted BM25 vectorizer\n    threshold : float, default=0.01\n        Minimum weight to highlight\n    background : bool, default=False\n        Whether to highlight background instead of text\n    \n    Returns:\n    --------\n    str\n        HTML-formatted text with highlights\n    \"\"\"\n    # Get token weights\n    tfidf_weights = get_token_weights(text, tfidf_vectorizer)\n    bm25_weights = get_token_weights(text, bm25_vectorizer)\n    \n    # Normalize weights to [0, 1] range for better visualization\n    max_tfidf = max(tfidf_weights.values()) if tfidf_weights else 1\n    max_bm25 = max(bm25_weights.values()) if bm25_weights else 1\n    \n    # Split text into words and punctuation\n    import re\n    tokens = re.findall(r'\\b\\w[\\w\\']*\\b|[^\\w\\s]', text)\n    \n    # Create highlighted HTML\n    highlighted = []\n    for i, token in enumerate(tokens):\n        token_lower = token.lower()\n        \n        # Get normalized weights\n        tfidf_weight = tfidf_weights.get(token_lower, 0) / max_tfidf if max_tfidf &gt; 0 else 0\n        bm25_weight = bm25_weights.get(token_lower, 0) / max_bm25 if max_bm25 &gt; 0 else 0\n        \n        # Determine if token should be highlighted\n        if max(tfidf_weight, bm25_weight) &gt; threshold:\n            # Calculate RGB values for highlighting\n            r = int(max(0, min(255, 255 * (1 - tfidf_weight))))\n            g = int(max(0, min(255, 255 * (1 - bm25_weight))))\n            b = 255  # Blue component remains constant\n            \n            if background:\n                # Highlight background with opposite color\n                bg_r = 255 - r\n                bg_g = 255 - g\n                bg_b = 255 - b\n                token = f'&lt;span style=\"background-color:rgb({bg_r},{bg_g},{bg_b})\"&gt;{token}&lt;/span&gt;'\n            else:\n                # Highlight text color\n                token = f'&lt;span style=\"color:rgb({r},{g},{b})\"&gt;{token}&lt;/span&gt;'\n        \n        highlighted.append(token)\n    \n    # Add a legend explaining the colors\n    legend = \"\"\"\n    &lt;div style=\"margin-top: 10px; font-size: 12px;\"&gt;\n        &lt;b&gt;Color Legend:&lt;/b&gt;&lt;br&gt;\n        &lt;span style=\"color:rgb(255,0,255)\"&gt;Purple&lt;/span&gt;: Low weight in both methods&lt;br&gt;\n        &lt;span style=\"color:rgb(0,0,255)\"&gt;Blue&lt;/span&gt;: High weight in both methods&lt;br&gt;\n        &lt;span style=\"color:rgb(255,0,0)\"&gt;Red&lt;/span&gt;: High TF-IDF weight, Low BM25 weight&lt;br&gt;\n        &lt;span style=\"color:rgb(0,255,0)\"&gt;Green&lt;/span&gt;: Low TF-IDF weight, High BM25 weight\n    &lt;/div&gt;\n    \"\"\"\n    \n    return ' '.join(highlighted) + legend\n\n\n# Main function to display comprehensive comparison\ndef compare_vectorization_methods(reviews, tfidf_vectorizer=None, bm25_vectorizer=None):\n    \"\"\"\n    Display comprehensive comparison of TF-IDF and BM25 vectorization for reviews.\n    \n    Parameters:\n    -----------\n    reviews : list of str\n        List of reviews to analyze\n    tfidf_vectorizer : TfidfVectorizer, optional\n        Pre-fitted TF-IDF vectorizer\n    bm25_vectorizer : BM25Vectorizer, optional\n        Pre-fitted BM25 vectorizer\n        \n    Returns:\n    --------\n    None\n        Displays HTML visualizations\n    \"\"\"\n    \n    # Display comparison for each review\n    for i, review in enumerate(reviews):\n        print(f\"Review #{i+1}:\")\n        print(\"-\" * 80)\n        print(f\"Text: {review[:150]}...\" if len(review) &gt; 150 else review)\n        print()\n        \n        # Display combined highlighting\n        display(HTML(f\"&lt;h3&gt;Combined Highlighting (Review #{i+1})&lt;/h3&gt;\"))\n        display(HTML(highlight_text_comparison(review, tfidf_vectorizer, bm25_vectorizer)))\n        \n        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n\n# Example usage (replace this with your actual reviews dataset)\n# Where both model differs\n\nsample_reviews = test_df.loc[all_predictions['tfidf_LogisticRegression'] != all_predictions['bm25_LogisticRegression'], 'text'].sample(5, replace=False, random_state=42).tolist()\n\n# Display the comparison\ncompare_vectorization_methods(sample_reviews, \n                              pipelines['tfidf_LogisticRegression']['vectorizer'],\n                               pipelines['bm25_LogisticRegression']['vectorizer'])\n</pre> from IPython.display import display, HTML  # Function to extract token weights from vectorizer def get_token_weights(text, vectorizer):     \"\"\"Extract weights for each token in a text using the given vectorizer.\"\"\"     # Transform the text to a vector     vector = vectorizer.transform([text])[0]          # Get the feature names     feature_names = vectorizer.get_feature_names_out()          # Extract non-zero weights and their corresponding tokens     token_weights = {}          # Split the text into tokens (considering the tokenization used by vectorizer)     if hasattr(vectorizer, 'preprocessor') and vectorizer.preprocessor is not None:         preprocessed_text = vectorizer.preprocessor(text)     else:         preprocessed_text = text          if hasattr(vectorizer, 'tokenizer') and vectorizer.tokenizer is not None:         tokens = vectorizer.tokenizer(preprocessed_text)     else:         # Default token pattern         import re         tokens = re.findall(vectorizer.token_pattern, preprocessed_text)          # Handle ngrams by creating all possible ngrams from tokens     ngrams = []     for n in range(vectorizer.ngram_range[0], vectorizer.ngram_range[1] + 1):         for i in range(len(tokens) - n + 1):             ngrams.append(' '.join(tokens[i:i+n]))          # Create a mapping from token indices to weights     indices = vector.indices     data = vector.data          # Map feature indices to weights     idx_to_weight = {idx: weight for idx, weight in zip(indices, data)}          # Map tokens to weights     for token in set(ngrams):         if token.lower() in feature_names:             idx = np.where(feature_names == token.lower())[0][0]             if idx in idx_to_weight:                 token_weights[token] = idx_to_weight[idx]          return token_weights  # Function to highlight text with both TF-IDF and BM25 weights def highlight_text_comparison(text, tfidf_vectorizer, bm25_vectorizer, threshold=0.01, background=False):     \"\"\"     Highlight tokens in text with different colors based on TF-IDF and BM25 weights.          Parameters:     -----------     text : str         Text to highlight     tfidf_vectorizer : TfidfVectorizer         Fitted TF-IDF vectorizer     bm25_vectorizer : BM25Vectorizer         Fitted BM25 vectorizer     threshold : float, default=0.01         Minimum weight to highlight     background : bool, default=False         Whether to highlight background instead of text          Returns:     --------     str         HTML-formatted text with highlights     \"\"\"     # Get token weights     tfidf_weights = get_token_weights(text, tfidf_vectorizer)     bm25_weights = get_token_weights(text, bm25_vectorizer)          # Normalize weights to [0, 1] range for better visualization     max_tfidf = max(tfidf_weights.values()) if tfidf_weights else 1     max_bm25 = max(bm25_weights.values()) if bm25_weights else 1          # Split text into words and punctuation     import re     tokens = re.findall(r'\\b\\w[\\w\\']*\\b|[^\\w\\s]', text)          # Create highlighted HTML     highlighted = []     for i, token in enumerate(tokens):         token_lower = token.lower()                  # Get normalized weights         tfidf_weight = tfidf_weights.get(token_lower, 0) / max_tfidf if max_tfidf &gt; 0 else 0         bm25_weight = bm25_weights.get(token_lower, 0) / max_bm25 if max_bm25 &gt; 0 else 0                  # Determine if token should be highlighted         if max(tfidf_weight, bm25_weight) &gt; threshold:             # Calculate RGB values for highlighting             r = int(max(0, min(255, 255 * (1 - tfidf_weight))))             g = int(max(0, min(255, 255 * (1 - bm25_weight))))             b = 255  # Blue component remains constant                          if background:                 # Highlight background with opposite color                 bg_r = 255 - r                 bg_g = 255 - g                 bg_b = 255 - b                 token = f'{token}'             else:                 # Highlight text color                 token = f'{token}'                  highlighted.append(token)          # Add a legend explaining the colors     legend = \"\"\"      Color Legend: Purple: Low weight in both methods Blue: High weight in both methods Red: High TF-IDF weight, Low BM25 weight Green: Low TF-IDF weight, High BM25 weight          \"\"\"          return ' '.join(highlighted) + legend   # Main function to display comprehensive comparison def compare_vectorization_methods(reviews, tfidf_vectorizer=None, bm25_vectorizer=None):     \"\"\"     Display comprehensive comparison of TF-IDF and BM25 vectorization for reviews.          Parameters:     -----------     reviews : list of str         List of reviews to analyze     tfidf_vectorizer : TfidfVectorizer, optional         Pre-fitted TF-IDF vectorizer     bm25_vectorizer : BM25Vectorizer, optional         Pre-fitted BM25 vectorizer              Returns:     --------     None         Displays HTML visualizations     \"\"\"          # Display comparison for each review     for i, review in enumerate(reviews):         print(f\"Review #{i+1}:\")         print(\"-\" * 80)         print(f\"Text: {review[:150]}...\" if len(review) &gt; 150 else review)         print()                  # Display combined highlighting         display(HTML(f\"Combined Highlighting (Review #{i+1})\"))         display(HTML(highlight_text_comparison(review, tfidf_vectorizer, bm25_vectorizer)))                  print(\"\\n\" + \"=\" * 80 + \"\\n\")  # Example usage (replace this with your actual reviews dataset) # Where both model differs  sample_reviews = test_df.loc[all_predictions['tfidf_LogisticRegression'] != all_predictions['bm25_LogisticRegression'], 'text'].sample(5, replace=False, random_state=42).tolist()  # Display the comparison compare_vectorization_methods(sample_reviews,                                pipelines['tfidf_LogisticRegression']['vectorizer'],                                pipelines['bm25_LogisticRegression']['vectorizer']) <pre>Review #1:\n--------------------------------------------------------------------------------\nText: The main word that comes to mind when considering this film is \"dodgy\". This is a low-quality film biography of one of the most iconic performers of a...\n\n</pre> Combined Highlighting (Review #1)  The main word that comes to mind when considering this film is \" dodgy \" . This is a low - quality film biography of one of the most iconic performers of all time . The Gloved One deserved better . &lt; br / &gt; &lt; br / &gt; Before getting into the meat of my thoughts on this biopic , I have to say that there are two things I found effective . First was the use of actual fan footage and interviews at certain points in the film , especially in the scenes depicting the first set of child molestation allegations . I feel that this contributed a certain authenticity that was * severely * lacking throughout the rest of the film . Second was the sequence depicting the courtship of Michael Jackson and Lisa Marie Presley . I will not comment on whether I believe the marriage was a sham , but by many accounts , it was a relationship where care and affection existed between the two parties involved . That really came across in this film ; Flex Anderson and Krista Rae had decent enough chemistry to pull it off . These successful points are enough to keep Man in the Mirror away from 1 - star status . &lt; br / &gt; &lt; br / &gt; That said . . . there was very little else here that worked . Very few of the actors looked like the people they were supposed to portray , most egregiously those playing Elizabeth Taylor , Janet Jackson , and Diana Ross . Also , the absence of Jackson's music was a huge loss . How can you effectively tell a story about him without his music ? ? I understand that they were unable to secure the rights to it with this being a low - budget , unauthorized production ; it seems , though , that if you can't have the man's music in a film about him , you might as well pack it up and go home , because you're missing out on an extremely important part of his life story . &lt; br / &gt; &lt; br / &gt; This film's characterization of Jackson bothered me a little , too . I won't argue that he was troubled and may have been a few fries short of a value meal , but here , he was portrayed as something close to mentally disabled . I don't believe that Jackson , known to have been a shrewd businessman , would have been quite as naive about how the adult world works as he was made out to be in this film . &lt; br / &gt; &lt; br / &gt; Finally , the way this film was written was nothing short of disgraceful . Many lines or exchanges of dialogue were either extremely corny , like Michael and Janet's \" Tinkerbell \" exchange , or nonsensical , like the \" Blanket of love \" comments made by Michael . Also , the screenwriters don't exactly have a knack for subtlety . There was a lot of telegraphing of upcoming events ( \" What could possibly go wrong ? ? \" sorts of lines ) and extremely overt hammering of themes and motifs in the film ( if I'd heard the word \" believe \" one more time . . . ) . This is what ultimately hobbled the film as something that could be considered awesomely bad . &lt; br / &gt; &lt; br / &gt; Perhaps when we are a few years , or even a decade or three , removed from Jackson's death , someone will be able to bring his story to life in a more deserving film . By that time , we might have a better perspective on his life , and someone will be able to present a truly thoughtful examination of who Michael Jackson really was and what he's meant to the world of entertainment . This very dodgy biopic was not that film .      Color Legend: Purple: Low weight in both methods Blue: High weight in both methods Red: High TF-IDF weight, Low BM25 weight Green: Low TF-IDF weight, High BM25 weight      <pre>\n================================================================================\n\nReview #2:\n--------------------------------------------------------------------------------\nText: \"Western Union\" is something of a forgotten classic western! Perhaps the reason for this lies in the fact of its unavailability on DVD in the United S...\n\n</pre> Combined Highlighting (Review #2)  \" Western Union \" is something of a forgotten classic western ! Perhaps the reason for this lies in the fact of its unavailability on DVD in the United States . However , all is not lost as it has now appeared on Region 2 in England . This - being a blessing in some ways - is not only incongruous but totally ironic when one considers that a movie depicting the founding and establishment of such a uniquely American organization as The Western Union Telegraph Company is without a Region 1 release . It beggars belief ! It simply doesn't make sense ! &lt; br / &gt; &lt; br / &gt; Produced by Fox in 1941 \" Western Union \" was directed by Fritz Lang . This was only the second occasion the great German director undertook to direct a western ! He had done an excellent job the year before with Fox's \" The Return Of Frank James \" and would have only one more western outing in 1952 with the splendid \" Rancho Notorious \" . Lang was no Ford or Hawks but with \" Western Union \" he turned in a fine solid western that holds up very well . Beautifully photographed in early three strip Technicolor by Edward Cronjager it boasted a good cast headed by Robert Young , Randolph Scott and Dean Jagger . The female lead is taken by Virginia Gilmore who really has little to do in the picture . An actress who never made anything of her career . Her presence here is merely cosmetic . &lt; br / &gt; &lt; br / &gt; It is curious that Robert Young has top billing over Scott ! It is clearly Scott's picture from the very beginning when we first see him in the film's terrific opening scene being chased by a posse across the plains . Young doesn't have much to do throughout the movie and seems out of place in a western . He just looks plain silly going up against Barton McLane in a gunfight ! An actor who never really distinguished himself - except perhaps with \" Crossfire \" ( 1947 ) - Young appeared in a string of forgettable romantic comedies in the forties and fifties culminating with his greatest success when for seven years he was TV's \" Marcus Welby MD \" in the seventies . He died in 1998 at the age of 91 . &lt; br / &gt; &lt; br / &gt; \" Western Union \" recounts the connection by telegraph wire of Omaha and Salt Lake City . Scott plays a reformed outlaw hired by Western Union boss Dean Jagger to protect the line from marauding Sioux and to also take on McLane and his gang who are trying to destroy the line for their own devious ends . Robert Young is the young engineer from back east who joins the company and vies with Scott for the affections of Miss Gilmore . Some comic relief is provided by - and irritatingly so some would say - by Slim Summerville and John Carradine turns up in a meager role as the company doctor . &lt; br / &gt; &lt; br / &gt; Altogether though a spanking good western , albeit on Region 2 , but in sparkling good quality that fans will be delighted with . My only crib is that there are no extras , not even a trailer and that terrible cover with those dull graphics . UGH ! &lt; br / &gt; &lt; br / &gt; Footnote : Interestingly the associate producer on \" Western Union \" was Harry Joe Brown who later with Randolph Scott would create a partnership that would produce some of Scott's finest westerns in the fifties .      Color Legend: Purple: Low weight in both methods Blue: High weight in both methods Red: High TF-IDF weight, Low BM25 weight Green: Low TF-IDF weight, High BM25 weight      <pre>\n================================================================================\n\nReview #3:\n--------------------------------------------------------------------------------\nText: The sequel is exactly what you will expect it to be. And it is good enough that everyone who would have wanted to watch this should leave it happy.&lt;br...\n\n</pre> Combined Highlighting (Review #3)  The sequel is exactly what you will expect it to be . And it is good enough that everyone who would have wanted to watch this should leave it happy . &lt; br / &gt; &lt; br / &gt; This is not a movie that will win an Academy award . But it does take what made the Jackass TV show and original movie a success , and it turns it up a notch . It is funnier , more brutal , and more disgusting than the original . And I loved every minute of it . &lt; br / &gt; &lt; br / &gt; The original had a few notorious stunts , and there is at least one stunt that this movie will be remembered for . You will wince , cringe , look away , and laugh very , very hard . &lt; br / &gt; &lt; br / &gt; In any event , you probably do not need to read this review , or any others , to know if you will like this movie , unless you have never heard of Jackass .      Color Legend: Purple: Low weight in both methods Blue: High weight in both methods Red: High TF-IDF weight, Low BM25 weight Green: Low TF-IDF weight, High BM25 weight      <pre>\n================================================================================\n\nReview #4:\n--------------------------------------------------------------------------------\nText: ''Wallace &amp; Gromit in The Curse of the Were-Rabbit'' is the same type of animation and from the same creators of ''Chicken run'', but the story now is...\n\n</pre> Combined Highlighting (Review #4)  ' ' Wallace &amp; Gromit in The Curse of the Were - Rabbit ' ' is the same type of animation and from the same creators of ' ' Chicken run ' ' , but the story now is other : Wallace , a inventor who loves cheese and his smart dog Gromit who is always helping Wallace in his problems , are trying to keep the rabbits away from everybody's vegetables , since there is in their town , an annual Giant Vegetable Competition . But when Wallace tries an invention he did , to make the rabbits avoids vegetable , the one who is going to be cursed is him . Before watching this movie I didn't knew that these two characters already existed and were famous . I loved Gromit , and I think he is one of the coolest dogs I already saw . &lt; br / &gt; &lt; br / &gt; aka \" Wallace &amp; Gromit - A Batalha dos Vegetais \" - Brazil      Color Legend: Purple: Low weight in both methods Blue: High weight in both methods Red: High TF-IDF weight, Low BM25 weight Green: Low TF-IDF weight, High BM25 weight      <pre>\n================================================================================\n\nReview #5:\n--------------------------------------------------------------------------------\nText: Hahahahah Probably one of the funniest movies i've even seen. Obviously this isn't intentional though. It takes about half the movie for the main char...\n\n</pre> Combined Highlighting (Review #5)  Hahahahah Probably one of the funniest movies i've even seen . Obviously this isn't intentional though . It takes about half the movie for the main characters to realize what the big hilly thing is in the middle of the city is spewing hot red stuff , and the other half spent diverting the lave flow through the city using fire trucks ( yer right ) . It certainly made me laugh . The acting makes Arnie look like a RSC thespian . It is amazing that films like this get commissioned . A more interesting version would be someone going near an active volcano and filming it , and would probably cost about \u00a3 20 to make . ( $ 40 ) I can see some guy pitching the film to a film company \" well there's this big VOLCANO and it erupts in a CITY . . . . pretty radical hey \" If you can find it in the dollar bins , maybe worth buying as after watching this most other films would look good .      Color Legend: Purple: Low weight in both methods Blue: High weight in both methods Red: High TF-IDF weight, Low BM25 weight Green: Low TF-IDF weight, High BM25 weight      <pre>\n================================================================================\n\n</pre> <p>INteresting, on the first review the tf-idf focuses on \"film\" whereas it's not the case for the BM25. This is interesting and shows the capacities of the BM25 to reduce the weight of the terms that may be overweighted in the TF-IDF.</p>"},{"location":"chapter1/Session_1_4_BM25/#bm25-a-better-tf-idf-for-text-retrieval","title":"BM25: A Better TF-IDF for Text Retrieval\u00b6","text":"<p>In this notebook, we'll explore BM25 (Best Matching 25), a ranking function that extends the traditional TF-IDF model with improvements for document retrieval. We'll compare BM25 with TF-IDF.</p>"},{"location":"chapter1/Session_1_4_BM25/#industrial-use-case-enterprise-knowledge-base-search","title":"Industrial Use Case: Enterprise Knowledge Base Search\u00b6","text":"<p>Many large organizations maintain extensive knowledge bases containing documentation, FAQs, product manuals, and internal policies. Employees need to find relevant information quickly when troubleshooting issues or answering customer queries.</p> <p>Use Case Scenario: We are implementing a search function for a technical support knowledge base where employees can find relevant articles about known issues and solutions. When an employee searches for a specific problem, the system needs to retrieve the most relevant documents, even if they don't use exactly the same terminology.</p> <p>We'll model this using the IMDB dataset, as we did in the previous notebook.</p> <p>One common mistake when it comes to RAG (we'll talk about this in Session 9) is to use complex retriever models. In this case, we'll use a simple vector-based retriever (cosine similarity) to retrieve paragraphs from the same section.</p>"},{"location":"chapter1/Session_1_4_BM25/#our-objectives","title":"Our Objectives:\u00b6","text":"<ol> <li>Implement a BM25Vectorizer that inherits from the same base class as TfidfVectorizer</li> <li>Visualize differences in the sparse representations</li> <li>Look into creating a retrieval pipeline with the BM25Vectorizer</li> <li>Compare the performance of BM25 with TF-IDF on IMDB dataset</li> </ol> <p>Let's begin!</p>"},{"location":"chapter1/Session_1_4_BM25/#setup-and-imports","title":"Setup and Imports\u00b6","text":"<p>First, we'll import the necessary libraries and set a random seed for reproducibility.</p>"},{"location":"chapter1/Session_1_4_BM25/#1-load-and-explore-the-data","title":"1. Load and Explore the Data\u00b6","text":"<p>We'll use the <code>imbd</code> dataset, which contains IMDB reviews with a label indicating if the review is positive or negative.</p>"},{"location":"chapter1/Session_1_4_BM25/#2-implementation-of-bm25vectorizer","title":"2. Implementation of BM25Vectorizer\u00b6","text":"<p>BM25 is an improvement over TF-IDF that addresses some of its limitations:</p> <ol> <li>It handles term frequency saturation better (multiple occurrences of a term give diminishing returns)</li> <li>It accounts for document length normalization in a more principled way</li> </ol> <p>The BM25 formula is:</p> <p>$$\\text{BM25}(D,Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{avgdl})}$$</p> <p>Where:</p> <ul> <li>$f(q_i, D)$ is the term frequency of term $q_i$ in document $D$</li> <li>$|D|$ is the length of document $D$</li> <li>$avgdl$ is the average document length across the corpus</li> <li>$k_1$ and $b$ are free parameters (typically, $k_1 \\in [1.2, 2.0]$ and $b = 0.75$)</li> <li>$\\text{IDF}(q_i)$ is the inverse document frequency of term $q_i$</li> </ul> <p>Let's implement a <code>BM25Vectorizer</code> that can be used as a drop-in replacement for scikit-learn's <code>TfidfVectorizer</code>.</p>"},{"location":"chapter1/Session_1_4_BM25/#3-implement-and-compare-the-retrieval-methods","title":"3. Implement and Compare the Retrieval Methods\u00b6","text":"<p>Now, we'll implement and compare two methods for paragraph retrieval:</p> <ol> <li>TF-IDF with cosine similarity</li> <li>BM25 with cosine similarity</li> </ol>"},{"location":"chapter1/Session_1_4_BM25/#visualize-differences-in-vector-representations","title":"Visualize Differences in Vector Representations\u00b6","text":"<p>Let's look at the differences between TF-IDF and BM25 representations for a few example paragraphs. This will help us understand how BM25 modifies the term weighting.</p>"},{"location":"chapter1/Session_1_4_BM25/#4-evaluation-and-comparison-functions","title":"4. Evaluation and Comparison Functions\u00b6","text":""},{"location":"chapter1/Session_1_4_BM25/#41-retrieval-evaluation","title":"4.1 Retrieval Evaluation\u00b6","text":"<p>Now let's imagine we want to know about some reviews about a movie or a type of movie. We could create a retrieval system to find the most similar reviews to a given query. We could use the cosine similarity to do this. The main idea is to compute the cosine similarity between the vectorized reviews and then use a threshold to classify the reviews as similar or not, we don't train anything here.</p> <p>First let's create a class to retrieve and rank documents based on query similarity.</p>"},{"location":"chapter1/Session_1_4_BM25/#42-classification-of-sentiment","title":"4.2 Classification Of Sentiment\u00b6","text":"<p>Now let's anticipate a bit from session 3 ans let's talk about a way of evaluating the quality of the text representation (we'll call that embedding). One way to do this is to use a classifier to predict the sentiment of a review based on the text representation. This is what we call \"extrinsic evaluation\". Let's see if there is a huge difference between TF-IDF and BM25 representations.</p> <p>We'll apply different classifiers to see if there is a huge difference between the two representations.</p>"},{"location":"chapter10/","title":"Agents","text":""},{"location":"chapter10/#session-10-llm-powered-agents-and-advanced-applications","title":"Session 10: LLM-powered Agents and Advanced Applications","text":"<p>This final session explores the frontier of NLP research and applications, focusing on LLM-powered agents, hallucination detection, and emerging capabilities of language models.</p>"},{"location":"chapter10/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the concept of LLM-powered agents and their applications</li> <li>Learn techniques for detecting and mitigating hallucinations in LLMs</li> <li>Explore the limitations of current language models</li> <li>Gain insights into the future directions of NLP research</li> <li>Develop a critical perspective on the capabilities and limitations of language technologies</li> </ul>"},{"location":"chapter10/#topics-covered","title":"Topics Covered","text":""},{"location":"chapter10/#hallucinations-limitations","title":"Hallucinations &amp; Limitations","text":"<ul> <li>Understanding hallucinations in language models</li> <li>Methods for detecting factual inconsistencies</li> <li>Self-consistency and ensemble approaches</li> <li>Retrieval-augmented generation for factuality</li> <li>Other limitations of current LLMs</li> <li>Evaluation frameworks for model reliability</li> <li>Strategies for mitigating hallucinations in applications</li> </ul>"},{"location":"chapter10/#introduction-to-agentic-frameworks","title":"Introduction to Agentic Frameworks","text":"<ul> <li>Conceptual foundations of LLM-powered agents</li> <li>ReAct: Synergizing reasoning and acting in Language Models</li> <li>Tool use and function calling capabilities</li> <li>Planning and decomposition of complex tasks</li> <li>Multi-agent systems and collaborative problem-solving</li> <li>Memory and context management in long-running agents</li> <li>Evaluation and benchmarking of agent capabilities</li> </ul>"},{"location":"chapter10/#emerging-research-directions","title":"Emerging Research Directions","text":"<ul> <li>World models and their role in language understanding</li> <li>Smaller, more efficient models vs. scaling trends</li> <li>Multimodal and embodied language understanding</li> <li>Alignment and safety considerations</li> <li>Interpretability and explainability research</li> <li>Computational efficiency and environmental impact</li> </ul>"},{"location":"chapter10/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>Yao et al. (2023) \"ReAct: Synergizing reasoning and acting in Language Models\"</li> <li>Manakul et al. (2023) \"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models\"</li> <li>Weng (2024) \"Extrinsic Hallucinations in LLMs\"</li> <li>Mitchell (2025) \"LLMs and World Models\"</li> <li>Vafa et al. (2024) \"Evaluating the World Model Implicit in a Generative Model\"</li> <li>Feng et al. (2024) \"Were RNNs All We Needed?\"</li> <li>Huyen (2025) \"AI Engineering\"</li> <li>Warner et al. (2024) \"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Fine Tuning and Inference\"</li> </ul>"},{"location":"chapter10/#practical-components","title":"Practical Components","text":"<ul> <li>Building a simple LLM-powered agent with tool-use capabilities</li> <li>Implementing hallucination detection mechanisms</li> <li>Designing evaluation frameworks for agent performance</li> <li>Case studies of successful agent applications</li> <li>Group discussion on the future of NLP and language agents</li> <li>Final project presentations and feedback</li> </ul>"},{"location":"chapter2/","title":"Session 2: Deep Learning for NLP","text":""},{"location":"chapter2/#course-materials","title":"\ud83c\udf93 Course Materials","text":""},{"location":"chapter2/#slides","title":"\ud83d\udcd1 Slides","text":"<p>Download Session 2 Slides (PDF)</p>"},{"location":"chapter2/#notebooks","title":"\ud83d\udcd3 Notebooks","text":"<ul> <li>Intro to Neural Nets &amp; Backprop (NumPy)</li> <li>Simple RNN for Text Generation (Tiny Shakespeare)</li> <li>LSTM for Sequence Classification</li> </ul>"},{"location":"chapter2/#session-2-neural-networks-backpropagation-rnns","title":"\ud83d\ude80 Session 2: Neural Networks, Backpropagation &amp; RNNs","text":"<p>In this second session, we move beyond the baselines of Session 1 and dive into the world of neural networks. From the foundational vanilla feedforward architecture to more advanced recurrent neural networks, you\u2019ll see how we capture sequential patterns crucial for language understanding. We\u2019ll also explore the intricacies of training these models, including gradient descent variants and backpropagation, as well as potential pitfalls like vanishing or exploding gradients.</p>"},{"location":"chapter2/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ol> <li>Understand the core mechanics of neural networks, from feedforward passes to computing gradients.</li> <li>Master backpropagation to see how weight updates flow through each layer.</li> <li>Explore Recurrent Neural Networks (RNNs) and see why they\u2019re pivotal for handling sequential data such as text.</li> <li>Learn about Long Short-Term Memory (LSTM) networks and how they solve the shortcomings of vanilla RNNs.</li> <li>Build a text generator that can produce plausible sequences, using an RNN trained on a small dataset (Tiny Shakespeare).</li> </ol>"},{"location":"chapter2/#topics-covered","title":"\ud83d\udcda Topics Covered","text":""},{"location":"chapter2/#neural-network-essentials","title":"Neural Network Essentials","text":"<ul> <li>Vanilla Networks: Single-layer networks, the chain rule in practice, and how we compute partial derivatives for each parameter.</li> <li>Gradient Descent: A closer look at batch, mini-batch, and stochastic variants. We\u2019ll discuss how they\u2019re used in frameworks like PyTorch or TensorFlow.</li> </ul>"},{"location":"chapter2/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNNs)","text":"<ul> <li>Sequential Data: Why standard NNs fail to capture dependencies in text, time-series, or speech.</li> <li>Vanishing/Exploding Gradients: Common training challenges in RNNs and strategies to mitigate them.</li> <li>Practical RNN Implementations: Adopting RNN variants, including LSTM and GRU, for tasks like language modeling and sequence labeling.</li> </ul>"},{"location":"chapter2/#bibliography-recommended-reading","title":"\ud83d\udcd6 Bibliography &amp; Recommended Reading","text":"<ul> <li> <p>Karpahty A. (2016) \"Yes You Should Understand Backprop\" - Blog Post   A blog post explaining backpropagation in detail.</p> </li> <li> <p>Silviu P. (2016) \"Written Memories: Understanding, Deriving and Extending the LSTM\" - Blog Post   A blog post explaining RNN and LSTM logic in detail.</p> </li> <li> <p>Colah, C. (2015) \"Understanding LSTMs\" - Blog Post   A blog post explaining LSTMs in detail.</p> </li> <li> <p>Karpathy, A. (2015) \"The Unreasonable Effectiveness of Recurrent Neural Networks\" - Blog Post   Classic blog post illustrating RNN text generation (tiny Shakespeare).</p> </li> <li> <p>Colah, C. (2016) \"Attention and Augmented Recurrent Neural Networks\" - Blog Post   A blog post explaining augmented RNNs in detail.</p> </li> <li> <p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). \"Learning internal representations by error propagation.\" Paper   Presents the backpropagation algorithm in detail.</p> </li> <li> <p>Hochreiter, S., &amp; Schmidhuber, J. (1997). \"Long short-term memory.\" Paper   Neural Computation, 9(8).   Original LSTM paper addressing the vanishing gradient problem in RNNs.</p> </li> <li> <p>Cho et al. (2014). \"Learning phrase representations using RNN encoder-decoder for statistical machine translation.\" Paper   Introduced the GRU (Gated Recurrent Unit) as a simpler alternative to LSTM.</p> </li> <li> <p>He et al. (2015). \"Deep Residual Learning for Image Recognition.\" Paper   Not directly NLP, but the notion of \u201cdegradation problem\u201d is generalizable to deep networks.</p> </li> </ul>"},{"location":"chapter2/#practical-components","title":"\ud83d\udcbb Practical Components","text":"<ul> <li>Implementing Gradient Descent: We\u2019ll code a simple neural net from scratch (via NumPy or PyTorch) to see how forward/backward passes work.</li> <li>Vanishing &amp; Exploding Gradients: In a toy RNN, we\u2019ll visualize how gradients can shrink or explode, and learn about gradient clipping.</li> <li>Recurrent Language Model: Train an RNN (or LSTM) on a small text corpus (e.g., Tiny Shakespeare) and watch it generate new text sequences.</li> </ul>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/","title":"Building Neural Networks from Scratch with NumPy","text":"In\u00a0[1]: Copied! <pre># Imports (No Keras/TensorFlow)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# For reproducibility:\nnp.random.seed(42)\n</pre> # Imports (No Keras/TensorFlow) import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from datasets import load_dataset from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # For reproducibility: np.random.seed(42) In\u00a0[2]: Copied! <pre>def sigmoid(x):\n    # clip to avoid overflow in large exponent\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return np.where(x &gt; 0, 1, 0)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    return 1 - np.tanh(x)**2\n\ndef softmax(x):\n    # numeric stability shift\n    shifted = x - np.max(x, axis=1, keepdims=True)\n    ex = np.exp(shifted)\n    return ex / np.sum(ex, axis=1, keepdims=True)\n\ndef categorical_cross_entropy(y_true, y_pred):\n    # clip predictions to avoid log(0)\n    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n</pre> def sigmoid(x):     # clip to avoid overflow in large exponent     return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  def sigmoid_derivative(x):     s = sigmoid(x)     return s * (1 - s)  def relu(x):     return np.maximum(0, x)  def relu_derivative(x):     return np.where(x &gt; 0, 1, 0)  def tanh(x):     return np.tanh(x)  def tanh_derivative(x):     return 1 - np.tanh(x)**2  def softmax(x):     # numeric stability shift     shifted = x - np.max(x, axis=1, keepdims=True)     ex = np.exp(shifted)     return ex / np.sum(ex, axis=1, keepdims=True)  def categorical_cross_entropy(y_true, y_pred):     # clip predictions to avoid log(0)     y_pred = np.clip(y_pred, 1e-15, 1-1e-15)     return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]  In\u00a0[3]: Copied! <pre>class NeuralNetwork:\n    def __init__(self, layer_sizes, activations):\n        \"\"\"\n        layer_sizes: list (e.g. [input_dim, hidden_dim, ..., output_dim])\n        activations: list of string names, e.g. ['relu','softmax'] (one per hidden/output layer)\n        \"\"\"\n        self.layer_sizes = layer_sizes\n        self.num_layers = len(layer_sizes)\n\n        # map from string to actual function\n        self.activation_map = {\n            'sigmoid': (sigmoid, sigmoid_derivative),\n            'relu': (relu, relu_derivative),\n            'softmax': (softmax, None),\n            'tanh': (tanh, tanh_derivative)\n        }\n\n        self.activations = []\n        self.activation_derivs = []\n        for act in activations:\n            f, d = self.activation_map[act]\n            self.activations.append(f)\n            self.activation_derivs.append(d)\n\n        self.initialize_parameters()\n        # placeholders for forward pass data\n        self.Z = [None]*(self.num_layers-1)\n        self.A = [None]*self.num_layers\n\n    def initialize_parameters(self):\n        # He initialization\n        self.weights = []\n        self.biases = []\n        for i in range(1, self.num_layers):\n            scale = np.sqrt(2.0 / self.layer_sizes[i-1])\n            W = np.random.randn(self.layer_sizes[i-1], self.layer_sizes[i]) * scale\n            b = np.zeros((1, self.layer_sizes[i]))\n            self.weights.append(W)\n            self.biases.append(b)\n\n    def forward_propagation(self, X):\n        self.A[0] = X\n        for i in range(self.num_layers-1):\n            self.Z[i] = np.dot(self.A[i], self.weights[i]) + self.biases[i]\n            self.A[i+1] = self.activations[i](self.Z[i])\n        return self.A[-1]\n\n    def compute_loss(self, y_true, y_pred):\n        return categorical_cross_entropy(y_true, y_pred)\n\n    def backward_propagation(self, y_true):\n        m = y_true.shape[0]\n        dW = [None]*(self.num_layers-1)\n        db = [None]*(self.num_layers-1)\n\n        # handle last layer\n        if self.activations[-1] == softmax:\n            dZ = self.A[-1] - y_true  # (y_pred - y_true)\n        else:\n            # e.g. sigmoid + cross-entropy\n            dA = -(y_true / self.A[-1] - (1 - y_true)/(1 - self.A[-1]))\n            dZ = dA * self.activation_derivs[-1](self.Z[-1])\n\n        dW[-1] = np.dot(self.A[-2].T, dZ)/m\n        db[-1] = np.sum(dZ, axis=0, keepdims=True)/m\n\n        for l in range(self.num_layers-3, -1, -1):\n            dA = np.dot(dZ, self.weights[l+1].T)\n            dZ = dA * self.activation_derivs[l](self.Z[l])\n            dW[l] = np.dot(self.A[l].T, dZ)/m\n            db[l] = np.sum(dZ, axis=0, keepdims=True)/m\n\n        return {'dW': dW, 'db': db}\n\n    def update_parameters(self, grads, lr):\n        dW = grads['dW']\n        db = grads['db']\n        for i in range(self.num_layers-1):\n            self.weights[i] -= lr * dW[i]\n            self.biases[i] -= lr * db[i]\n\n    def train_step(self, X_batch, y_batch, lr):\n        y_pred = self.forward_propagation(X_batch)\n        loss = self.compute_loss(y_batch, y_pred)\n        grads = self.backward_propagation(y_batch)\n        self.update_parameters(grads, lr)\n        return loss\n\n    def train(self, X_train, y_train, X_val=None, y_val=None, batch_size=32, learning_rate=0.01,\n              epochs=10, verbose=True, record_every=1):\n        m = X_train.shape[0]\n        history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n\n        for epoch in range(epochs):\n            perm = np.random.permutation(m)\n            X_shuf = X_train[perm]\n            y_shuf = y_train[perm]\n\n            epoch_loss = 0\n            num_batches = int(np.ceil(m/batch_size))\n            for i in range(num_batches):\n                start = i*batch_size\n                end = min((i+1)*batch_size, m)\n                X_batch = X_shuf[start:end]\n                y_batch = y_shuf[start:end]\n                b_loss = self.train_step(X_batch, y_batch, learning_rate)\n                epoch_loss += b_loss * (end - start)\n            epoch_loss /= m\n\n            # track train/val if needed\n            if epoch % record_every == 0 or epoch == epochs-1:\n                y_pred_train = self.predict(X_train)\n                train_acc = np.mean(np.argmax(y_pred_train, axis=1) == np.argmax(y_train, axis=1))\n                history['train_loss'].append(epoch_loss)\n                history['train_acc'].append(train_acc)\n\n                if X_val is not None and y_val is not None:\n                    y_pred_val = self.predict(X_val)\n                    val_loss = self.compute_loss(y_val, y_pred_val)\n                    val_acc  = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_val, axis=1))\n                    history['val_loss'].append(val_loss)\n                    history['val_acc'].append(val_acc)\n                    if verbose:\n                        print(f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {train_acc:.4f} - val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f}\")\n                else:\n                    if verbose:\n                        print(f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {train_acc:.4f}\")\n        return history\n\n    def predict(self, X):\n        return self.forward_propagation(X)\n</pre> class NeuralNetwork:     def __init__(self, layer_sizes, activations):         \"\"\"         layer_sizes: list (e.g. [input_dim, hidden_dim, ..., output_dim])         activations: list of string names, e.g. ['relu','softmax'] (one per hidden/output layer)         \"\"\"         self.layer_sizes = layer_sizes         self.num_layers = len(layer_sizes)          # map from string to actual function         self.activation_map = {             'sigmoid': (sigmoid, sigmoid_derivative),             'relu': (relu, relu_derivative),             'softmax': (softmax, None),             'tanh': (tanh, tanh_derivative)         }          self.activations = []         self.activation_derivs = []         for act in activations:             f, d = self.activation_map[act]             self.activations.append(f)             self.activation_derivs.append(d)          self.initialize_parameters()         # placeholders for forward pass data         self.Z = [None]*(self.num_layers-1)         self.A = [None]*self.num_layers      def initialize_parameters(self):         # He initialization         self.weights = []         self.biases = []         for i in range(1, self.num_layers):             scale = np.sqrt(2.0 / self.layer_sizes[i-1])             W = np.random.randn(self.layer_sizes[i-1], self.layer_sizes[i]) * scale             b = np.zeros((1, self.layer_sizes[i]))             self.weights.append(W)             self.biases.append(b)      def forward_propagation(self, X):         self.A[0] = X         for i in range(self.num_layers-1):             self.Z[i] = np.dot(self.A[i], self.weights[i]) + self.biases[i]             self.A[i+1] = self.activations[i](self.Z[i])         return self.A[-1]      def compute_loss(self, y_true, y_pred):         return categorical_cross_entropy(y_true, y_pred)      def backward_propagation(self, y_true):         m = y_true.shape[0]         dW = [None]*(self.num_layers-1)         db = [None]*(self.num_layers-1)          # handle last layer         if self.activations[-1] == softmax:             dZ = self.A[-1] - y_true  # (y_pred - y_true)         else:             # e.g. sigmoid + cross-entropy             dA = -(y_true / self.A[-1] - (1 - y_true)/(1 - self.A[-1]))             dZ = dA * self.activation_derivs[-1](self.Z[-1])          dW[-1] = np.dot(self.A[-2].T, dZ)/m         db[-1] = np.sum(dZ, axis=0, keepdims=True)/m          for l in range(self.num_layers-3, -1, -1):             dA = np.dot(dZ, self.weights[l+1].T)             dZ = dA * self.activation_derivs[l](self.Z[l])             dW[l] = np.dot(self.A[l].T, dZ)/m             db[l] = np.sum(dZ, axis=0, keepdims=True)/m          return {'dW': dW, 'db': db}      def update_parameters(self, grads, lr):         dW = grads['dW']         db = grads['db']         for i in range(self.num_layers-1):             self.weights[i] -= lr * dW[i]             self.biases[i] -= lr * db[i]      def train_step(self, X_batch, y_batch, lr):         y_pred = self.forward_propagation(X_batch)         loss = self.compute_loss(y_batch, y_pred)         grads = self.backward_propagation(y_batch)         self.update_parameters(grads, lr)         return loss      def train(self, X_train, y_train, X_val=None, y_val=None, batch_size=32, learning_rate=0.01,               epochs=10, verbose=True, record_every=1):         m = X_train.shape[0]         history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}          for epoch in range(epochs):             perm = np.random.permutation(m)             X_shuf = X_train[perm]             y_shuf = y_train[perm]              epoch_loss = 0             num_batches = int(np.ceil(m/batch_size))             for i in range(num_batches):                 start = i*batch_size                 end = min((i+1)*batch_size, m)                 X_batch = X_shuf[start:end]                 y_batch = y_shuf[start:end]                 b_loss = self.train_step(X_batch, y_batch, learning_rate)                 epoch_loss += b_loss * (end - start)             epoch_loss /= m              # track train/val if needed             if epoch % record_every == 0 or epoch == epochs-1:                 y_pred_train = self.predict(X_train)                 train_acc = np.mean(np.argmax(y_pred_train, axis=1) == np.argmax(y_train, axis=1))                 history['train_loss'].append(epoch_loss)                 history['train_acc'].append(train_acc)                  if X_val is not None and y_val is not None:                     y_pred_val = self.predict(X_val)                     val_loss = self.compute_loss(y_val, y_pred_val)                     val_acc  = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_val, axis=1))                     history['val_loss'].append(val_loss)                     history['val_acc'].append(val_acc)                     if verbose:                         print(f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {train_acc:.4f} - val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f}\")                 else:                     if verbose:                         print(f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {train_acc:.4f}\")         return history      def predict(self, X):         return self.forward_propagation(X)  In\u00a0[4]: Copied! <pre># XOR dataset: 2 inputs -&gt; 2 classes, one-hot.\nX_xor = np.array([[0,0],[0,1],[1,0],[1,1]])\ny_xor = np.array([[1,0],[0,1],[0,1],[1,0]])\n\n# define net\nxor_net = NeuralNetwork(\n    layer_sizes=[2,4,2],\n    activations=['sigmoid','sigmoid']\n)\n\nhist_xor = xor_net.train(\n    X_xor,\n    y_xor,\n    batch_size=4,  # full batch\n    learning_rate=0.5,\n    epochs=1000,\n    verbose=False,\n    record_every=100\n)\n\n# Evaluate\ny_out = xor_net.predict(X_xor)\npred_cls = np.argmax(y_out, axis=1)\ntrue_cls = np.argmax(y_xor, axis=1)\nacc = np.mean(pred_cls==true_cls)\nprint(\"XOR Accuracy:\", acc)\n\nplt.plot(hist_xor['train_loss'], label='Train Loss')\nplt.title('XOR Training Loss')\nplt.xlabel('Checkpoints')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n</pre> # XOR dataset: 2 inputs -&gt; 2 classes, one-hot. X_xor = np.array([[0,0],[0,1],[1,0],[1,1]]) y_xor = np.array([[1,0],[0,1],[0,1],[1,0]])  # define net xor_net = NeuralNetwork(     layer_sizes=[2,4,2],     activations=['sigmoid','sigmoid'] )  hist_xor = xor_net.train(     X_xor,     y_xor,     batch_size=4,  # full batch     learning_rate=0.5,     epochs=1000,     verbose=False,     record_every=100 )  # Evaluate y_out = xor_net.predict(X_xor) pred_cls = np.argmax(y_out, axis=1) true_cls = np.argmax(y_xor, axis=1) acc = np.mean(pred_cls==true_cls) print(\"XOR Accuracy:\", acc)  plt.plot(hist_xor['train_loss'], label='Train Loss') plt.title('XOR Training Loss') plt.xlabel('Checkpoints') plt.ylabel('Loss') plt.legend() plt.show() <pre>XOR Accuracy: 1.0\n</pre> In\u00a0[5]: Copied! <pre>spam_data = load_dataset(\"NotShrirang/email-spam-filter\")\ndf = spam_data['train'].to_pandas()\ndf.head()\n</pre> spam_data = load_dataset(\"NotShrirang/email-spam-filter\") df = spam_data['train'].to_pandas() df.head() Out[5]: Unnamed: 0 label text label_num 0 605 ham Subject: enron methanol ; meter # : 988291\\nth... 0 1 2349 ham Subject: hpl nom for january 9 , 2001\\n( see a... 0 2 3624 ham Subject: neon retreat\\nho ho ho , we ' re arou... 0 3 4685 spam Subject: photoshop , windows , office . cheap ... 1 4 2030 ham Subject: re : indian springs\\nthis deal is to ... 0 In\u00a0[6]: Copied! <pre>train_df, test_df = train_test_split(\n    df, test_size=0.2, stratify=df['label_num'], random_state=42)\ntrain_df, dev_df = train_test_split(\n    train_df, test_size=0.2, stratify=train_df['label_num'], random_state=42)\n\nprint(len(train_df), len(dev_df), len(test_df))\n\nvectorizer = TfidfVectorizer(stop_words='english', max_features=2000)\nX_train_tfidf = vectorizer.fit_transform(train_df['text']).toarray()\ny_train_nums = train_df['label_num'].values\n\nX_dev_tfidf = vectorizer.transform(dev_df['text']).toarray()\ny_dev_nums = dev_df['label_num'].values\n\nX_test_tfidf = vectorizer.transform(test_df['text']).toarray()\ny_test_nums = test_df['label_num'].values\n\ndef to_one_hot(lbls, num_classes=2):\n    return np.eye(num_classes)[lbls]\n\ny_train_oh = to_one_hot(y_train_nums, 2)\ny_dev_oh   = to_one_hot(y_dev_nums, 2)\ny_test_oh  = to_one_hot(y_test_nums, 2)\n</pre> train_df, test_df = train_test_split(     df, test_size=0.2, stratify=df['label_num'], random_state=42) train_df, dev_df = train_test_split(     train_df, test_size=0.2, stratify=train_df['label_num'], random_state=42)  print(len(train_df), len(dev_df), len(test_df))  vectorizer = TfidfVectorizer(stop_words='english', max_features=2000) X_train_tfidf = vectorizer.fit_transform(train_df['text']).toarray() y_train_nums = train_df['label_num'].values  X_dev_tfidf = vectorizer.transform(dev_df['text']).toarray() y_dev_nums = dev_df['label_num'].values  X_test_tfidf = vectorizer.transform(test_df['text']).toarray() y_test_nums = test_df['label_num'].values  def to_one_hot(lbls, num_classes=2):     return np.eye(num_classes)[lbls]  y_train_oh = to_one_hot(y_train_nums, 2) y_dev_oh   = to_one_hot(y_dev_nums, 2) y_test_oh  = to_one_hot(y_test_nums, 2)  <pre>3308 828 1035\n</pre> In\u00a0[7]: Copied! <pre>spam_net_np = NeuralNetwork(\n    layer_sizes=[2000,64,2],\n    activations=['relu','softmax']\n)\n\nhist_spam_np = spam_net_np.train(\n    X_train_tfidf, y_train_oh,\n    X_dev_tfidf,   y_dev_oh,\n    batch_size=64,\n    learning_rate=0.2,\n    epochs=50,\n    verbose=True,\n    record_every=1\n)\n\ny_test_pred_proba_np = spam_net_np.predict(X_test_tfidf)\ny_test_pred_class_np = np.argmax(y_test_pred_proba_np, axis=1)\ntest_acc_np = np.mean(y_test_pred_class_np == y_test_nums)\nprint(f\"\\nNumPy Network Test Accuracy: {test_acc_np:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_nums, y_test_pred_class_np))\n\ncm = confusion_matrix(y_test_nums, y_test_pred_class_np)\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\nplt.title('Spam Detection (NumPy NN) - Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n</pre> spam_net_np = NeuralNetwork(     layer_sizes=[2000,64,2],     activations=['relu','softmax'] )  hist_spam_np = spam_net_np.train(     X_train_tfidf, y_train_oh,     X_dev_tfidf,   y_dev_oh,     batch_size=64,     learning_rate=0.2,     epochs=50,     verbose=True,     record_every=1 )  y_test_pred_proba_np = spam_net_np.predict(X_test_tfidf) y_test_pred_class_np = np.argmax(y_test_pred_proba_np, axis=1) test_acc_np = np.mean(y_test_pred_class_np == y_test_nums) print(f\"\\nNumPy Network Test Accuracy: {test_acc_np:.4f}\")  print(\"\\nClassification Report:\") print(classification_report(y_test_nums, y_test_pred_class_np))  cm = confusion_matrix(y_test_nums, y_test_pred_class_np) sns.heatmap(cm, annot=True, cmap='Blues', fmt='d') plt.title('Spam Detection (NumPy NN) - Confusion Matrix') plt.xlabel('Predicted') plt.ylabel('True') plt.show() <pre>Epoch 1/50 - loss: 0.5815 - acc: 0.7101 - val_loss: 0.5329 - val_acc: 0.7101\nEpoch 2/50 - loss: 0.4722 - acc: 0.7443 - val_loss: 0.4096 - val_acc: 0.7403\nEpoch 3/50 - loss: 0.3412 - acc: 0.9531 - val_loss: 0.2936 - val_acc: 0.9300\nEpoch 4/50 - loss: 0.2433 - acc: 0.9628 - val_loss: 0.2230 - val_acc: 0.9372\nEpoch 5/50 - loss: 0.1849 - acc: 0.9680 - val_loss: 0.1820 - val_acc: 0.9589\nEpoch 6/50 - loss: 0.1487 - acc: 0.9743 - val_loss: 0.1527 - val_acc: 0.9614\nEpoch 7/50 - loss: 0.1242 - acc: 0.9831 - val_loss: 0.1363 - val_acc: 0.9626\nEpoch 8/50 - loss: 0.1056 - acc: 0.9840 - val_loss: 0.1183 - val_acc: 0.9686\nEpoch 9/50 - loss: 0.0925 - acc: 0.9876 - val_loss: 0.1064 - val_acc: 0.9698\nEpoch 10/50 - loss: 0.0814 - acc: 0.9897 - val_loss: 0.0991 - val_acc: 0.9722\nEpoch 11/50 - loss: 0.0720 - acc: 0.9909 - val_loss: 0.0947 - val_acc: 0.9734\nEpoch 12/50 - loss: 0.0658 - acc: 0.9915 - val_loss: 0.0857 - val_acc: 0.9771\nEpoch 13/50 - loss: 0.0595 - acc: 0.9921 - val_loss: 0.0867 - val_acc: 0.9722\nEpoch 14/50 - loss: 0.0544 - acc: 0.9933 - val_loss: 0.0776 - val_acc: 0.9783\nEpoch 15/50 - loss: 0.0510 - acc: 0.9946 - val_loss: 0.0781 - val_acc: 0.9734\nEpoch 16/50 - loss: 0.0476 - acc: 0.9955 - val_loss: 0.0736 - val_acc: 0.9758\nEpoch 17/50 - loss: 0.0443 - acc: 0.9961 - val_loss: 0.0701 - val_acc: 0.9771\nEpoch 18/50 - loss: 0.0412 - acc: 0.9967 - val_loss: 0.0681 - val_acc: 0.9771\nEpoch 19/50 - loss: 0.0384 - acc: 0.9958 - val_loss: 0.0695 - val_acc: 0.9771\nEpoch 20/50 - loss: 0.0364 - acc: 0.9964 - val_loss: 0.0657 - val_acc: 0.9795\nEpoch 21/50 - loss: 0.0340 - acc: 0.9967 - val_loss: 0.0655 - val_acc: 0.9783\nEpoch 22/50 - loss: 0.0321 - acc: 0.9961 - val_loss: 0.0688 - val_acc: 0.9771\nEpoch 23/50 - loss: 0.0310 - acc: 0.9973 - val_loss: 0.0626 - val_acc: 0.9771\nEpoch 24/50 - loss: 0.0290 - acc: 0.9976 - val_loss: 0.0644 - val_acc: 0.9795\nEpoch 25/50 - loss: 0.0275 - acc: 0.9973 - val_loss: 0.0616 - val_acc: 0.9771\nEpoch 26/50 - loss: 0.0264 - acc: 0.9976 - val_loss: 0.0609 - val_acc: 0.9783\nEpoch 27/50 - loss: 0.0250 - acc: 0.9973 - val_loss: 0.0605 - val_acc: 0.9807\nEpoch 28/50 - loss: 0.0238 - acc: 0.9979 - val_loss: 0.0614 - val_acc: 0.9795\nEpoch 29/50 - loss: 0.0229 - acc: 0.9976 - val_loss: 0.0597 - val_acc: 0.9795\nEpoch 30/50 - loss: 0.0220 - acc: 0.9979 - val_loss: 0.0599 - val_acc: 0.9807\nEpoch 31/50 - loss: 0.0209 - acc: 0.9988 - val_loss: 0.0622 - val_acc: 0.9783\nEpoch 32/50 - loss: 0.0200 - acc: 0.9985 - val_loss: 0.0598 - val_acc: 0.9807\nEpoch 33/50 - loss: 0.0194 - acc: 0.9985 - val_loss: 0.0593 - val_acc: 0.9807\nEpoch 34/50 - loss: 0.0187 - acc: 0.9991 - val_loss: 0.0603 - val_acc: 0.9807\nEpoch 35/50 - loss: 0.0177 - acc: 0.9985 - val_loss: 0.0588 - val_acc: 0.9783\nEpoch 36/50 - loss: 0.0171 - acc: 0.9985 - val_loss: 0.0586 - val_acc: 0.9795\nEpoch 37/50 - loss: 0.0167 - acc: 0.9991 - val_loss: 0.0592 - val_acc: 0.9807\nEpoch 38/50 - loss: 0.0160 - acc: 0.9991 - val_loss: 0.0604 - val_acc: 0.9807\nEpoch 39/50 - loss: 0.0154 - acc: 0.9994 - val_loss: 0.0633 - val_acc: 0.9795\nEpoch 40/50 - loss: 0.0150 - acc: 0.9991 - val_loss: 0.0587 - val_acc: 0.9795\nEpoch 41/50 - loss: 0.0146 - acc: 0.9991 - val_loss: 0.0586 - val_acc: 0.9795\nEpoch 42/50 - loss: 0.0141 - acc: 0.9991 - val_loss: 0.0587 - val_acc: 0.9795\nEpoch 43/50 - loss: 0.0136 - acc: 0.9991 - val_loss: 0.0588 - val_acc: 0.9783\nEpoch 44/50 - loss: 0.0132 - acc: 0.9991 - val_loss: 0.0587 - val_acc: 0.9795\nEpoch 45/50 - loss: 0.0129 - acc: 0.9994 - val_loss: 0.0590 - val_acc: 0.9807\nEpoch 46/50 - loss: 0.0126 - acc: 0.9994 - val_loss: 0.0604 - val_acc: 0.9819\nEpoch 47/50 - loss: 0.0121 - acc: 0.9994 - val_loss: 0.0619 - val_acc: 0.9807\nEpoch 48/50 - loss: 0.0118 - acc: 0.9994 - val_loss: 0.0635 - val_acc: 0.9807\nEpoch 49/50 - loss: 0.0116 - acc: 0.9994 - val_loss: 0.0591 - val_acc: 0.9807\nEpoch 50/50 - loss: 0.0114 - acc: 0.9994 - val_loss: 0.0600 - val_acc: 0.9819\n\nNumPy Network Test Accuracy: 0.9855\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       735\n           1       0.97      0.98      0.98       300\n\n    accuracy                           0.99      1035\n   macro avg       0.98      0.98      0.98      1035\nweighted avg       0.99      0.99      0.99      1035\n\n</pre> <p>Quite impressive ! 98.5% accuracy !! Which is much more than the 50% of the baseline and what we created in previous session with baseline.</p> In\u00a0[8]: Copied! <pre>plt.figure(figsize=(8,4))\nplt.plot(hist_spam_np['train_loss'], label='Train Loss')\nif len(hist_spam_np['val_loss'])&gt;0:\n    plt.plot(hist_spam_np['val_loss'], label='Dev Loss')\nplt.title('Spam (NumPy) Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(8,4)) plt.plot(hist_spam_np['train_loss'], label='Train Loss') if len(hist_spam_np['val_loss'])&gt;0:     plt.plot(hist_spam_np['val_loss'], label='Dev Loss') plt.title('Spam (NumPy) Training Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() plt.show() <p>Nevertheless, we see the dev loss starting to increase after 20 epochs. Which means we most likely overfit the training data past that point. To avoid this, we could have used early stopping or regularization.</p> In\u00a0[9]: Copied! <pre>batch_sizes = [32, 128, 512]\nlrs = [0.001, 0.01, 0.1]\n\nresults = {}\n\nfor bs in batch_sizes:\n    for lr in lrs:\n        key = f\"BS={bs}-LR={lr}\"\n        net_temp = NeuralNetwork(\n            layer_sizes=[2000,32,2],\n            activations=['relu','softmax']\n        )\n        hist_temp = net_temp.train(\n            X_train_tfidf, y_train_oh,\n            X_dev_tfidf,   y_dev_oh,\n            batch_size=bs,\n            learning_rate=lr,\n            epochs=50,\n            verbose=False,\n            record_every=1\n        )\n        results[key] = hist_temp\n\n# Plot the training loss curves\nplt.figure(figsize=(15,8))\nfor key, hist_ in results.items():\n    plt.plot(hist_['train_loss'], label=key)\nplt.title('Spam (NumPy) with Different Batch Sizes &amp; LRs')\nplt.xlabel('Epoch')\nplt.ylabel('Train Loss')\nplt.legend()\nplt.show()\n</pre> batch_sizes = [32, 128, 512] lrs = [0.001, 0.01, 0.1]  results = {}  for bs in batch_sizes:     for lr in lrs:         key = f\"BS={bs}-LR={lr}\"         net_temp = NeuralNetwork(             layer_sizes=[2000,32,2],             activations=['relu','softmax']         )         hist_temp = net_temp.train(             X_train_tfidf, y_train_oh,             X_dev_tfidf,   y_dev_oh,             batch_size=bs,             learning_rate=lr,             epochs=50,             verbose=False,             record_every=1         )         results[key] = hist_temp  # Plot the training loss curves plt.figure(figsize=(15,8)) for key, hist_ in results.items():     plt.plot(hist_['train_loss'], label=key) plt.title('Spam (NumPy) with Different Batch Sizes &amp; LRs') plt.xlabel('Epoch') plt.ylabel('Train Loss') plt.legend() plt.show() <p>You can see the differences in loss curves for different batch sizes and learning rates. Generally with really low learning rates the curves are quite flat and the loss is high. And at equal learning rates, the lower batch sizes converge faster.</p> In\u00a0[10]: Copied! <pre>def gradient_norm(nn, X_batch, y_batch):\n    _ = nn.forward_propagation(X_batch)\n    grads = nn.backward_propagation(y_batch)\n    total_sq = 0\n    for dw, db in zip(grads['dW'], grads['db']):\n        total_sq += np.sum(dw**2) + np.sum(db**2)\n    return np.sqrt(total_sq)\n\ndef train_with_grad_tracking(nn, X, y, epochs=20, batch_size=32, lr=0.01):\n    m = X.shape[0]\n    grad_norms = []\n    losses = []\n\n    for e in range(epochs):\n        perm = np.random.permutation(m)\n        X_shuf = X[perm]\n        y_shuf = y[perm]\n        epoch_loss = 0\n        num_batches = int(np.ceil(m/batch_size))\n\n        for i in range(num_batches):\n            start = i*batch_size\n            end = min((i+1)*batch_size, m)\n            X_batch = X_shuf[start:end]\n            y_batch = y_shuf[start:end]\n\n            # measure gradient norm before update (arbitrary choice)\n            gn = gradient_norm(nn, X_batch, y_batch)\n            grad_norms.append(gn)\n\n            b_loss = nn.train_step(X_batch, y_batch, lr)\n            epoch_loss += b_loss*(end-start)\n\n        epoch_loss /= m\n        losses.append(epoch_loss)\n\n    return grad_norms, losses\n\n# We'll create artificial data for demonstration\n# E.g. 500 samples, dimension=20, 5-output classification\nN = 500\nD = 20\nC = 5\n\nX_demo = np.random.randn(N, D)\ny_demo_lbls = np.random.randint(0,C,size=(N,))\ny_demo_oh   = np.eye(C)[y_demo_lbls]\n</pre> def gradient_norm(nn, X_batch, y_batch):     _ = nn.forward_propagation(X_batch)     grads = nn.backward_propagation(y_batch)     total_sq = 0     for dw, db in zip(grads['dW'], grads['db']):         total_sq += np.sum(dw**2) + np.sum(db**2)     return np.sqrt(total_sq)  def train_with_grad_tracking(nn, X, y, epochs=20, batch_size=32, lr=0.01):     m = X.shape[0]     grad_norms = []     losses = []      for e in range(epochs):         perm = np.random.permutation(m)         X_shuf = X[perm]         y_shuf = y[perm]         epoch_loss = 0         num_batches = int(np.ceil(m/batch_size))          for i in range(num_batches):             start = i*batch_size             end = min((i+1)*batch_size, m)             X_batch = X_shuf[start:end]             y_batch = y_shuf[start:end]              # measure gradient norm before update (arbitrary choice)             gn = gradient_norm(nn, X_batch, y_batch)             grad_norms.append(gn)              b_loss = nn.train_step(X_batch, y_batch, lr)             epoch_loss += b_loss*(end-start)          epoch_loss /= m         losses.append(epoch_loss)      return grad_norms, losses  # We'll create artificial data for demonstration # E.g. 500 samples, dimension=20, 5-output classification N = 500 D = 20 C = 5  X_demo = np.random.randn(N, D) y_demo_lbls = np.random.randint(0,C,size=(N,)) y_demo_oh   = np.eye(C)[y_demo_lbls] In\u00a0[11]: Copied! <pre># 5.1 Vanishing Gradient Example\n\nclass SmallInitNet(NeuralNetwork):\n    def initialize_parameters(self):\n        # Extremely small init =&gt; prone to vanishing\n        self.weights = []\n        self.biases = []\n        for i in range(1, self.num_layers):\n            W = np.random.normal(0, 1e-2, (self.layer_sizes[i-1], self.layer_sizes[i]))\n            b = np.zeros((1, self.layer_sizes[i]))\n            self.weights.append(W)\n            self.biases.append(b)\n\n# All 'sigmoid' =&gt; more saturation =&gt; easier vanishing\nvanish_net = SmallInitNet(\n    layer_sizes=[20, 64, 64, 5],\n    activations=['tanh','tanh','sigmoid','sigmoid']\n)\n\n\ngn_vanish, loss_vanish = train_with_grad_tracking(\n    vanish_net,\n    X_demo,     # e.g. a random or real dataset of shape (N, 20)\n    y_demo_oh,  # e.g. one-hot labels of shape (N, 5)\n    epochs=20,\n    batch_size=16,\n    lr=0.01\n)\n\nplt.figure(figsize=(10,4))\n\n# Left: gradient norm curve\nplt.subplot(1,2,1)\nplt.plot(gn_vanish)\nplt.title('Vanishing Gradient Norm (Deep Net + Very Small Init + Tanh and Sigmoid)')\nplt.xlabel('Batch Updates')\nplt.ylabel('Grad Norm')\n\n# Right: training loss curve\nplt.subplot(1,2,2)\nplt.plot(loss_vanish)\nplt.title('Vanishing - Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.tight_layout()\nplt.show()\n</pre> # 5.1 Vanishing Gradient Example  class SmallInitNet(NeuralNetwork):     def initialize_parameters(self):         # Extremely small init =&gt; prone to vanishing         self.weights = []         self.biases = []         for i in range(1, self.num_layers):             W = np.random.normal(0, 1e-2, (self.layer_sizes[i-1], self.layer_sizes[i]))             b = np.zeros((1, self.layer_sizes[i]))             self.weights.append(W)             self.biases.append(b)  # All 'sigmoid' =&gt; more saturation =&gt; easier vanishing vanish_net = SmallInitNet(     layer_sizes=[20, 64, 64, 5],     activations=['tanh','tanh','sigmoid','sigmoid'] )   gn_vanish, loss_vanish = train_with_grad_tracking(     vanish_net,     X_demo,     # e.g. a random or real dataset of shape (N, 20)     y_demo_oh,  # e.g. one-hot labels of shape (N, 5)     epochs=20,     batch_size=16,     lr=0.01 )  plt.figure(figsize=(10,4))  # Left: gradient norm curve plt.subplot(1,2,1) plt.plot(gn_vanish) plt.title('Vanishing Gradient Norm (Deep Net + Very Small Init + Tanh and Sigmoid)') plt.xlabel('Batch Updates') plt.ylabel('Grad Norm')  # Right: training loss curve plt.subplot(1,2,2) plt.plot(loss_vanish) plt.title('Vanishing - Training Loss') plt.xlabel('Epoch') plt.ylabel('Loss')  plt.tight_layout() plt.show()  In\u00a0[12]: Copied! <pre># 5.2 Exploding Gradient Example\nclass LargeInitNet(NeuralNetwork):\n    def initialize_parameters(self):\n        self.weights = []\n        self.biases = []\n        for i in range(1, self.num_layers):\n            W = np.random.normal(0, 1e-1, (self.layer_sizes[i-1], self.layer_sizes[i]))\n            b = np.zeros((1, self.layer_sizes[i]))\n            self.weights.append(W)\n            self.biases.append(b)\n\nexplode_net = LargeInitNet(\n    layer_sizes=[20,64,64,64,5],\n    activations=['relu','relu','relu','softmax']\n)\n\ngn_explode, loss_explode = train_with_grad_tracking(\n    explode_net,\n    X_demo, y_demo_oh,\n    epochs=20,\n    batch_size=8,\n    lr=3.0  # intentionally large\n)\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(gn_explode)\nplt.title('Exploding Gradient Norm (Large Init + LR=1.0)')\nplt.xlabel('Batch updates')\nplt.ylabel('Grad Norm')\n\nplt.subplot(1,2,2)\nplt.plot(loss_explode)\nplt.title('Exploding - Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.tight_layout()\nplt.show()\n</pre> # 5.2 Exploding Gradient Example class LargeInitNet(NeuralNetwork):     def initialize_parameters(self):         self.weights = []         self.biases = []         for i in range(1, self.num_layers):             W = np.random.normal(0, 1e-1, (self.layer_sizes[i-1], self.layer_sizes[i]))             b = np.zeros((1, self.layer_sizes[i]))             self.weights.append(W)             self.biases.append(b)  explode_net = LargeInitNet(     layer_sizes=[20,64,64,64,5],     activations=['relu','relu','relu','softmax'] )  gn_explode, loss_explode = train_with_grad_tracking(     explode_net,     X_demo, y_demo_oh,     epochs=20,     batch_size=8,     lr=3.0  # intentionally large )  plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.plot(gn_explode) plt.title('Exploding Gradient Norm (Large Init + LR=1.0)') plt.xlabel('Batch updates') plt.ylabel('Grad Norm')  plt.subplot(1,2,2) plt.plot(loss_explode) plt.title('Exploding - Training Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.tight_layout() plt.show()"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#building-neural-networks-from-scratch-with-numpy","title":"Building Neural Networks from Scratch with NumPy\u00b6","text":"<p>This notebook demonstrates how to:</p> <ol> <li>Implement a feedforward neural network (forward and backward pass) using NumPy.</li> <li>Train the network on spam detection from the <code>NotShrirang/email-spam-filter</code> dataset, using TF-IDF for text.</li> <li>Experiment with different batch sizes and learning rates.</li> <li>Illustrate vanishing and exploding gradients in deeper networks.</li> </ol> <p>No Keras or TensorFlow is used\u2014it's all done with core Python/NumPy.</p>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#1-activation-and-loss-functions-numpy","title":"1. Activation and Loss Functions (NumPy)\u00b6","text":"<p>We'll define some basic building blocks: <code>sigmoid</code>, <code>relu</code>, <code>softmax</code>, etc., plus a cross-entropy loss function.</p>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#2-base-neural-network-in-numpy","title":"2. Base Neural Network in NumPy\u00b6","text":"<p>Implements forward/backward passes, He initialization, and can train via a simple loop. We'll do mini-batches inside a <code>.train(...)</code> method for basic gradient descent.</p>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#quick-xor-check","title":"Quick XOR Check\u00b6","text":"<p>We confirm that the above network can learn XOR with a small architecture.</p>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#3-spam-detection-numpy","title":"3. Spam Detection (NumPy)\u00b6","text":"<p>We will:</p> <ol> <li>Load <code>NotShrirang/email-spam-filter</code></li> <li>Split train/dev/test</li> <li>TF-IDF vectorize</li> <li>Convert labels to one-hot for a 2-output softmax</li> <li>Train a <code>[2000 -&gt; 64 -&gt; 2]</code> network for ~10 epochs.</li> </ol>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#31-split-into-traindevtest-tf-idf-and-one-hot","title":"3.1 Split into Train/Dev/Test, TF-IDF, and One-Hot\u00b6","text":""},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#32-build-a-2000-64-2-net-with-relu-softmax","title":"3.2 Build a <code>[2000-&gt;64-&gt;2]</code> Net with ReLU + Softmax\u00b6","text":"<p>Train for 10 epochs and check the test accuracy.</p>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#33-plot-the-trainingdev-loss","title":"3.3 Plot the Training/Dev Loss\u00b6","text":""},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#4-different-batch-sizes-and-learning-rates-on-spam-data-numpy","title":"4. Different Batch Sizes and Learning Rates on Spam Data (NumPy)\u00b6","text":"<p>We'll define a short experiment, training for a small number of epochs, just to illustrate how the training loss changes across <code>(batch_size, learning_rate)</code> combos.</p>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#5-vanishing-and-exploding-gradients","title":"5. Vanishing and Exploding Gradients\u00b6","text":"<p>We'll build deeper networks in NumPy. By controlling:</p> <ul> <li>Small init or <code>sigmoid</code> for vanishing</li> <li>Large init or large LR for exploding We'll track the gradient norm at each epoch to see the pattern.</li> </ul>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#observing-the-norm-plots","title":"Observing the Norm Plots\u00b6","text":"<ul> <li>Vanishing: the gradient norm typically goes near 0, leading to minimal changes in weights.</li> <li>Exploding: the gradient norm can skyrocket (sometimes NaN), and training becomes unstable.</li> </ul>"},{"location":"chapter2/Session_2_1_NeuralNets_with_Numpy/#conclusion","title":"Conclusion\u00b6","text":"<p>We've built a neural network with pure NumPy:</p> <ul> <li>It handles feedforward/backprop.</li> <li>We tested it on XOR.</li> <li>We applied it to spam detection using TF-IDF.</li> <li>We tried different batch sizes and learning rates.</li> <li>We demonstrated vanishing/exploding gradients by tracking gradient norms in a deeper net.</li> </ul> <p>This clarifies many core concepts that frameworks like PyTorch or TensorFlow manage under the hood.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/","title":"Tiny Shakespeare RNN (Inspired by Karpathy)","text":"In\u00a0[1]: Copied! <pre># Install dependencies if needed:\n# !pip install datasets torch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nseed = 42\ntorch.manual_seed(seed)\n</pre> # Install dependencies if needed: # !pip install datasets torch  import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt from datasets import load_dataset  device = 'cuda' if torch.cuda.is_available() else 'cpu' seed = 42 torch.manual_seed(seed) Out[1]: <pre>&lt;torch._C.Generator at 0x10bce6950&gt;</pre> In\u00a0[2]: Copied! <pre>tiny_data = load_dataset('karpathy/tiny_shakespeare', )\ntrain_texts = tiny_data['train']['text']  # list of strings\n\n# Typically there's only 1 record with entire text.\nall_text = \" \".join(train_texts)\nprint(f\"Dataset length in chars: {len(all_text)}\")\n\n# Build vocab\nchars = sorted(list(set(all_text)))\nvocab_size = len(chars)\nprint(f\"Vocabulary size: {vocab_size}\")\n\n# char to int, int to char\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for ch,i in stoi.items() }\n</pre> tiny_data = load_dataset('karpathy/tiny_shakespeare', ) train_texts = tiny_data['train']['text']  # list of strings  # Typically there's only 1 record with entire text. all_text = \" \".join(train_texts) print(f\"Dataset length in chars: {len(all_text)}\")  # Build vocab chars = sorted(list(set(all_text))) vocab_size = len(chars) print(f\"Vocabulary size: {vocab_size}\")  # char to int, int to char stoi = { ch:i for i,ch in enumerate(chars) } itos = { i:ch for ch,i in stoi.items() } <pre>Dataset length in chars: 1003854\nVocabulary size: 65\n</pre> <p>We\u2019ll define a function to encode sequences of characters to integer IDs and decode back to characters. Then we\u2019ll create train splits. For simplicity, we\u2019ll just keep the entire text as a single sequence, though more sophisticated approaches might do chunking.</p> In\u00a0[3]: Copied! <pre>def encode_text(text):\n    return [stoi[ch] for ch in text]\n\ndef decode_ids(ids):\n    return ''.join(itos[i] for i in ids)\n\ndata_ids = torch.tensor(encode_text(all_text), dtype=torch.long)\nprint(\"Encoded data shape:\", data_ids.shape)\n\n# Let's do 90% for train, 10% for val\nn = int(0.9*len(data_ids))\ntrain_ids = data_ids[:n]\nval_ids   = data_ids[n:]\nprint(\"train, val shapes:\", train_ids.shape, val_ids.shape)\n</pre> def encode_text(text):     return [stoi[ch] for ch in text]  def decode_ids(ids):     return ''.join(itos[i] for i in ids)  data_ids = torch.tensor(encode_text(all_text), dtype=torch.long) print(\"Encoded data shape:\", data_ids.shape)  # Let's do 90% for train, 10% for val n = int(0.9*len(data_ids)) train_ids = data_ids[:n] val_ids   = data_ids[n:] print(\"train, val shapes:\", train_ids.shape, val_ids.shape) <pre>Encoded data shape: torch.Size([1003854])\ntrain, val shapes: torch.Size([903468]) torch.Size([100386])\n</pre> In\u00a0[4]: Copied! <pre>class CharRNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim=64, hidden_size=128):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=1, batch_first=True)\n        self.fc  = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, hidden=None):\n        # x: (batch, seq)\n        emb = self.embed(x)  # (batch, seq, embed_dim)\n        if hidden is None:\n            out, hidden = self.rnn(emb)\n        else:\n            out, hidden = self.rnn(emb, hidden)\n\n        logits = self.fc(out)  # (batch, seq, vocab_size)\n        return logits, hidden\n\n    def generate(self, start_char, max_new_tokens=100):\n        # quick sampling method\n        self.eval()\n        with torch.no_grad():\n            hidden = None\n            x = torch.tensor([[stoi[start_char]]], dtype=torch.long, device=device)\n            out_str = [start_char]\n            for _ in range(max_new_tokens):\n                logits, hidden = self.forward(x, hidden=hidden)\n                # take the last time step\n                last_logits = logits[:,-1,:]  # shape (1, vocab_size)\n                probs = F.softmax(last_logits, dim=-1)\n                # sample\n                ix = torch.multinomial(probs, num_samples=1)\n                x = ix\n                ch = itos[ix.item()]\n                out_str.append(ch)\n            return ''.join(out_str)\n\n    def next_token_probs(self, context):\n        # Returns the next-token prob distribution for a given context string.\n        self.eval()\n        with torch.no_grad():\n            hidden = None\n            x = torch.tensor([encode_text(context)], dtype=torch.long, device=device)\n            logits, hidden = self.forward(x)\n            last_logits = logits[0,-1,:]  # shape (vocab_size,)\n            probs = F.softmax(last_logits, dim=0)\n        return probs.cpu().numpy()\n\n    def get_activations(self, context):\n        # We'll fetch hidden state after feeding context\n        self.eval()\n        with torch.no_grad():\n            x = torch.tensor([encode_text(context)], dtype=torch.long, device=device)\n            emb = self.embed(x)\n            out, hidden = self.rnn(emb)\n            # out shape: (1, seq_len, hidden_size)\n            # hidden: tuple((1, batch, hidden_size), (1, batch, hidden_size)) for LSTM\n        return out.squeeze(0).cpu().numpy()  # shape: (seq_len, hidden_size)\n</pre> class CharRNN(nn.Module):     def __init__(self, vocab_size, embed_dim=64, hidden_size=128):         super().__init__()         self.embed = nn.Embedding(vocab_size, embed_dim)         self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=1, batch_first=True)         self.fc  = nn.Linear(hidden_size, vocab_size)      def forward(self, x, hidden=None):         # x: (batch, seq)         emb = self.embed(x)  # (batch, seq, embed_dim)         if hidden is None:             out, hidden = self.rnn(emb)         else:             out, hidden = self.rnn(emb, hidden)          logits = self.fc(out)  # (batch, seq, vocab_size)         return logits, hidden      def generate(self, start_char, max_new_tokens=100):         # quick sampling method         self.eval()         with torch.no_grad():             hidden = None             x = torch.tensor([[stoi[start_char]]], dtype=torch.long, device=device)             out_str = [start_char]             for _ in range(max_new_tokens):                 logits, hidden = self.forward(x, hidden=hidden)                 # take the last time step                 last_logits = logits[:,-1,:]  # shape (1, vocab_size)                 probs = F.softmax(last_logits, dim=-1)                 # sample                 ix = torch.multinomial(probs, num_samples=1)                 x = ix                 ch = itos[ix.item()]                 out_str.append(ch)             return ''.join(out_str)      def next_token_probs(self, context):         # Returns the next-token prob distribution for a given context string.         self.eval()         with torch.no_grad():             hidden = None             x = torch.tensor([encode_text(context)], dtype=torch.long, device=device)             logits, hidden = self.forward(x)             last_logits = logits[0,-1,:]  # shape (vocab_size,)             probs = F.softmax(last_logits, dim=0)         return probs.cpu().numpy()      def get_activations(self, context):         # We'll fetch hidden state after feeding context         self.eval()         with torch.no_grad():             x = torch.tensor([encode_text(context)], dtype=torch.long, device=device)             emb = self.embed(x)             out, hidden = self.rnn(emb)             # out shape: (1, seq_len, hidden_size)             # hidden: tuple((1, batch, hidden_size), (1, batch, hidden_size)) for LSTM         return out.squeeze(0).cpu().numpy()  # shape: (seq_len, hidden_size) In\u00a0[5]: Copied! <pre>def get_batch(data_ids, seq_len=128, batch_size=32):\n    # Random starting indices\n    ix = np.random.randint(0, len(data_ids) - seq_len - 1, (batch_size,))\n    Xb = []\n    Yb = []\n    for i in ix:\n        chunk = data_ids[i:i+seq_len]\n        target = data_ids[i+1:i+seq_len+1]\n        Xb.append(chunk)\n        Yb.append(target)\n    # Use torch.stack to combine the tensors\n    Xb = torch.stack(Xb)\n    Yb = torch.stack(Yb)\n    return Xb, Yb\n</pre> def get_batch(data_ids, seq_len=128, batch_size=32):     # Random starting indices     ix = np.random.randint(0, len(data_ids) - seq_len - 1, (batch_size,))     Xb = []     Yb = []     for i in ix:         chunk = data_ids[i:i+seq_len]         target = data_ids[i+1:i+seq_len+1]         Xb.append(chunk)         Yb.append(target)     # Use torch.stack to combine the tensors     Xb = torch.stack(Xb)     Yb = torch.stack(Yb)     return Xb, Yb  In\u00a0[6]: Copied! <pre>model = CharRNN(vocab_size=vocab_size, embed_dim=64, hidden_size=128).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n\ndef compute_loss_fn(logits, targets):\n    B,S,V = logits.shape\n    logits_flat = logits.view(B*S, V)\n    targets_flat = targets.view(B*S)\n    return F.cross_entropy(logits_flat, targets_flat)\n\nmax_epochs = 3000\nseq_len = 128\nbatch_size = 32\nlosses = []\n\nfor epoch in range(0, max_epochs+1):\n    model.train()\n    Xb, Yb = get_batch(train_ids, seq_len=seq_len, batch_size=batch_size)\n    Xb, Yb = Xb.to(device), Yb.to(device)\n    optimizer.zero_grad()\n    logits, _ = model(Xb)\n    loss = compute_loss_fn(logits, Yb)\n    loss.backward()\n    optimizer.step()\n\n    losses.append(loss.item())\n\n    if epoch%50==0:\n        print(f\"Epoch {epoch}, loss: {loss.item():.4f}\")\n    \n    # sample text at epochs 50, 100, 150, 200, 250, 300...\n    if epoch in range(0, 3001, 50):\n        model.eval()\n        sample = model.generate(start_char='T', max_new_tokens=200)\n        print(f\"\\n=== Sample after epoch {epoch} ===\\n{sample}\\n===========================\\n\")\n</pre> model = CharRNN(vocab_size=vocab_size, embed_dim=64, hidden_size=128).to(device) optimizer = torch.optim.Adam(model.parameters(), lr=0.003)  def compute_loss_fn(logits, targets):     B,S,V = logits.shape     logits_flat = logits.view(B*S, V)     targets_flat = targets.view(B*S)     return F.cross_entropy(logits_flat, targets_flat)  max_epochs = 3000 seq_len = 128 batch_size = 32 losses = []  for epoch in range(0, max_epochs+1):     model.train()     Xb, Yb = get_batch(train_ids, seq_len=seq_len, batch_size=batch_size)     Xb, Yb = Xb.to(device), Yb.to(device)     optimizer.zero_grad()     logits, _ = model(Xb)     loss = compute_loss_fn(logits, Yb)     loss.backward()     optimizer.step()      losses.append(loss.item())      if epoch%50==0:         print(f\"Epoch {epoch}, loss: {loss.item():.4f}\")          # sample text at epochs 50, 100, 150, 200, 250, 300...     if epoch in range(0, 3001, 50):         model.eval()         sample = model.generate(start_char='T', max_new_tokens=200)         print(f\"\\n=== Sample after epoch {epoch} ===\\n{sample}\\n===========================\\n\") <pre>Epoch 0, loss: 4.1908\n\n=== Sample after epoch 0 ===\nT!'iEU;kTgUglOvNvkinqP;gRRCzryBWc;UcrRtoYz-JcZ;nbL?uZc TrqoS-Kr sS!v,xszIW?F!Cmz,Zp-&amp;\npHugDZok.z-Yf$cbzL$rDaB!\nApEgY-S lOQ\n wHYc?!JTR$pXRT$:qt..e:lFqOFw EHlpXNAHARJUVTmSgDN ',XJd,M!hYyrXRM;oJIlv.\n.byhy\n===========================\n\nEpoch 50, loss: 2.4465\n\n=== Sample after epoch 50 ===\nT:hen touvlh,\nWonorith I fouis kouso nnvo he'n son shofrshen th met aret altour y yor axLl, of fig orerl\n\nee too itisisg waplshathe sove, pnoc, be tof homr itn\n;e, toe.\nA h.\nEReN thedic fithe for imaus\n===========================\n\nEpoch 100, loss: 2.2058\n\n=== Sample after epoch 100 ===\nTould be crecke sont stell tuntat breccand nofthak thy book.\nFoun pont mudio fonce shet sy loost ss of thish vest thee, to-wed yur,; and, forith as seen cumne I it all owull dethe' file.er kev\nNome.\nTh\n===========================\n\nEpoch 150, loss: 2.0445\n\n=== Sample after epoch 150 ===\nTh ring thiph the shell chat kinent, and cakfingh?\nPinchabe the have.\nShy linve themsand, widing that thin I ham hes kinds\nOUH Nf rues attion a trirveat theum gekie beef, beat on I beav.\n\nLULENMV:\nHom \n===========================\n\nEpoch 200, loss: 2.0038\n\n=== Sample after epoch 200 ===\nT! havering deads of collouss rich.\n\nCOFYILIUS:\nThis to spouce: Rothe suth my parkarys\nCove's thun siap chadow'e, with hay greeng?\nAnd sten's ance will ic ulefte nocdoutule to blow, had puntorr of ou y\n===========================\n\nEpoch 250, loss: 1.9360\n\n=== Sample after epoch 250 ===\nT-KI\nLizend of my now rest a dilk you lingon:\nTo body?\n\nWARICHO:\nWe lord:\nO, father wist,\nGo ture a pead him comy that wither in these contrurs's!\nMy prise\nJdie. Vitine,\nOr fart at spasicared,\nNoned hi\n===========================\n\nEpoch 300, loss: 1.8067\n\n=== Sample after epoch 300 ===\nT:\nIf menter a habketh,\n\nCatciang,\nHow all thee case\nThere our daghter\nSo what, braind with dondake \nin stakn.\nSeiwele,\nWo eavem: and JKwarter hom all shall have it twe, tatle with hast.\n\nRUCIY:\nIn the\n===========================\n\nEpoch 350, loss: 1.8255\n\n=== Sample after epoch 350 ===\nTEO:\nBy to\nWhat apperopsce.\n\nLARIO:\nVormand then.\n\nMENENTIUS:\nToath proncomernid, in thou supon I iscgoot word homore him to winsed fonoum is'row--purt there celones you relioker-shalf your low?\nA till\n===========================\n\nEpoch 400, loss: 1.7309\n\n=== Sample after epoch 400 ===\nTheat,\nMull by your me, newed what I havring bose,\nMone that sh by Sill.\n\nRICHY:\nWhil deetby of he fuperins is the liencher chad; tite, heard.\nshim say, lowd?\nBut we ight King and by more 'notle shall \n===========================\n\nEpoch 450, loss: 1.6591\n\n=== Sample after epoch 450 ===\nTV\nRombrows knows be be astered with werrand awfores barswaid.\n\nSeind I'll there:\nto though it the vein let.\n\nLADY CARWICLIND:\nLectife hill for, stis, penwick,\nAnd gear? Thy inaturent, we will lears oy\n===========================\n\nEpoch 500, loss: 1.6838\n\n=== Sample after epoch 500 ===\nThre chanted langbatter? ake hearbly in islence lawing a oul with despand that, sit! CFlinter streacl thee vering of word alouncpedonce, thou then their daues Richt a knong I hord and but come alms the\n===========================\n\nEpoch 550, loss: 1.6996\n\n=== Sample after epoch 550 ===\nTI' she conmend and\nThat there was, is hound tounce\nher hive of thus veiases,\nAte sim\nI'll beining all not ver weeven, is eary o' tel, lite brage thy, Prainw.\n\nKING HENRTY OV:\nGo, you gold placinity an\n===========================\n\nEpoch 600, loss: 1.6507\n\n=== Sample after epoch 600 ===\nTGBURLIBAN:\nList the harten bantul, not buttent of he: I dome, aSmind not plasor?\n\nLON:\nO KING EDWARD III::\nThou, my frandighs in heow upon they?\n\nVOR:\nMeace was yet is nease that your ourpers!\nO mest \n===========================\n\nEpoch 650, loss: 1.5590\n\n=== Sample after epoch 650 ===\nThe earcued'd reme say I\nfoll? 'on she know of that countin\nthe party:\nCrorrain: the onoring figfer's bliing yurses, set theo wons dots the paich.'\n\nQUEEN ELIZA:\nNowe, sir; she all thy soons the waupy \n===========================\n\nEpoch 700, loss: 1.6363\n\n=== Sample after epoch 700 ===\nTust and belunfiers up;\nWhen it us is thee, father at the? As I hath distabes Hed or would word call, had\nit man beard-ghel?\n\nDUKE OF YORK:\n\nFrird:\nNot Grave; mown and yough youm abuth it spasted there\n===========================\n\nEpoch 750, loss: 1.6209\n\n=== Sample after epoch 750 ===\nTH:\nNeep.\n\nSOMINBET:\nCome grace!\n\nERCIT:\nLet hand tentroun where so as this' the draw,\nIt father is menty, thou melin's down,\nBe uss! o' gall. Schap, we seetland he priece?\nhas that so wops, all peast,\n===========================\n\nEpoch 800, loss: 1.6257\n\n=== Sample after epoch 800 ===\nTy.\n\nBENVOLIO:\nAy, she\nEvile appeaming mine come:\nBut twe lead, sir, loke thee Geem this with shoull Morrart and\nTo may your grace\nMy shold company murtly fold be perseft as swo;\nWhy, we beys this boy.\n===========================\n\nEpoch 850, loss: 1.6097\n\n=== Sample after epoch 850 ===\nTERONTE:\nWhat speak a repole-thim contale, and comes, by the perpot poieds: be go you spewamest have meet sich your daster,\nI did, nack, roy,--\nMusting, but where thring him! I cannaly love not I proil\n===========================\n\nEpoch 900, loss: 1.5910\n\n=== Sample after epoch 900 ===\nTPUF Lard:\nS didelf\nTo us hea, s know, we men, lofe, Gevent the\nspeed it the our serveins\nits, some mus,\nMy gring poor restaging held he may mighters requress to us.\nWhenoness the know\nAn as from the k\n===========================\n\nEpoch 950, loss: 1.5435\n\n=== Sample after epoch 950 ===\nTERS O, for prouns too smalt'd my fait,\nWhy, wilt he handward Rome is a griant--it-blood, or hours the king.\n\nFirst Servingmer:\nScause hill?\n\nELWARD:\nNo, my vowardles.\n\nGXELTER:\nAnd to thy trair.\nWhich\n===========================\n\nEpoch 1000, loss: 1.5303\n\n=== Sample after epoch 1000 ===\nThat,\nThe peace, beyoud done of play my bryose abours bewill not shapple here; as I may me ruming,\nThat saon, we are us seet thou cannot toldie-tint; and shall been of my foother an incont. We'eves;\nHe\n===========================\n\nEpoch 1050, loss: 1.5319\n\n=== Sample after epoch 1050 ===\nTe prity.\n\nOSFORK:\nNo; by roued.\n\nKING RICHARD III:\nWould have wetching after night.\nThon the bide.\nThe spoth or my his:\nNo follow fall.\n\nMENENIUS:\nWay away, of the comity\nYour deatn:\nRnet excellfmess \n===========================\n\nEpoch 1100, loss: 1.6140\n\n=== Sample after epoch 1100 ===\nTNuke lady not exfude, puke see, that cheere. This mistare to liek have use Edward wasten;\nFor a noblers. But is timeon'd ments up not;\nFor coundeal:\nA pray?\n\nKING HENRY VI:\nHell you are\nThat it; so be\n===========================\n\nEpoch 1150, loss: 1.5286\n\n=== Sample after epoch 1150 ===\nTINGBROKE:\nNot may the more!\nSo Mantumbatious gentlem! way,\nThy wisse 'If Comets than my mongers.\n\nPISARD MANNGRY:\nIf even's suldy last\nBecear off,\nI waul be'd thy hand;\nThat when free, beat if you are\n===========================\n\nEpoch 1200, loss: 1.5888\n\n=== Sample after epoch 1200 ===\nThat way the fair call you speak come to speak?\nNor ones\nThe world:\nIs you have it brist\nTo comfout,\nCenuly pitty of Hastinger's faticeer, and you care, how after I, hast and competty to me your lord. \n===========================\n\nEpoch 1250, loss: 1.5573\n\n=== Sample after epoch 1250 ===\nTcuntuls. I do be by must, feart\nTo me.\nWhom it with this:\nWe how as your hards, thy duke will be can leases,\nHow, by Kenrel in heir is nofly some hand, you will her is a pards; of his but 'at excust-c\n===========================\n\nEpoch 1300, loss: 1.5149\n\n=== Sample after epoch 1300 ===\nThus child made.\n\nCORIOLANUS:\nThen you cannot when so we heelf, every are pursed in my\nlevenge't and sad, there streath\nMine, it think am assident the victoraty: if entcam, they some, peace.\n\nBENVOLIO:\n===========================\n\nEpoch 1350, loss: 1.5509\n\n=== Sample after epoch 1350 ===\nTuling at up, or Rickle as it with me;\nThou, what good many ingain't\nbeing pleasure, no counser; the pasting my youstain'd cormost his plain, shis, there is gentrement my follows on eyes to the mottore\n===========================\n\nEpoch 1400, loss: 1.4928\n\n=== Sample after epoch 1400 ===\nTo strongs all,\nWhiise true, thou wiltlcoman and bidds, hearty, and therefore hate, leave be thy resome against cousin our brother, I pot our talous toone prease: and it is do?\n\nHERMERBESARD:\nWhat iste\n===========================\n\nEpoch 1450, loss: 1.5000\n\n=== Sample after epoch 1450 ===\nTar naturn that you let a death for slingure and for makes is gentlement's then common,\nCome?\n\nFirst Murderer:\nVincel to the gand King and appand, resure when chands, as God all Pully, bool, and thou t\n===========================\n\nEpoch 1500, loss: 1.4978\n\n=== Sample after epoch 1500 ===\nT:\nAnd look but thuson\ntooble thries, the but my ravoldden in twaths!\nAnd An? this is ciling, like pludines\nThe world.\nTakep me wrath to the him murse.\n\nPOMPEY:\nCome, my send of Englentle\nNow then I am\n===========================\n\nEpoch 1550, loss: 1.5436\n\n=== Sample after epoch 1550 ===\nThis moraft, that they for Mead\nAnd thou well.\nO, but thou shave\nAs once not such among thee run, thereance thy will to fair mentlememiet;\nShould he greath; I will not's his latester slain with honour'\n===========================\n\nEpoch 1600, loss: 1.5095\n\n=== Sample after epoch 1600 ===\nThol my smon breford IVed play is thy hoee, answer the heard!\nProvord'd truit\nrewar: Nor amonty;\nWho small fill, I tanny it\nO'erled!\nMy Lord what this maken\nand to the plastorys of our know breats and \n===========================\n\nEpoch 1650, loss: 1.5353\n\n=== Sample after epoch 1650 ===\nTice and child;\nAppey! 'Tswixted my bearned.\nHow to let make me good like me not\nme to more own asiness.\n\nCORIOLANUS:\nWhat we new, they now whoy he say! hose beginal was naith your singer to me;\nBesend\n===========================\n\nEpoch 1700, loss: 1.4564\n\n=== Sample after epoch 1700 ===\nTo seep them for years;\nWho has me not?\n\nNoLe I lady:\nThe shen'ly deprive Ournot letted,\nEndear thee is mother\nNor I\nGive the are-proper hath other, and with sudenity you was it. Your pride her away; h\n===========================\n\nEpoch 1750, loss: 1.4577\n\n=== Sample after epoch 1750 ===\nTERS:\nGords have hast, though, that ides with with thy speak to a yous arce,\nTo mible fellessgal bests, if by-condention.\n\nPRINCE EDWARD:\nNo, distry majesty,\nDiding of the\nLeard Poriuble Duking my fine\n===========================\n\nEpoch 1800, loss: 1.4972\n\n=== Sample after epoch 1800 ===\nTo less and biles with hast of my mocess--\nIn a kind made I find you,\nWith destreath\nPlate\nShall be least my brother beat should\ncondedue heats away\nTo knows it word, and him shame how must not, and wh\n===========================\n\nEpoch 1850, loss: 1.4385\n\n=== Sample after epoch 1850 ===\nTANGBY:\nAnd are out to thee, paint again from the thing in fastary; the say. We comes my moie;\nHis ear?\n\nAUTOLYCUS:\nSorrow not be lingel, wread on Bo.\n\nNussend:\nNo. Hence all was found to tell that woo\n===========================\n\nEpoch 1900, loss: 1.4748\n\n=== Sample after epoch 1900 ===\nThied have not become all the father it maipent to put earth with allise put mine lady's hurty thou banos never busines quitterce should done his elin.\nBut of is thou will sir, Baporning not thousesses\n===========================\n\nEpoch 1950, loss: 1.4893\n\n=== Sample after epoch 1950 ===\nTHBy Lord you to denutencanch a pinco\nWhem, my bodg! Good to there in bround covervein;\nHe say the gods.\nIf,\nTupest strazed do have you dots I frof marry when ever with thyself!\nWill hell my hands me d\n===========================\n\nEpoch 2000, loss: 1.4765\n\n=== Sample after epoch 2000 ===\nThy shall may Roman!\n\nAUTOLYCUS:\nLet empland? There\nIntiling tains not bloods:\nIn hear no changed, and that I am lady's pleasure from top to hear he our sperciat crusners, vance heavencus,\nNor for the \n===========================\n\nEpoch 2050, loss: 1.4833\n\n=== Sample after epoch 2050 ===\nThoist, and lay tho, I'll be on ermid blood, stay-famed to me,\nThe basweef sad work'd with on, tet is to pervany,\nAnd sweet them, By you, so we have shall you banirity-hat.\n\nActoer:\nWhy, why make you f\n===========================\n\nEpoch 2100, loss: 1.4370\n\n=== Sample after epoch 2100 ===\nT! Scarth o' now,\nPut frie's fortummeth of my high and on homes to hidlemnetters is fill,\nAnd, is am bening in dageth and our as young and end him.\n\nLUCIUS:\nHe carlies I was done to me,\nFor bring chang\n===========================\n\nEpoch 2150, loss: 1.4534\n\n=== Sample after epoch 2150 ===\nTI:\nMightress'd to your man read,\nShe seem\nI will make alasing willy myself.\n\nWARWICK:\nO, since that sir, if this very man comes herefork,\nHe shalt with when thou art but which as sweet beaut, where co\n===========================\n\nEpoch 2200, loss: 1.4652\n\n=== Sample after epoch 2200 ===\nThie\nsir,\nOr in etermy for since to char by secuty to seven other liquit\nOf God!'\nAmend,\nbut more tage\nTo it sorrow with born.\nAway of to England.\n\nISABELLA:\n'That pray upon the king.\n\nLEONTES:\nNot of \n===========================\n\nEpoch 2250, loss: 1.4394\n\n=== Sample after epoch 2250 ===\nThe need fear;\nWeht Talus I might doth reason's falling. Fir, shafe, I have forged, in my enemy of Bothand him sin and crotherwer too.\n\nTHOMAS MIWBR:\nNow'd it been to two Sicies: upon, I be speak are m\n===========================\n\nEpoch 2300, loss: 1.4567\n\n=== Sample after epoch 2300 ===\nThis true,\nThere I can slafe wits re;\nOf I? Yet-I like in the noble mimer of thy words more to wolkeng forger;\nAnd that rootes is end were rue my gracious worture of are thing thereaffer him and hart h\n===========================\n\nEpoch 2350, loss: 1.4665\n\n=== Sample after epoch 2350 ===\nTINGBROK:\nI do beaught whath defend.\n\nLUCIO:\nHow conlie and turn'd,\nAnd hath the neyard more! Ireporle no?\n\nCAMILLO:\nI did without the rammeggal princely action:\nWhen he have has not ascience; but they\n===========================\n\nEpoch 2400, loss: 1.4228\n\n=== Sample after epoch 2400 ===\nTHANRY OF LAD:\nUp, though not be slew the foot thou arm to kingdous death of all;\nIf Warwick what is not this mown I bements of Leinst, then were all these.\n\nPOLIXENES:\nO, good deadness.\n\nThird Antir o\n===========================\n\nEpoch 2450, loss: 1.3804\n\n=== Sample after epoch 2450 ===\nTINGBR OF AULEL:\nIf I have an every,\nAnd read much standed be far helbed upon off like Chirdly gracious; thou should to like when devise your looker; and let the Lord lommment Bencond it.\n\nClown:\nNo mo\n===========================\n\nEpoch 2500, loss: 1.4343\n\n=== Sample after epoch 2500 ===\nTHERS HARBEON:\nWhat is masters for sworn, and all the heart with a deet, I no\nloves and I'll put a divine after\nThat sin.\nI have things and strike and not me, strong of vitious one giff.\n\nKING RICHARD \n===========================\n\nEpoch 2550, loss: 1.4236\n\n=== Sample after epoch 2550 ===\nTo do I say, yet, my lord,\nHe should,\nAnd such you me out made it, holderly, Samibums,\nWill not new than hath it slaught as the peace.\n\nVOLUMNIA:\nTraivility.\n\nGREGORY:\nmore, not presence our friend\nHee\n===========================\n\nEpoch 2600, loss: 1.4352\n\n=== Sample after epoch 2600 ===\nThis\nshould yet offen cry me? What I thou westy beg thy princely and renothers, for success:\nFefer land their it,--wife, you do speed' the setself abling are design muging in on thy wit these queen,-lo\n===========================\n\nEpoch 2650, loss: 1.3579\n\n=== Sample after epoch 2650 ===\nThis:\nO furfaced. Pompere art.\nThine I merton.\n\nSICINIUS:\nYou, so lording enemy, ratter to Moncen,\nShe whils six your gracious calreaty\nnake mises the stonger I resent succossipt that I name.\nGod, defe\n===========================\n\nEpoch 2700, loss: 1.4649\n\n=== Sample after epoch 2700 ===\nTH lay'd encurate Hamb:\nBut lose heart, and postisnit of mastershin,\nOf our mootus.\n\nLORD STANLET:\nWhat that, he straights to Romeo the dissestance in this? What, is the lideness\nOcl warrion!\nWh Camili\n===========================\n\nEpoch 2750, loss: 1.3758\n\n=== Sample after epoch 2750 ===\nTurn.\n\nGLOUCESTER:\nSpeak to him in the sword adied cursb,\nOnd your a proveds; will save you,\nOne, if I must comes me she'er by burn'd of the pursue and for upon with her by dost been unousurence.\n? tho\n===========================\n\nEpoch 2800, loss: 1.4230\n\n=== Sample after epoch 2800 ===\nThose wounds away.\n\nLUCIO:\nI partel Chirdly schasters by the wounded with the offigrow and the Boin'd attend me the mind play, noble King are rest.\n\nGLOUCESTER:\nAway, tell he your, been, I dare lameedn\n===========================\n\nEpoch 2850, loss: 1.4183\n\n=== Sample after epoch 2850 ===\nThite the give; since there can thou shalt lovely dew,\nThanks woman, your pale,\nLe't person from him.\n\nKING RICHARD III:\nSir I smeets right,\nFor it worship, you, 'twas had were\nsomething.\n\nKING RICHARD\n===========================\n\nEpoch 2900, loss: 1.4308\n\n=== Sample after epoch 2900 ===\nThe rewer,\nWhat Henry that honourach for then, care to procliots:\nAft will here\n\nChord\nThat I must known heart loys away in put so ophould herp in wruth's upon this herd?\nSee he know'st noble disserbt,\n===========================\n\nEpoch 2950, loss: 1.4675\n\n=== Sample after epoch 2950 ===\nThas thereof onmils. O, if lies in this liberty,\nAnd lay trouble.\n\nQUEEN ELIZABETH:\nThe worpen your gracious consmalish and\nTo thost! Comener did! all be them not he hapor, grant home to speak,\nTo time\n===========================\n\nEpoch 3000, loss: 1.3882\n\n=== Sample after epoch 3000 ===\nTH lord, her honour, he hat yet people,\nLord to this live, they save me sworn it should hence:\nThese knew, and true arcean of him of it alive a gods had the\nThat to back remauses and late, weary says\nA\n===========================\n\n</pre> <p>Quite impressive, at first we have just random characters, but after a few epochs we see that the networks have learned to create bags of characters. Then it really starts to generate coherent words even if the bag of words are not very good. And we see some patterns with capital letters and punctuation. It definitely gets the idea of the character speaking. And it looks to master the punctuation at the end, and even we get some kind of good couple of words in a row.</p> In\u00a0[7]: Copied! <pre>plt.plot(losses)\nplt.title('Training Loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.show()\n</pre> plt.plot(losses) plt.title('Training Loss') plt.xlabel('Iteration') plt.ylabel('Loss') plt.show() In\u00a0[8]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import HTML, display\n\n\ndef top5_probs_across_phrase(model, phrase, stoi, itos):\n    \"\"\"\n    For each character in 'phrase', compute the RNN model's next-token probability\n    distribution, pick top-5 tokens, store them, and return a matrix for heatmap, etc.\n\n    model: A char-level RNN (PyTorch) with a forward(...) that returns (logits, hidden).\n    phrase: The input string we feed character by character.\n    stoi: dict mapping chars -&gt; integer IDs.\n    itos: dict mapping integer IDs -&gt; chars.\n\n    Returns:\n      top5_probs: shape (T, 5), top-5 probabilities for each time step\n      top5_chars: list of lists of top-5 characters per time step\n    \"\"\"\n    model.eval()\n    distributions = []\n    top5_indices_list = []\n\n    hidden = None\n    with torch.no_grad():\n        for t in range(len(phrase)):\n            # feed context up to (t+1)th character\n            context_sub = phrase[:t+1]\n            x = torch.tensor([[stoi[ch] for ch in context_sub]], dtype=torch.long, device=device)\n            logits, hidden = model(x, hidden=hidden)\n\n            # take the last output =&gt; next char distribution\n            last_logits = logits[0, -1, :]  # shape (vocab_size,)\n            probs = torch.softmax(last_logits, dim=-1).cpu().numpy()\n\n            distributions.append(probs)\n\n            # top-5 indices\n            idx_sorted = np.argsort(-probs)\n            top5_idx = idx_sorted[:5]\n            top5_indices_list.append(top5_idx)\n\n    # Convert to arrays\n    distributions = np.array(distributions)  # shape (T, vocab_size)\n    T = len(phrase)\n    top5_probs = np.zeros((T, 5))\n    top5_chars = []\n\n    for i in range(T):\n        t5i = top5_indices_list[i]\n        t5p = distributions[i][t5i]\n        top5_probs[i,:] = t5p\n        t5c = [itos[j] for j in t5i]\n        top5_chars.append(t5c)\n\n    return top5_probs, top5_chars\n</pre> import numpy as np import matplotlib.pyplot as plt import seaborn as sns from IPython.display import HTML, display   def top5_probs_across_phrase(model, phrase, stoi, itos):     \"\"\"     For each character in 'phrase', compute the RNN model's next-token probability     distribution, pick top-5 tokens, store them, and return a matrix for heatmap, etc.      model: A char-level RNN (PyTorch) with a forward(...) that returns (logits, hidden).     phrase: The input string we feed character by character.     stoi: dict mapping chars -&gt; integer IDs.     itos: dict mapping integer IDs -&gt; chars.      Returns:       top5_probs: shape (T, 5), top-5 probabilities for each time step       top5_chars: list of lists of top-5 characters per time step     \"\"\"     model.eval()     distributions = []     top5_indices_list = []      hidden = None     with torch.no_grad():         for t in range(len(phrase)):             # feed context up to (t+1)th character             context_sub = phrase[:t+1]             x = torch.tensor([[stoi[ch] for ch in context_sub]], dtype=torch.long, device=device)             logits, hidden = model(x, hidden=hidden)              # take the last output =&gt; next char distribution             last_logits = logits[0, -1, :]  # shape (vocab_size,)             probs = torch.softmax(last_logits, dim=-1).cpu().numpy()              distributions.append(probs)              # top-5 indices             idx_sorted = np.argsort(-probs)             top5_idx = idx_sorted[:5]             top5_indices_list.append(top5_idx)      # Convert to arrays     distributions = np.array(distributions)  # shape (T, vocab_size)     T = len(phrase)     top5_probs = np.zeros((T, 5))     top5_chars = []      for i in range(T):         t5i = top5_indices_list[i]         t5p = distributions[i][t5i]         top5_probs[i,:] = t5p         t5c = [itos[j] for j in t5i]         top5_chars.append(t5c)      return top5_probs, top5_chars  In\u00a0[9]: Copied! <pre># Example usage\ncontext_phrase = \"The orchard is\"\n\ntop5_matrix, top5_chs = top5_probs_across_phrase(model, context_phrase, stoi, itos)\n\n# 1) Print top-5 tokens for each step\nfor i, ch in enumerate(context_phrase):\n    print(f\"\\nAfter reading '{context_phrase[:i+1]}' (last char '{ch}'):\")\n    t5c = top5_chs[i]\n    t5p = top5_matrix[i]\n    # sort them by descending probability for readability\n    idx_desc = np.argsort(-t5p)\n    for rank in range(5):\n        char_ = t5c[idx_desc[rank]]\n        prob_ = t5p[idx_desc[rank]]\n        print(f\"   '{char_}' -&gt; {prob_:.3f}\")\n\n# 2) Heatmap of top-5 probabilities (T x 5)\nplt.figure(figsize=(8,6))\nsns.heatmap(top5_matrix, annot=False, cmap='Blues')\nplt.title(f\"Top-5 Probability Heatmap for '{context_phrase}'\")\nplt.xlabel(\"Top-5 tokens (not necessarily the same at each step)\") \nplt.ylabel(\"Time-step in phrase\")\nplt.show()\n</pre> # Example usage context_phrase = \"The orchard is\"  top5_matrix, top5_chs = top5_probs_across_phrase(model, context_phrase, stoi, itos)  # 1) Print top-5 tokens for each step for i, ch in enumerate(context_phrase):     print(f\"\\nAfter reading '{context_phrase[:i+1]}' (last char '{ch}'):\")     t5c = top5_chs[i]     t5p = top5_matrix[i]     # sort them by descending probability for readability     idx_desc = np.argsort(-t5p)     for rank in range(5):         char_ = t5c[idx_desc[rank]]         prob_ = t5p[idx_desc[rank]]         print(f\"   '{char_}' -&gt; {prob_:.3f}\")  # 2) Heatmap of top-5 probabilities (T x 5) plt.figure(figsize=(8,6)) sns.heatmap(top5_matrix, annot=False, cmap='Blues') plt.title(f\"Top-5 Probability Heatmap for '{context_phrase}'\") plt.xlabel(\"Top-5 tokens (not necessarily the same at each step)\")  plt.ylabel(\"Time-step in phrase\") plt.show() <pre>\nAfter reading 'T' (last char 'T'):\n   'h' -&gt; 0.437\n   'o' -&gt; 0.184\n   'I' -&gt; 0.073\n   'H' -&gt; 0.055\n   'E' -&gt; 0.049\n\nAfter reading 'Th' (last char 'h'):\n   'y' -&gt; 0.262\n   'e' -&gt; 0.221\n   'i' -&gt; 0.209\n   'o' -&gt; 0.174\n   'u' -&gt; 0.078\n\nAfter reading 'The' (last char 'e'):\n   'n' -&gt; 0.414\n   'r' -&gt; 0.232\n   's' -&gt; 0.181\n   ' ' -&gt; 0.100\n   'R' -&gt; 0.022\n\nAfter reading 'The ' (last char ' '):\n   'r' -&gt; 0.232\n   'C' -&gt; 0.091\n   'm' -&gt; 0.060\n   's' -&gt; 0.060\n   't' -&gt; 0.048\n\nAfter reading 'The o' (last char 'o'):\n   'r' -&gt; 0.194\n   't' -&gt; 0.183\n   'n' -&gt; 0.162\n   'w' -&gt; 0.137\n   'f' -&gt; 0.093\n\nAfter reading 'The or' (last char 'r'):\n   ' ' -&gt; 0.865\n   'd' -&gt; 0.021\n   '\n' -&gt; 0.019\n   'e' -&gt; 0.019\n   'i' -&gt; 0.011\n\nAfter reading 'The orc' (last char 'c'):\n   'e' -&gt; 0.769\n   'h' -&gt; 0.123\n   'o' -&gt; 0.078\n   'u' -&gt; 0.020\n   'i' -&gt; 0.004\n\nAfter reading 'The orch' (last char 'h'):\n   'e' -&gt; 0.382\n   'u' -&gt; 0.232\n   'a' -&gt; 0.196\n   'i' -&gt; 0.116\n   'o' -&gt; 0.061\n\nAfter reading 'The orcha' (last char 'a'):\n   'l' -&gt; 0.416\n   'n' -&gt; 0.231\n   'r' -&gt; 0.105\n   's' -&gt; 0.097\n   'm' -&gt; 0.036\n\nAfter reading 'The orchar' (last char 'r'):\n   'd' -&gt; 0.721\n   'g' -&gt; 0.053\n   'r' -&gt; 0.050\n   'e' -&gt; 0.045\n   's' -&gt; 0.026\n\nAfter reading 'The orchard' (last char 'd'):\n   ',' -&gt; 0.304\n   ' ' -&gt; 0.149\n   'e' -&gt; 0.129\n   '.' -&gt; 0.074\n   ';' -&gt; 0.063\n\nAfter reading 'The orchard ' (last char ' '):\n   't' -&gt; 0.171\n   'i' -&gt; 0.113\n   'o' -&gt; 0.079\n   'w' -&gt; 0.077\n   'a' -&gt; 0.077\n\nAfter reading 'The orchard i' (last char 'i'):\n   'n' -&gt; 0.483\n   's' -&gt; 0.394\n   't' -&gt; 0.082\n   'm' -&gt; 0.014\n   'f' -&gt; 0.013\n\nAfter reading 'The orchard is' (last char 's'):\n   ' ' -&gt; 0.903\n   ',' -&gt; 0.037\n   '\n' -&gt; 0.019\n   '.' -&gt; 0.017\n   ';' -&gt; 0.006\n</pre> <p>We see that sometimes the network is really sure about the next character, and sometimes it is not. Maybe we should look at clear characters to look for potential overfitting.</p> In\u00a0[10]: Copied! <pre>def color_val_for_activation(a_value):\n    \"\"\"\n    Map activation in [-1, 1] to a color gradient:\n    blue at -1, white at 0, green at 1.\n    R=0..0, G=0..255, B=255..0\n    \"\"\"\n    av = max(-1.0, min(1.0, a_value))\n    scale = (av + 1.0) * 0.5  # 0 =&gt; -1, 1 =&gt; +1\n    g = int(scale * 255)\n    b = int((1.0 - scale) * 255)\n    return f\"rgb(0,{g},{b})\"\n\ndef color_val_for_prob(p):\n    \"\"\"\n    Map probability in [0,1] to a color from white(0) to red(1).\n    We'll do purely red: white =&gt; p=0 =&gt; rgb(255,255,255)\n                        red =&gt; p=1 =&gt; rgb(255,0,0)\n    \"\"\"\n    p_clamped = max(0.0, min(1.0, p))\n    r = 255\n    g = int(255 - 255*p_clamped)\n    b = int(255 - 255*p_clamped)\n    return f\"rgb({r},{g},{b})\"\n\ndef visualize_firing_and_guesses(\n    text,\n    hidden_vals,     # shape (T, hidden_size)\n    top5_ix,         # shape (T, 5)\n    top5_probs,      # shape (T, 5)\n    encoded_input,   # shape (T,)\n    neuron_idx = 0,\n    itos = None\n):\n    \"\"\"\n    text: original input string\n    hidden_vals: (T, hidden_size), hidden states for each time step\n    top5_ix: (T, 5)\n    top5_probs: (T, 5)\n    neuron_idx: which dimension of hidden state to visualize\n    itos: index-&gt;char mapping\n    \"\"\"\n    T = len(text)\n    html = []\n    html.append(\"&lt;div style='font-family:monospace;'&gt;\")\n    html.append(\"&lt;table&gt;\")\n\n    # Row 1: input characters colored by hidden state dimension\n    html.append(\"&lt;tr&gt;\")\n    for t in range(T):\n        char_ = text[t]\n        a_val = hidden_vals[t, neuron_idx]  # e.g. LSTM hidden dimension\n        ccol = color_val_for_activation(a_val)\n        html.append(f\"&lt;td style='background-color:{ccol}; padding:4px;'&gt;{char_}&lt;/td&gt;\")\n    html.append(\"&lt;/tr&gt;\")\n\n    # Row 2: top-5 guesses\n    html.append(\"&lt;tr&gt;\")\n    for t in range(T):\n        guesses_ix = top5_ix[t]\n        guesses_pb = top5_probs[t]\n        cell_lines = []\n        for i in range(5):\n            gix = guesses_ix[i]\n            p_ = guesses_pb[i]\n            ch_ = itos[gix] if itos else f\"{gix}\"\n            ccol = color_val_for_prob(p_)\n            cell_lines.append(f\"&lt;span style='background-color:{ccol}'&gt;{ch_} {p_:.2f}&lt;/span&gt;\")\n        cell_html = \"&lt;br/&gt;\".join(cell_lines)\n        html.append(f\"&lt;td style='vertical-align:top; padding:4px;'&gt;{cell_html}&lt;/td&gt;\")\n    html.append(\"&lt;/tr&gt;\")\n\n    html.append(\"&lt;/table&gt;\")\n    html.append(\"&lt;/div&gt;\")\n    return \"\".join(html)\n\n# We'll define a hypothetical function that obtains hidden states + top5\n# for each step of a phrase. It's similar to 'top5_probs_across_phrase', but also\n# we store hidden states. Let's call it `get_firing_and_top5(...)`.\n\ndef get_firing_and_top5(model, text, stoi, itos):\n    \"\"\"\n    Returns:\n      hidden_vals: shape (T, hidden_size)\n      top5_ix: (T,5)\n      top5_probs: (T,5)\n    \"\"\"\n    model.eval()\n    hidden_vals = []\n    top5_ix = []\n    top5_pb = []\n    hidden = None\n\n    with torch.no_grad():\n        for t in range(len(text)):\n            context_sub = text[:t+1]\n            x = torch.tensor([[stoi[ch] for ch in context_sub]], dtype=torch.long, device=device)\n            logits, hidden_state = model(x, hidden=hidden)\n            hidden = hidden_state  # LSTM =&gt; tuple (h, c)\n\n            # Extract hidden vector =&gt; shape (1,1,hidden_size) =&gt; pick [0,0,:]\n            # or for a GRU =&gt; shape (1,1,hidden_size)\n            # We'll assume LSTM for example:\n            h_vec = hidden_state[0][0,0,:].cpu().numpy()  # shape (hidden_size,)\n            hidden_vals.append(h_vec)\n\n            # Next char dist\n            last_logits = logits[0,-1,:]\n            probs = torch.softmax(last_logits, dim=-1).cpu().numpy()\n\n            # top-5\n            idx_sorted = np.argsort(-probs)\n            t5i = idx_sorted[:5]\n            t5p = probs[t5i]\n\n            top5_ix.append(t5i)\n            top5_pb.append(t5p)\n\n    hidden_vals = np.stack(hidden_vals, axis=0)     # (T, hidden_size)\n    top5_ix = np.stack(top5_ix, axis=0)            # (T, 5)\n    top5_pb = np.stack(top5_pb, axis=0)            # (T, 5)\n    return hidden_vals, top5_ix, top5_pb\n\n# Now let's do final usage:\nmy_text = \"The orchard is\"\nhvals, t5ix, t5pb = get_firing_and_top5(model, my_text, stoi, itos)\n\nhtml_str = visualize_firing_and_guesses(\n    text=my_text,\n    hidden_vals=hvals,\n    top5_ix=t5ix,\n    top5_probs=t5pb,\n    encoded_input=[stoi[ch] for ch in my_text],\n    neuron_idx=10,   # e.g. dimension #10\n    itos=itos\n)\n\ndisplay(HTML(html_str))\n</pre> def color_val_for_activation(a_value):     \"\"\"     Map activation in [-1, 1] to a color gradient:     blue at -1, white at 0, green at 1.     R=0..0, G=0..255, B=255..0     \"\"\"     av = max(-1.0, min(1.0, a_value))     scale = (av + 1.0) * 0.5  # 0 =&gt; -1, 1 =&gt; +1     g = int(scale * 255)     b = int((1.0 - scale) * 255)     return f\"rgb(0,{g},{b})\"  def color_val_for_prob(p):     \"\"\"     Map probability in [0,1] to a color from white(0) to red(1).     We'll do purely red: white =&gt; p=0 =&gt; rgb(255,255,255)                         red =&gt; p=1 =&gt; rgb(255,0,0)     \"\"\"     p_clamped = max(0.0, min(1.0, p))     r = 255     g = int(255 - 255*p_clamped)     b = int(255 - 255*p_clamped)     return f\"rgb({r},{g},{b})\"  def visualize_firing_and_guesses(     text,     hidden_vals,     # shape (T, hidden_size)     top5_ix,         # shape (T, 5)     top5_probs,      # shape (T, 5)     encoded_input,   # shape (T,)     neuron_idx = 0,     itos = None ):     \"\"\"     text: original input string     hidden_vals: (T, hidden_size), hidden states for each time step     top5_ix: (T, 5)     top5_probs: (T, 5)     neuron_idx: which dimension of hidden state to visualize     itos: index-&gt;char mapping     \"\"\"     T = len(text)     html = []     html.append(\"\")     html.append(\"\")      # Row 1: input characters colored by hidden state dimension     html.append(\"\")     for t in range(T):         char_ = text[t]         a_val = hidden_vals[t, neuron_idx]  # e.g. LSTM hidden dimension         ccol = color_val_for_activation(a_val)         html.append(f\"{char_}\")     html.append(\"\")      # Row 2: top-5 guesses     html.append(\"\")     for t in range(T):         guesses_ix = top5_ix[t]         guesses_pb = top5_probs[t]         cell_lines = []         for i in range(5):             gix = guesses_ix[i]             p_ = guesses_pb[i]             ch_ = itos[gix] if itos else f\"{gix}\"             ccol = color_val_for_prob(p_)             cell_lines.append(f\"{ch_} {p_:.2f}\")         cell_html = \"\".join(cell_lines)         html.append(f\"{cell_html}\")     html.append(\"\")      html.append(\"\")     html.append(\"\")     return \"\".join(html)  # We'll define a hypothetical function that obtains hidden states + top5 # for each step of a phrase. It's similar to 'top5_probs_across_phrase', but also # we store hidden states. Let's call it `get_firing_and_top5(...)`.  def get_firing_and_top5(model, text, stoi, itos):     \"\"\"     Returns:       hidden_vals: shape (T, hidden_size)       top5_ix: (T,5)       top5_probs: (T,5)     \"\"\"     model.eval()     hidden_vals = []     top5_ix = []     top5_pb = []     hidden = None      with torch.no_grad():         for t in range(len(text)):             context_sub = text[:t+1]             x = torch.tensor([[stoi[ch] for ch in context_sub]], dtype=torch.long, device=device)             logits, hidden_state = model(x, hidden=hidden)             hidden = hidden_state  # LSTM =&gt; tuple (h, c)              # Extract hidden vector =&gt; shape (1,1,hidden_size) =&gt; pick [0,0,:]             # or for a GRU =&gt; shape (1,1,hidden_size)             # We'll assume LSTM for example:             h_vec = hidden_state[0][0,0,:].cpu().numpy()  # shape (hidden_size,)             hidden_vals.append(h_vec)              # Next char dist             last_logits = logits[0,-1,:]             probs = torch.softmax(last_logits, dim=-1).cpu().numpy()              # top-5             idx_sorted = np.argsort(-probs)             t5i = idx_sorted[:5]             t5p = probs[t5i]              top5_ix.append(t5i)             top5_pb.append(t5p)      hidden_vals = np.stack(hidden_vals, axis=0)     # (T, hidden_size)     top5_ix = np.stack(top5_ix, axis=0)            # (T, 5)     top5_pb = np.stack(top5_pb, axis=0)            # (T, 5)     return hidden_vals, top5_ix, top5_pb  # Now let's do final usage: my_text = \"The orchard is\" hvals, t5ix, t5pb = get_firing_and_top5(model, my_text, stoi, itos)  html_str = visualize_firing_and_guesses(     text=my_text,     hidden_vals=hvals,     top5_ix=t5ix,     top5_probs=t5pb,     encoded_input=[stoi[ch] for ch in my_text],     neuron_idx=10,   # e.g. dimension #10     itos=itos )  display(HTML(html_str))  The orchard ish 0.44o 0.18I 0.07H 0.06E 0.05y 0.26e 0.22i 0.21o 0.17u 0.08n 0.41r 0.23s 0.18  0.10R 0.02r 0.23C 0.09m 0.06s 0.06t 0.05r 0.19t 0.18n 0.16w 0.14f 0.09  0.86d 0.02  0.02e 0.02i 0.01e 0.77h 0.12o 0.08u 0.02i 0.00e 0.38u 0.23a 0.20i 0.12o 0.06l 0.42n 0.23r 0.10s 0.10m 0.04d 0.72g 0.05r 0.05e 0.05s 0.03, 0.30  0.15e 0.13. 0.07; 0.06t 0.17i 0.11o 0.08w 0.08a 0.08n 0.48s 0.39t 0.08m 0.01f 0.01  0.90, 0.04  0.02. 0.02; 0.01 <p>So yes, we see the difference between some predictions, like for instance it's quite sure about <code>d</code> after <code>a</code> but not about <code>h</code> after <code>c</code>.</p> In\u00a0[11]: Copied! <pre># Now let's do final usage:\nmy_text = \"Have you heard\"\nhvals, t5ix, t5pb = get_firing_and_top5(model, my_text, stoi, itos)\n\nhtml_str = visualize_firing_and_guesses(\n    text=my_text,\n    hidden_vals=hvals,\n    top5_ix=t5ix,\n    top5_probs=t5pb,\n    encoded_input=[stoi[ch] for ch in my_text],\n    neuron_idx=10,   # e.g. dimension #10\n    itos=itos\n)\n\ndisplay(HTML(html_str))\n</pre> # Now let's do final usage: my_text = \"Have you heard\" hvals, t5ix, t5pb = get_firing_and_top5(model, my_text, stoi, itos)  html_str = visualize_firing_and_guesses(     text=my_text,     hidden_vals=hvals,     top5_ix=t5ix,     top5_probs=t5pb,     encoded_input=[stoi[ch] for ch in my_text],     neuron_idx=10,   # e.g. dimension #10     itos=itos )  display(HTML(html_str)) Have you heardA 0.30e 0.27a 0.13i 0.06E 0.05v 0.48t 0.25n 0.08d 0.07s 0.05e 0.88i 0.12y 0.00o 0.00a 0.00  0.85s 0.04  0.03, 0.02n 0.01t 0.24m 0.13y 0.10a 0.07h 0.05o 0.85e 0.12i 0.03a 0.00u 0.00u 0.99n 0.00w 0.00r 0.00t 0.00r 0.46  0.32, 0.09. 0.04  0.03t 0.26a 0.10w 0.07b 0.07n 0.07a 0.51e 0.23i 0.16o 0.09u 0.01a 0.40r 0.32n 0.10  0.07l 0.05r 0.71v 0.15d 0.10t 0.02l 0.01  0.53s 0.10d 0.10t 0.09, 0.05  0.50, 0.18  0.09. 0.05s 0.03 <p>And here we see that actually the netwrok is more sure about the next character. It may be quite correlated with the frequencies of the characters in the training set. This is something we should look at.</p> In\u00a0[12]: Copied! <pre>acts = model.get_activations(\"Thou art\")  # shape (seq_len, hidden_size)\nprint(\"Activations shape:\", acts.shape)\nplt.figure(figsize=(10,3))\nplt.imshow(acts.T, aspect='auto', cmap='viridis')\nplt.colorbar()\nplt.title(\"RNN Hidden State Activations (each row = one hidden dim)\")\nplt.xlabel(\"Time Step (for each char in 'Thou art')\")\nplt.ylabel(\"Hidden Dimension\")\nplt.show()\n</pre> acts = model.get_activations(\"Thou art\")  # shape (seq_len, hidden_size) print(\"Activations shape:\", acts.shape) plt.figure(figsize=(10,3)) plt.imshow(acts.T, aspect='auto', cmap='viridis') plt.colorbar() plt.title(\"RNN Hidden State Activations (each row = one hidden dim)\") plt.xlabel(\"Time Step (for each char in 'Thou art')\") plt.ylabel(\"Hidden Dimension\") plt.show() <pre>Activations shape: (8, 128)\n</pre> <p>From the above heatmap, you can see how some hidden dimensions are more active (bright) or less active (dark) at each time step. That shows what the RNN is focusing on, reminiscent of Karpathy\u2019s blog post approach.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#tiny-shakespeare-rnn-inspired-by-karpathy","title":"Tiny Shakespeare RNN (Inspired by Karpathy)\u00b6","text":"<p>In this notebook:</p> <ol> <li>Load the karpathy/tiny_shakespeare dataset.</li> <li>Build a simple character-level RNN (PyTorch) to predict next characters.</li> <li>Train for multiple epochs, showing generated text after 10, 20, 30 epochs, etc.</li> <li>Display next-token probability distributions.</li> <li>Display an activation map for the RNN (hidden states) to see what\u2019s lighting up.</li> </ol> <p>For reference, see Andrej Karpathy\u2019s RNN effectiveness blog post.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#1-load-the-tiny-shakespeare-data","title":"1. Load the Tiny Shakespeare Data\u00b6","text":"<p>We get a single text string from the dataset, then build a char-level vocabulary.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#2-create-a-char-level-rnn-model","title":"2. Create a Char-Level RNN Model\u00b6","text":"<p>We\u2019ll do a small 1-layer LSTM or basic RNN that outputs logits over <code>vocab_size</code> for each step. (You can also do a custom naive RNN if you prefer following Karpathy\u2019s minimal example, but here we can do PyTorch\u2019s built-in so we can easily show the hidden states, etc.)</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#21-prepare-a-batch-loader-function","title":"2.1 Prepare a batch loader function\u00b6","text":"<p>We\u2019ll define a simple function that, given a chunk of text, returns <code>(X, Y)</code> where <code>X</code> is input chars and <code>Y</code> is next chars.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#3-training-the-rnn","title":"3. Training the RNN\u00b6","text":"<p>We\u2019ll train for 3000 epochs total, sampling text after training for 50 epochs to show progress. The blog post indicates small RNNs can generate surprisingly coherent text even after a short time.</p> <p>We also keep the next-token probability map and an activation map for a sample context.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#plot-training-loss","title":"Plot Training Loss\u00b6","text":""},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#4-next-token-probability-map","title":"4. Next-Token Probability Map\u00b6","text":"<p>We'll pick a small context (e.g. <code>'Thou art'</code>) and see the predicted distribution for the next character.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#1-function-top5_probs_across_phrase","title":"1) Function: <code>top5_probs_across_phrase</code>\u00b6","text":"<p>This function:</p> <ul> <li>Iterates over a given phrase character by character,</li> <li>Feeds the substring to the model,</li> <li>Captures the softmax distribution for the next character,</li> <li>Extracts the top-5 indices and probabilities for each position,</li> <li>Returns a matrix of probabilities and the corresponding top-5 characters.</li> </ul>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#2-using-top5_probs_across_phrase-heatmap","title":"2) Using <code>top5_probs_across_phrase</code> + Heatmap\u00b6","text":"<ul> <li>We generate top-5 predictions for each step of a phrase.</li> <li>We print them in text form.</li> <li>We create a heatmap to visualize how probabilities evolve over each time step.</li> </ul>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#3-color-functions-for-activation-and-probability","title":"3) Color Functions for Activation and Probability\u00b6","text":"<p>The next functions color a value from [-1,1] in a blue-&gt;white-&gt;green scale, and from [0,1] in white-&gt;red scale, for probability.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#5-activation-map","title":"5. Activation Map\u00b6","text":"<p>We\u2019ll pick the same context <code>'Thou art'</code> and retrieve the RNN hidden states to see which dimensions are lighting up. We\u2019ll do a simple heatmap with matplotlib.</p>"},{"location":"chapter2/Session_2_2_Text_Generation_with_RNN/#conclusion","title":"Conclusion\u00b6","text":"<ul> <li>We loaded <code>tiny_shakespeare</code>, built a small character-level LSTM in PyTorch.</li> <li>We trained for multiple epochs, showing sample text at epochs 10, 20, 30.</li> <li>We displayed next-token probability distribution for a chosen context.</li> <li>We plotted the RNN hidden state activation map to see which neurons activate.</li> </ul> <p>Following Karpathy\u2019s blog post, even a small RNN can learn to generate Shakespeare-like text with enough training epochs and the right hyperparameters. This approach demonstrates how each additional epoch yields more coherent text.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/","title":"LSTM Text Classification vs. TF-IDF + Logistic Regression","text":"In\u00a0[1]: Copied! <pre># !pip install scikit-learn tensorflow keras matplotlib seaborn --quiet\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nnp.random.seed(42)\n</pre> # !pip install scikit-learn tensorflow keras matplotlib seaborn --quiet  import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from datasets import load_dataset from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, classification_report  import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers  np.random.seed(42) In\u00a0[2]: Copied! <pre># Placeholder: load your data here.\n# For demonstration, let's create a small synthetic dataset.\n\n# Load the imdb\ndataset = load_dataset('imdb')\nprint(dataset)\n\n\n# Convert to pandas DataFrame for easier manipulation\ntrain_df = dataset['train'].shuffle(seed=42).to_pandas()\ntest_df = dataset['test'].shuffle(seed=42).to_pandas()\n# Clean the memory\ndel dataset\n</pre> # Placeholder: load your data here. # For demonstration, let's create a small synthetic dataset.  # Load the imdb dataset = load_dataset('imdb') print(dataset)   # Convert to pandas DataFrame for easier manipulation train_df = dataset['train'].shuffle(seed=42).to_pandas() test_df = dataset['test'].shuffle(seed=42).to_pandas() # Clean the memory del dataset <pre>DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n</pre> In\u00a0[3]: Copied! <pre># 3.1 Vectorize text with TF-IDF\ntfidf_vec = TfidfVectorizer(stop_words='english', min_df=5, max_features=20000)\n\nX_train_tfidf = tfidf_vec.fit_transform(train_df['text'])\ny_train = train_df['label'].values\n\nX_test_tfidf = tfidf_vec.transform(test_df['text'])\ny_test = test_df['label'].values\n\nprint(\"X_train shape:\", X_train_tfidf.shape, \", y_train shape:\", y_train.shape)\n</pre> # 3.1 Vectorize text with TF-IDF tfidf_vec = TfidfVectorizer(stop_words='english', min_df=5, max_features=20000)  X_train_tfidf = tfidf_vec.fit_transform(train_df['text']) y_train = train_df['label'].values  X_test_tfidf = tfidf_vec.transform(test_df['text']) y_test = test_df['label'].values  print(\"X_train shape:\", X_train_tfidf.shape, \", y_train shape:\", y_train.shape) <pre>X_train shape: (25000, 20000) , y_train shape: (25000,)\n</pre> In\u00a0[4]: Copied! <pre># 3.2 Train a Logistic Regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train_tfidf, y_train)\n\n# Evaluate on test\ny_pred_logreg = logreg.predict(X_test_tfidf)\n\nprint(\"LogisticRegression Test Classification Report:\")\nprint(classification_report(y_test, y_pred_logreg))\ncm_logreg = confusion_matrix(y_test, y_pred_logreg)\nprint(\"Confusion Matrix:\", cm_logreg)\n</pre> # 3.2 Train a Logistic Regression model logreg = LogisticRegression() logreg.fit(X_train_tfidf, y_train)  # Evaluate on test y_pred_logreg = logreg.predict(X_test_tfidf)  print(\"LogisticRegression Test Classification Report:\") print(classification_report(y_test, y_pred_logreg)) cm_logreg = confusion_matrix(y_test, y_pred_logreg) print(\"Confusion Matrix:\", cm_logreg) <pre>LogisticRegression Test Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.88      0.88     12500\n           1       0.88      0.88      0.88     12500\n\n    accuracy                           0.88     25000\n   macro avg       0.88      0.88      0.88     25000\nweighted avg       0.88      0.88      0.88     25000\n\nConfusion Matrix: [[10979  1521]\n [ 1495 11005]]\n</pre> In\u00a0[5]: Copied! <pre>plt.figure(figsize=(4,4))\nsns.heatmap(cm_logreg, annot=True, cmap='Blues', fmt='d', cbar=False)\nplt.title(\"LogReg TF-IDF - Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()\n</pre> plt.figure(figsize=(4,4)) sns.heatmap(cm_logreg, annot=True, cmap='Blues', fmt='d', cbar=False) plt.title(\"LogReg TF-IDF - Confusion Matrix\") plt.xlabel(\"Predicted\") plt.ylabel(\"True\") plt.show() In\u00a0[6]: Copied! <pre>from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 1000\nmax_len = 64\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"&lt;OOV&gt;\")\ntokenizer.fit_on_texts(train_df['text'])\n</pre> from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences  vocab_size = 1000 max_len = 64  tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"\") tokenizer.fit_on_texts(train_df['text']) In\u00a0[7]: Copied! <pre>seqs = tokenizer.texts_to_sequences(train_df['text'])\nprint(seqs[0])\nprint(tokenizer.sequences_to_texts([seqs[0]]))\nprint(train_df['text'][0])\n</pre> seqs = tokenizer.texts_to_sequences(train_df['text']) print(seqs[0]) print(tokenizer.sequences_to_texts([seqs[0]])) print(train_df['text'][0]) <pre>[48, 7, 55, 1, 31, 30, 198, 1, 3, 1, 19, 2, 190, 13, 197, 24, 566, 199, 42, 1, 1, 1, 270, 1, 1, 270, 354, 1, 1, 24, 177, 604, 1, 112, 24, 228, 51, 1, 1, 270, 51, 38, 1, 1, 45, 73, 26, 6, 1, 1, 2, 291, 107, 7, 813, 3, 1, 19, 26, 1, 82, 38, 6, 1, 6, 1, 6, 1, 87, 42, 41, 1, 161, 152, 97, 82, 485, 1, 270, 296, 19, 21, 2, 83, 506, 1, 34, 1, 296, 199, 277, 43, 2, 1, 40, 2, 1, 19, 11, 102, 12, 199, 7, 51, 629, 72, 296, 32, 2, 94, 2, 154, 24, 64, 50, 3, 161, 2, 114, 7, 22, 1, 31, 30]\n[\"there is no &lt;OOV&gt; at all between &lt;OOV&gt; and &lt;OOV&gt; but the fact that both are police series about &lt;OOV&gt; &lt;OOV&gt; &lt;OOV&gt; looks &lt;OOV&gt; &lt;OOV&gt; looks classic &lt;OOV&gt; &lt;OOV&gt; are quite simple &lt;OOV&gt; plot are far more &lt;OOV&gt; &lt;OOV&gt; looks more like &lt;OOV&gt; &lt;OOV&gt; if we have to &lt;OOV&gt; &lt;OOV&gt; the main character is weak and &lt;OOV&gt; but have &lt;OOV&gt; people like to &lt;OOV&gt; to &lt;OOV&gt; to &lt;OOV&gt; how about just &lt;OOV&gt; funny thing too people writing &lt;OOV&gt; looks american but on the other hand &lt;OOV&gt; they &lt;OOV&gt; american series maybe it's the &lt;OOV&gt; or the &lt;OOV&gt; but i think this series is more english than american by the way the actors are really good and funny the acting is not &lt;OOV&gt; at all\"]\nThere is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n</pre> In\u00a0[8]: Copied! <pre>seqs_padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n</pre> seqs_padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post') In\u00a0[9]: Copied! <pre>#Create the tokenizer with the same vocabulary as the TF-IDF vectorizer\ncustom_vocab = tfidf_vec.vocabulary_\nvocab_size = len(custom_vocab) + 1 # +1 for the OOV token\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"&lt;OOV&gt;\")\n\n# Manually assign word index\ntokenizer.word_index = {word: i for i, word in enumerate(custom_vocab)}\ntokenizer.word_index[tokenizer.oov_token] = len(custom_vocab)\nprint(len(tokenizer.word_index))\n\n#DO NOT fit the tokenizer on the training data otherwise you will have a mismatch between the tokenizer and the TF-IDF vectorizer\n\n##Let's increase the max length of the sequences to 128\nmax_len = 128\n\n# Let's create a function to convert the text to a sequence of token IDs and pad them to the same length: 128\n\ndef text_to_seq(df_col, max_len=128):\n    seqs = tokenizer.texts_to_sequences(df_col)\n    # pad\n    seqs_padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n    return seqs_padded\n\nX_train_seq = text_to_seq(train_df['text'].map(lambda x: x.lower()), max_len=max_len)\nX_test_seq = text_to_seq(test_df['text'].map(lambda x: x.lower()), max_len=max_len)\n\ny_train_lstm = train_df['label'].values\ny_test_lstm  = test_df['label'].values\n\nprint(train_df['text'][0].split()[:max_len])\nprint(X_train_seq[0])\nprint(y_train_lstm[0])\n\nX_train_seq.shape, y_train_lstm.shape\n</pre> #Create the tokenizer with the same vocabulary as the TF-IDF vectorizer custom_vocab = tfidf_vec.vocabulary_ vocab_size = len(custom_vocab) + 1 # +1 for the OOV token tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"\")  # Manually assign word index tokenizer.word_index = {word: i for i, word in enumerate(custom_vocab)} tokenizer.word_index[tokenizer.oov_token] = len(custom_vocab) print(len(tokenizer.word_index))  #DO NOT fit the tokenizer on the training data otherwise you will have a mismatch between the tokenizer and the TF-IDF vectorizer  ##Let's increase the max length of the sequences to 128 max_len = 128  # Let's create a function to convert the text to a sequence of token IDs and pad them to the same length: 128  def text_to_seq(df_col, max_len=128):     seqs = tokenizer.texts_to_sequences(df_col)     # pad     seqs_padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')     return seqs_padded  X_train_seq = text_to_seq(train_df['text'].map(lambda x: x.lower()), max_len=max_len) X_test_seq = text_to_seq(test_df['text'].map(lambda x: x.lower()), max_len=max_len)  y_train_lstm = train_df['label'].values y_test_lstm  = test_df['label'].values  print(train_df['text'][0].split()[:max_len]) print(X_train_seq[0]) print(y_train_lstm[0])  X_train_seq.shape, y_train_lstm.shape <pre>20001\n['There', 'is', 'no', 'relation', 'at', 'all', 'between', 'Fortier', 'and', 'Profiler', 'but', 'the', 'fact', 'that', 'both', 'are', 'police', 'series', 'about', 'violent', 'crimes.', 'Profiler', 'looks', 'crispy,', 'Fortier', 'looks', 'classic.', 'Profiler', 'plots', 'are', 'quite', 'simple.', \"Fortier's\", 'plot', 'are', 'far', 'more', 'complicated...', 'Fortier', 'looks', 'more', 'like', 'Prime', 'Suspect,', 'if', 'we', 'have', 'to', 'spot', 'similarities...', 'The', 'main', 'character', 'is', 'weak', 'and', 'weirdo,', 'but', 'have', '\"clairvoyance\".', 'People', 'like', 'to', 'compare,', 'to', 'judge,', 'to', 'evaluate.', 'How', 'about', 'just', 'enjoying?', 'Funny', 'thing', 'too,', 'people', 'writing', 'Fortier', 'looks', 'American', 'but,', 'on', 'the', 'other', 'hand,', 'arguing', 'they', 'prefer', 'American', 'series', '(!!!).', 'Maybe', \"it's\", 'the', 'language,', 'or', 'the', 'spirit,', 'but', 'I', 'think', 'this', 'series', 'is', 'more', 'English', 'than', 'American.', 'By', 'the', 'way,', 'the', 'actors', 'are', 'really', 'good', 'and', 'funny.', 'The', 'acting', 'is', 'not', 'superficial', 'at', 'all...']\n[20000 20000 20000     0 20000 20000 20000 20000 20000 20000 20000 20000\n     1 20000 20000 20000     2     3 20000     4     5 20000     6 20000\n 20000     6     7 20000     8 20000     9    10 20000    11 20000    12\n 20000    13 20000     6 20000    14    15    16 20000 20000 20000 20000\n    17    18 20000    19    20 20000    21 20000    22 20000 20000    23\n    24    14 20000    25 20000    26 20000    27 20000 20000    28    29\n    30    31 20000    24    32 20000     6    33 20000 20000 20000 20000\n    34    35 20000    36    33     3    37 20000 20000    38 20000 20000\n    39 20000 20000    40 20000     3 20000 20000    41 20000    33 20000\n 20000    42 20000    43 20000    44    45 20000    30 20000    46 20000\n 20000    47 20000 20000     0     0     0     0]\n1\n</pre> Out[9]: <pre>((25000, 128), (25000,))</pre> In\u00a0[10]: Copied! <pre># let's look at the first review in the training set\n# and see the TF-IDF vector where we have the weights for each word\n# and look at weights &gt; 0\n\nprint('TF-IDF vector for the first review in the training set:')\nprint(tfidf_vec.transform(train_df['text'][:1]).toarray())\n\nprint('\\nTF-IDF vector for the first review in the training set where we have the weights for each word:')\nnon_zero_indices = np.where(tfidf_vec.transform(train_df['text'][:1]).toarray() &gt; 0)[1]\nfor index in non_zero_indices[:10]:\n    print(f\"{tfidf_vec.get_feature_names_out()[index]}, {index} -&gt; {tfidf_vec.transform(train_df['text'][:1]).toarray()[0, index]}\")\n\nprint('\\nTF-IDF vector for the first review in the training set where we have the weights for each word:')\nprint(tfidf_vec.transform(train_df['text'][:1]).toarray()[0, non_zero_indices])\n</pre> # let's look at the first review in the training set # and see the TF-IDF vector where we have the weights for each word # and look at weights &gt; 0  print('TF-IDF vector for the first review in the training set:') print(tfidf_vec.transform(train_df['text'][:1]).toarray())  print('\\nTF-IDF vector for the first review in the training set where we have the weights for each word:') non_zero_indices = np.where(tfidf_vec.transform(train_df['text'][:1]).toarray() &gt; 0)[1] for index in non_zero_indices[:10]:     print(f\"{tfidf_vec.get_feature_names_out()[index]}, {index} -&gt; {tfidf_vec.transform(train_df['text'][:1]).toarray()[0, index]}\")  print('\\nTF-IDF vector for the first review in the training set where we have the weights for each word:') print(tfidf_vec.transform(train_df['text'][:1]).toarray()[0, non_zero_indices]) <pre>TF-IDF vector for the first review in the training set:\n[[0. 0. 0. ... 0. 0. 0.]]\n\nTF-IDF vector for the first review in the training set where we have the weights for each word:\nacting, 411 -&gt; 0.06453486881043621\nactors, 422 -&gt; 0.07451506986747358\namerican, 798 -&gt; 0.282849599154727\narguing, 1097 -&gt; 0.18188205992788842\ncharacter, 3121 -&gt; 0.06712120649061068\nclairvoyance, 3387 -&gt; 0.22993873098246653\nclassic, 3407 -&gt; 0.09600506078617589\ncompare, 3741 -&gt; 0.13682793832789433\ncomplicated, 3788 -&gt; 0.15271278591080203\ncrimes, 4363 -&gt; 0.16069252150549165\n\nTF-IDF vector for the first review in the training set where we have the weights for each word:\n[0.06453487 0.07451507 0.2828496  0.18188206 0.06712121 0.22993873\n 0.09600506 0.13682794 0.15271279 0.16069252 0.1132163  0.15364272\n 0.20665086 0.07954201 0.08338438 0.15625659 0.04963082 0.10494199\n 0.14360214 0.04733146 0.12566851 0.08952358 0.35838411 0.09075236\n 0.08986416 0.12151923 0.06613852 0.14117013 0.11334531 0.15152355\n 0.14943343 0.07901726 0.05558572 0.16987687 0.27193871 0.1675908\n 0.10945524 0.12873432 0.13291014 0.16355615 0.13930737 0.07366632\n 0.06416329 0.12690591 0.06153084 0.11729007 0.19316474 0.10345318]\n</pre> <p>Nevertheless, those weights don\u2019t capture relationships between words. They are learned from the data and are fixed for each word. We basically put all the information of one word in a single number. The semantic meaning is limited.</p> In\u00a0[11]: Copied! <pre>embedding_dim = 16\n# The input is a sequence of integers of length max_len (128)\ninputs = keras.Input(shape=(max_len,), dtype=\"int32\")\n# Embed each integer in a 16-dimensional vector\nx = layers.Embedding(vocab_size, embedding_dim)(inputs)\n# Add bidirectional LSTMs\nx = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(x)\nx = layers.Flatten()(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.summary()\n</pre> embedding_dim = 16 # The input is a sequence of integers of length max_len (128) inputs = keras.Input(shape=(max_len,), dtype=\"int32\") # Embed each integer in a 16-dimensional vector x = layers.Embedding(vocab_size, embedding_dim)(inputs) # Add bidirectional LSTMs x = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(x) x = layers.Flatten()(x) outputs = layers.Dense(1, activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs) model.summary() <pre>Model: \"functional\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer (InputLayer)        \u2502 (None, 128)            \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 embedding (Embedding)           \u2502 (None, 128, 16)        \u2502       320,016 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bidirectional (Bidirectional)   \u2502 (None, 128, 64)        \u2502        12,544 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatten (Flatten)               \u2502 (None, 8192)           \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense (Dense)                   \u2502 (None, 1)              \u2502         8,193 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 340,753 (1.30 MB)\n</pre> <pre> Trainable params: 340,753 (1.30 MB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[12]: Copied! <pre>optimizer = keras.optimizers.Adam(learning_rate=0.001)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=['accuracy', 'recall', 'precision']\n)\n</pre> optimizer = keras.optimizers.Adam(learning_rate=0.001)  model.compile(     optimizer=optimizer,     loss=keras.losses.BinaryCrossentropy(from_logits=False),     metrics=['accuracy', 'recall', 'precision'] ) In\u00a0[13]: Copied! <pre>from tensorflow.keras.callbacks import EarlyStopping\n\nhistory = model.fit(\n    X_train_seq, y_train_lstm,\n    validation_split=0.2,\n    epochs=20,\n    batch_size=128,\n    verbose=1,\n    callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n)\n</pre> from tensorflow.keras.callbacks import EarlyStopping  history = model.fit(     X_train_seq, y_train_lstm,     validation_split=0.2,     epochs=20,     batch_size=128,     verbose=1,     callbacks=[EarlyStopping(monitor='val_loss', patience=3)] ) <pre>Epoch 1/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 55ms/step - accuracy: 0.5588 - loss: 0.6901 - precision: 0.5706 - recall: 0.4375 - val_accuracy: 0.7020 - val_loss: 0.5479 - val_precision: 0.6357 - val_recall: 0.9717\nEpoch 2/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 67ms/step - accuracy: 0.8026 - loss: 0.4260 - precision: 0.7837 - recall: 0.8272 - val_accuracy: 0.8424 - val_loss: 0.3636 - val_precision: 0.8712 - val_recall: 0.8103\nEpoch 3/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 72ms/step - accuracy: 0.9045 - loss: 0.2484 - precision: 0.8973 - recall: 0.9110 - val_accuracy: 0.8550 - val_loss: 0.3502 - val_precision: 0.8479 - val_recall: 0.8716\nEpoch 4/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 74ms/step - accuracy: 0.9407 - loss: 0.1600 - precision: 0.9370 - recall: 0.9450 - val_accuracy: 0.8368 - val_loss: 0.4265 - val_precision: 0.8314 - val_recall: 0.8523\nEpoch 5/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 66ms/step - accuracy: 0.9700 - loss: 0.0837 - precision: 0.9673 - recall: 0.9729 - val_accuracy: 0.8358 - val_loss: 0.4794 - val_precision: 0.8446 - val_recall: 0.8303\nEpoch 6/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 72ms/step - accuracy: 0.9856 - loss: 0.0437 - precision: 0.9850 - recall: 0.9858 - val_accuracy: 0.8172 - val_loss: 0.7322 - val_precision: 0.8523 - val_recall: 0.7753\n</pre> In\u00a0[14]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title('LSTM Loss')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.legend()\nplt.title('LSTM Accuracy')\nplt.show()\n</pre> plt.figure(figsize=(12,4)) plt.subplot(1,2,1) plt.plot(history.history['loss'], label='Train Loss') plt.plot(history.history['val_loss'], label='Val Loss') plt.legend() plt.title('LSTM Loss')  plt.subplot(1,2,2) plt.plot(history.history['accuracy'], label='Train Acc') plt.plot(history.history['val_accuracy'], label='Val Acc') plt.legend() plt.title('LSTM Accuracy') plt.show() In\u00a0[15]: Copied! <pre>y_pred_lstm_prob = model.predict(X_test_seq)\ny_pred_lstm = (y_pred_lstm_prob&gt;0.5).astype(int)\nprint(\"LSTM Classification Report:\")\nprint(classification_report(y_test_lstm, y_pred_lstm))\n</pre> y_pred_lstm_prob = model.predict(X_test_seq) y_pred_lstm = (y_pred_lstm_prob&gt;0.5).astype(int) print(\"LSTM Classification Report:\") print(classification_report(y_test_lstm, y_pred_lstm)) <pre>782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 8ms/step\nLSTM Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.74      0.86      0.80     12500\n           1       0.83      0.70      0.76     12500\n\n    accuracy                           0.78     25000\n   macro avg       0.79      0.78      0.78     25000\nweighted avg       0.79      0.78      0.78     25000\n\n</pre> <p>Okay that's interesting. We see that the validation loss is decreasing more or less till epoch 3 and then it starts to increase. This is a sign of overfitting. Plus the validation accuracy is quite stable or slightly decreasing after the same number of epochs. We can try to reduce the overfitting by using regularization or dropout. We even see that the results are bad at the end of the training.</p> <p>Ans we see that the results are not so good (at least worst than the logistic regression with TF-IDF). 78% of accuracy is not so good compared to the 88% of the logistic regression with TF-IDF. The overfitting may be one reason. Let's try to reduce the overfitting.</p> In\u00a0[16]: Copied! <pre>embedding_dim = 16\ninputs = keras.Input(shape=(max_len,), dtype=\"int32\")\nx = layers.Embedding(vocab_size, embedding_dim)(inputs)\nx = layers.Bidirectional(layers.LSTM(32, return_sequences=True, \n                                     activity_regularizer=keras.regularizers.l2(0.01)))(x)\nx = layers.Flatten()(x)\nx = layers.Dropout(0.15)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.summary()\n</pre> embedding_dim = 16 inputs = keras.Input(shape=(max_len,), dtype=\"int32\") x = layers.Embedding(vocab_size, embedding_dim)(inputs) x = layers.Bidirectional(layers.LSTM(32, return_sequences=True,                                       activity_regularizer=keras.regularizers.l2(0.01)))(x) x = layers.Flatten()(x) x = layers.Dropout(0.15)(x) outputs = layers.Dense(1, activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs) model.summary() <pre>Model: \"functional_1\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer_1 (InputLayer)      \u2502 (None, 128)            \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 embedding_1 (Embedding)         \u2502 (None, 128, 16)        \u2502       320,016 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bidirectional_1 (Bidirectional) \u2502 (None, 128, 64)        \u2502        12,544 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatten_1 (Flatten)             \u2502 (None, 8192)           \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dropout (Dropout)               \u2502 (None, 8192)           \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 1)              \u2502         8,193 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 340,753 (1.30 MB)\n</pre> <pre> Trainable params: 340,753 (1.30 MB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[17]: Copied! <pre>X_train_seq = text_to_seq(train_df['text'].map(lambda x: x.lower()), max_len=max_len)\nX_test_seq = text_to_seq(test_df['text'].map(lambda x: x.lower()), max_len=max_len)\n\ny_train_lstm = train_df['label'].values\ny_test_lstm  = test_df['label'].values\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=['accuracy', 'recall', 'precision']\n)\n\n\nhistory = model.fit(\n    X_train_seq, y_train_lstm,\n    validation_split=0.2,\n    epochs=20,\n    batch_size=128,\n    verbose=1,\n    callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n    )\n</pre> X_train_seq = text_to_seq(train_df['text'].map(lambda x: x.lower()), max_len=max_len) X_test_seq = text_to_seq(test_df['text'].map(lambda x: x.lower()), max_len=max_len)  y_train_lstm = train_df['label'].values y_test_lstm  = test_df['label'].values  optimizer = keras.optimizers.Adam(learning_rate=0.001)  model.compile(     optimizer=optimizer,     loss=keras.losses.BinaryCrossentropy(from_logits=False),     metrics=['accuracy', 'recall', 'precision'] )   history = model.fit(     X_train_seq, y_train_lstm,     validation_split=0.2,     epochs=20,     batch_size=128,     verbose=1,     callbacks=[EarlyStopping(monitor='val_loss', patience=3)]     ) <pre>Epoch 1/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11s 59ms/step - accuracy: 0.5379 - loss: 1.2022 - precision: 0.5372 - recall: 0.5748 - val_accuracy: 0.4914 - val_loss: 0.7867 - val_precision: 1.0000 - val_recall: 0.0012\nEpoch 2/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 77ms/step - accuracy: 0.7081 - loss: 0.6530 - precision: 0.7136 - recall: 0.6873 - val_accuracy: 0.7082 - val_loss: 0.5926 - val_precision: 0.9338 - val_recall: 0.4595\nEpoch 3/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 80ms/step - accuracy: 0.8634 - loss: 0.4473 - precision: 0.8639 - recall: 0.8596 - val_accuracy: 0.7942 - val_loss: 0.5179 - val_precision: 0.7234 - val_recall: 0.9647\nEpoch 4/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 82ms/step - accuracy: 0.8992 - loss: 0.3468 - precision: 0.8948 - recall: 0.9029 - val_accuracy: 0.8278 - val_loss: 0.4581 - val_precision: 0.7774 - val_recall: 0.9273\nEpoch 5/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 83ms/step - accuracy: 0.9214 - loss: 0.2873 - precision: 0.9197 - recall: 0.9233 - val_accuracy: 0.6434 - val_loss: 0.8884 - val_precision: 0.9727 - val_recall: 0.3083\nEpoch 6/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 82ms/step - accuracy: 0.9262 - loss: 0.2653 - precision: 0.9313 - recall: 0.9200 - val_accuracy: 0.8382 - val_loss: 0.4100 - val_precision: 0.8694 - val_recall: 0.8028\nEpoch 7/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 80ms/step - accuracy: 0.9495 - loss: 0.2212 - precision: 0.9520 - recall: 0.9454 - val_accuracy: 0.7612 - val_loss: 0.6576 - val_precision: 0.9395 - val_recall: 0.5676\nEpoch 8/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 80ms/step - accuracy: 0.9578 - loss: 0.1923 - precision: 0.9584 - recall: 0.9566 - val_accuracy: 0.7886 - val_loss: 0.5987 - val_precision: 0.7179 - val_recall: 0.9635\nEpoch 9/20\n157/157 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 81ms/step - accuracy: 0.9607 - loss: 0.1848 - precision: 0.9594 - recall: 0.9615 - val_accuracy: 0.8402 - val_loss: 0.4436 - val_precision: 0.8390 - val_recall: 0.8492\n</pre> In\u00a0[18]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title('LSTM Loss')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.legend()\nplt.title('LSTM Accuracy')\nplt.show()\n</pre> plt.figure(figsize=(12,4)) plt.subplot(1,2,1) plt.plot(history.history['loss'], label='Train Loss') plt.plot(history.history['val_loss'], label='Val Loss') plt.legend() plt.title('LSTM Loss')  plt.subplot(1,2,2) plt.plot(history.history['accuracy'], label='Train Acc') plt.plot(history.history['val_accuracy'], label='Val Acc') plt.legend() plt.title('LSTM Accuracy') plt.show() <p>And yes we see that the validation loss has decreased more. But less than the training loss and the loss is higher than the one without regularization. It shows how complex it is to find the best hyperparameters for neural networks.</p> In\u00a0[19]: Copied! <pre>y_pred_lstm_prob = model.predict(X_test_seq)\ny_pred_lstm = (y_pred_lstm_prob&gt;0.5).astype(int)\nprint(\"LSTM Classification Report:\")\nprint(classification_report(y_test_lstm, y_pred_lstm))\n</pre> y_pred_lstm_prob = model.predict(X_test_seq) y_pred_lstm = (y_pred_lstm_prob&gt;0.5).astype(int) print(\"LSTM Classification Report:\") print(classification_report(y_test_lstm, y_pred_lstm)) <pre>782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 9ms/step\nLSTM Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.77      0.81      0.79     12500\n           1       0.80      0.77      0.78     12500\n\n    accuracy                           0.79     25000\n   macro avg       0.79      0.79      0.79     25000\nweighted avg       0.79      0.79      0.79     25000\n\n</pre> <p>The accuracy has decreased compared to the one without regularization. It shows the complexity of how to find the best hyperparameters and the best way to tweak neural networks.</p> <p>By adding the regularization and the dropout we aimed to limit overfitting, but we see that it has not been so effective on the test set. Maybe adding more epochs or increasing the embedding dimension could help. Those would be things to try.</p> <p>Also, the embedding matrix is initialized randomly. So it could be that the embedding matrix is not optimal for the task. We could try to use a pretrained embedding matrix (like GloVe or Word2Vec) or pre-train it ourselves on domain-specific data. This could inject more semantic understanding into the model and help the LSTM make better predictions.</p> <p>If we want to find the best hyperparameters for the LSTM model we can use the <code>keras_tuner</code> library.</p> <p>We define the model-building function with hyperparameters and then use the <code>Hyperband</code> tuner to find the best hyperparameters.</p> <p>We then use the <code>get_best_hyperparameters</code> function to get the best hyperparameters and then use the <code>build_model</code> function to build the model with the best hyperparameters.</p> <p>We use the callback <code>EarlyStopping</code> to stop the training if the validation loss does not improve for 3 epochs. But even with this it will take around 1h to find the best hyperparameters.</p> In\u00a0[25]: Copied! <pre>import keras_tuner as kt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Define the model-building function with hyperparameters\ndef build_model(hp):\n    embedding_dim = hp.Int('embedding_dim', min_value=16, max_value=64, step=16)\n    lstm_units = hp.Int('lstm_units', min_value=16, max_value=64, step=16)\n    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, step=2e-4)\n    hp.Choice(\"batch_size\", values=[32, 64, 128])\n    \n    inputs = keras.Input(shape=(max_len,), dtype=\"int32\")\n    x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n    x = layers.Flatten()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    model = keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\", \"recall\", \"precision\"]\n    )\n    return model\n\n# Initialize the tuner\ntuner = kt.Hyperband(\n    build_model,\n    objective='val_loss',\n    max_epochs=10,\n    factor=3,\n    directory='keras_tuner',\n    project_name='lstm_classifier',\n    hyperparameters=None,\n    tune_new_entries=True,\n    allow_new_entries=True\n)\n\n# Display search space summary\ntuner.search_space_summary()\n\ndef run_trial(hp, tuner, x, y, **kwargs):\n    # Build the model\n    model = tuner.hypermodel.build(hp)\n    \n    # Get batch size from hyperparameters\n    batch_size = hp.get('batch_size')\n    \n    # Run model with the chosen batch size\n    history = model.fit(\n        x, y,\n        batch_size=batch_size,\n        **kwargs\n    )\n    return history\n\n# Run the search using the run_trial function\nbatch_sizes = []\nfor trial in tuner.oracle.get_best_trials(10):\n    hp = trial.hyperparameters\n    batch_sizes.append(hp.get('batch_size'))\n    \n# Use tuner directly with custom execution\ntuner.search(\n    X_train_seq, y_train_lstm,\n    validation_split=0.2,\n    epochs=10,\n    callbacks=[keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')],\n    batch_size=32  # This will be overridden\n)\n\n# Get the best model\nbest_model = tuner.get_best_models(num_models=1)[0]\nbest_model.summary()\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Best embedding dimension: {best_hps.get('embedding_dim')}\")\nprint(f\"Best LSTM units: {best_hps.get('lstm_units')}\")\nprint(f\"Best dropout rate: {best_hps.get('dropout_rate')}\")\nprint(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n</pre> import keras_tuner as kt from tensorflow import keras from tensorflow.keras import layers  # Define the model-building function with hyperparameters def build_model(hp):     embedding_dim = hp.Int('embedding_dim', min_value=16, max_value=64, step=16)     lstm_units = hp.Int('lstm_units', min_value=16, max_value=64, step=16)     dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)     learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, step=2e-4)     hp.Choice(\"batch_size\", values=[32, 64, 128])          inputs = keras.Input(shape=(max_len,), dtype=\"int32\")     x = layers.Embedding(vocab_size, embedding_dim)(inputs)     x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)     x = layers.Flatten()(x)     x = layers.Dropout(dropout_rate)(x)     outputs = layers.Dense(1, activation=\"sigmoid\")(x)          model = keras.Model(inputs, outputs)     model.compile(         optimizer=keras.optimizers.Adam(learning_rate=learning_rate),         loss=\"binary_crossentropy\",         metrics=[\"accuracy\", \"recall\", \"precision\"]     )     return model  # Initialize the tuner tuner = kt.Hyperband(     build_model,     objective='val_loss',     max_epochs=10,     factor=3,     directory='keras_tuner',     project_name='lstm_classifier',     hyperparameters=None,     tune_new_entries=True,     allow_new_entries=True )  # Display search space summary tuner.search_space_summary()  def run_trial(hp, tuner, x, y, **kwargs):     # Build the model     model = tuner.hypermodel.build(hp)          # Get batch size from hyperparameters     batch_size = hp.get('batch_size')          # Run model with the chosen batch size     history = model.fit(         x, y,         batch_size=batch_size,         **kwargs     )     return history  # Run the search using the run_trial function batch_sizes = [] for trial in tuner.oracle.get_best_trials(10):     hp = trial.hyperparameters     batch_sizes.append(hp.get('batch_size'))      # Use tuner directly with custom execution tuner.search(     X_train_seq, y_train_lstm,     validation_split=0.2,     epochs=10,     callbacks=[keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')],     batch_size=32  # This will be overridden )  # Get the best model best_model = tuner.get_best_models(num_models=1)[0] best_model.summary()  # Get the optimal hyperparameters best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] print(f\"Best embedding dimension: {best_hps.get('embedding_dim')}\") print(f\"Best LSTM units: {best_hps.get('lstm_units')}\") print(f\"Best dropout rate: {best_hps.get('dropout_rate')}\") print(f\"Best learning rate: {best_hps.get('learning_rate')}\") <pre>Trial 30 Complete [00h 02m 11s]\nval_loss: 0.345576673746109\n\nBest val_loss So Far: 0.33851927518844604\nTotal elapsed time: 00h 49m 08s\n</pre> <pre>Model: \"functional\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 input_layer (InputLayer)        \u2502 (None, 128)            \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 embedding (Embedding)           \u2502 (None, 128, 48)        \u2502       960,048 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 bidirectional (Bidirectional)   \u2502 (None, 128, 96)        \u2502        37,248 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 flatten (Flatten)               \u2502 (None, 12288)          \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dropout (Dropout)               \u2502 (None, 12288)          \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense (Dense)                   \u2502 (None, 1)              \u2502        12,289 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 1,009,585 (3.85 MB)\n</pre> <pre> Trainable params: 1,009,585 (3.85 MB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> <pre>Best embedding dimension: 48\nBest LSTM units: 48\nBest dropout rate: 0.0\nBest learning rate: 0.0005\n</pre> In\u00a0[26]: Copied! <pre>y_pred_lstm = best_model.predict(X_test_seq)\ny_pred_lstm = (y_pred_lstm&gt;0.5).astype(int)\nprint(classification_report(y_test_lstm, y_pred_lstm))\n</pre> y_pred_lstm = best_model.predict(X_test_seq) y_pred_lstm = (y_pred_lstm&gt;0.5).astype(int) print(classification_report(y_test_lstm, y_pred_lstm))  <pre>782/782 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 12ms/step\n              precision    recall  f1-score   support\n\n           0       0.82      0.82      0.82     12500\n           1       0.82      0.82      0.82     12500\n\n    accuracy                           0.82     25000\n   macro avg       0.82      0.82      0.82     25000\nweighted avg       0.82      0.82      0.82     25000\n\n</pre> In\u00a0[27]: Copied! <pre># Let's just do a quick note on each model's accuracy for a simple numeric comparison.\n\nacc_logreg = (y_pred_logreg==y_test).mean()\nacc_lstm   = (y_pred_lstm.reshape(-1)==y_test_lstm).mean()\nprint(f\"Logistic Regression Test Accuracy: {acc_logreg:.3f}\")\nprint(f\"LSTM Test Accuracy: {acc_lstm:.3f}\")\n</pre> # Let's just do a quick note on each model's accuracy for a simple numeric comparison.  acc_logreg = (y_pred_logreg==y_test).mean() acc_lstm   = (y_pred_lstm.reshape(-1)==y_test_lstm).mean() print(f\"Logistic Regression Test Accuracy: {acc_logreg:.3f}\") print(f\"LSTM Test Accuracy: {acc_lstm:.3f}\") <pre>Logistic Regression Test Accuracy: 0.879\nLSTM Test Accuracy: 0.825\n</pre> <p>So basically, with hyperparameter tuning we can get a better accuracy, we gained 1.5pt of accuracy. But we are still much lower than the Logistic Regression with TF-IDF that we did not even try to tune. It shows the complexity of the LSTM model.</p> <p>The most likely reason lies in the embeddings which are initialized randomly. So it must be that the embeddings are not optimal for the task. We could try to use a pretrained embedding matrix (like GloVe or Word2Vec) or pre-train it ourselves on domain-specific data. This could inject more semantic understanding into the model and help the LSTM make better predictions. We will see this in next session.</p> In\u00a0[37]: Copied! <pre>import numpy as np\nfrom IPython.display import HTML, display\n\ndef get_word_importance_leave_one_out(model, input_text, tokenizer):\n    \"\"\"\n    Calculate word importance by leaving each word out and measuring prediction change.\n    A more reliable approach when gradient methods fail.\n    \"\"\"\n    # Tokenize the input text\n    tokens = tokenizer.texts_to_sequences([input_text])[0]\n    sequence = np.array(tokens)\n    \n    # Create padded sequence for model input\n    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=max_len)\n    \n    # Get baseline prediction for full text\n    baseline_prediction = model.predict(padded_sequence, verbose=0)[0][0]\n    \n    # Calculate impact of each word by removing it\n    word_importance = []\n    for i in range(len(sequence)):\n        # Create a modified sequence with word i removed\n        modified_sequence = sequence.copy()\n        modified_sequence = np.delete(modified_sequence, i)\n        \n        # Pad the modified sequence\n        padded_modified = tf.keras.preprocessing.sequence.pad_sequences([modified_sequence], maxlen=max_len)\n        \n        # Get prediction without this word\n        modified_prediction = model.predict(padded_modified, verbose=0)[0][0]\n        \n        # Calculate importance (positive = removing word decreases sentiment)\n        importance = baseline_prediction - modified_prediction\n        word_importance.append(importance)\n    \n    # Convert token IDs back to words\n    idx2word = {v: k for k, v in tokenizer.word_index.items()}\n    words = [idx2word.get(token_id, '[UNK]') for token_id in sequence]\n    \n    return words, np.array(word_importance)\n\ndef visualize_word_importance_html(words, word_importance, threshold=0.01):\n    \"\"\"\n    Visualize word importance using HTML with colored backgrounds.\n    - Green: Positive sentiment contribution\n    - Red: Negative sentiment contribution\n    \"\"\"\n    # Normalize importance to range -1 to 1 if values are significant\n    max_importance = max(abs(np.max(word_importance)), abs(np.min(word_importance)))\n    if max_importance &gt; threshold:\n        normalized_importance = word_importance / max_importance\n    else:\n        normalized_importance = word_importance\n    \n    # Start HTML\n    html = '&lt;div style=\"font-size: 16px; line-height: 2; font-family: Arial, sans-serif; padding: 10px;\"&gt;'\n    \n    # Show the predicted sentiment\n    # Create a span for each word with color intensity based on importance\n    for word, importance in zip(words, normalized_importance):\n        # Calculate color intensity (0-1)\n        intensity = min(abs(importance), 1.0)\n        \n        # Skip very low importance words (show as neutral)\n        if abs(importance) &lt; threshold:\n            html += f'&lt;span style=\"padding: 3px; margin: 2px; border-radius: 3px;\"&gt;{word}&lt;/span&gt; '\n            continue\n            \n        # Determine color (red for negative, green for positive)\n        if importance &gt; 0:\n            # Positive sentiment (green)\n            color = f'rgba(0, 128, 0, {intensity})'\n        else:\n            # Negative sentiment (red)\n            color = f'rgba(255, 0, 0, {intensity})'\n            \n        # Create span with background color\n        html += f'&lt;span style=\"background-color: {color}; padding: 3px; margin: 2px; border-radius: 3px;\"&gt;{word}&lt;/span&gt; '\n    \n    # Close HTML div\n    html += '&lt;/div&gt;'\n    \n    # Display HTML\n    display(HTML(html))\n\n# Example usage:\nreview = \"The movie was terrible but the actors did a great job\"\nwords, importances = get_word_importance_leave_one_out(best_model, review, tokenizer)\nprint(f\"Word importances: {importances}\")\nvisualize_word_importance_html(words, importances)\n\n# Try another example\nreview = \"I absolutely loved this film, it was amazing from start to finish\"\nwords, importances = get_word_importance_leave_one_out(best_model, review, tokenizer)\nvisualize_word_importance_html(words, importances)\n\n# Try a negative example\nreview = \"This was the worst movie I have ever seen, complete waste of time\"\nwords, importances = get_word_importance_leave_one_out(best_model, review, tokenizer)\nvisualize_word_importance_html(words, importances)\n</pre> import numpy as np from IPython.display import HTML, display  def get_word_importance_leave_one_out(model, input_text, tokenizer):     \"\"\"     Calculate word importance by leaving each word out and measuring prediction change.     A more reliable approach when gradient methods fail.     \"\"\"     # Tokenize the input text     tokens = tokenizer.texts_to_sequences([input_text])[0]     sequence = np.array(tokens)          # Create padded sequence for model input     padded_sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=max_len)          # Get baseline prediction for full text     baseline_prediction = model.predict(padded_sequence, verbose=0)[0][0]          # Calculate impact of each word by removing it     word_importance = []     for i in range(len(sequence)):         # Create a modified sequence with word i removed         modified_sequence = sequence.copy()         modified_sequence = np.delete(modified_sequence, i)                  # Pad the modified sequence         padded_modified = tf.keras.preprocessing.sequence.pad_sequences([modified_sequence], maxlen=max_len)                  # Get prediction without this word         modified_prediction = model.predict(padded_modified, verbose=0)[0][0]                  # Calculate importance (positive = removing word decreases sentiment)         importance = baseline_prediction - modified_prediction         word_importance.append(importance)          # Convert token IDs back to words     idx2word = {v: k for k, v in tokenizer.word_index.items()}     words = [idx2word.get(token_id, '[UNK]') for token_id in sequence]          return words, np.array(word_importance)  def visualize_word_importance_html(words, word_importance, threshold=0.01):     \"\"\"     Visualize word importance using HTML with colored backgrounds.     - Green: Positive sentiment contribution     - Red: Negative sentiment contribution     \"\"\"     # Normalize importance to range -1 to 1 if values are significant     max_importance = max(abs(np.max(word_importance)), abs(np.min(word_importance)))     if max_importance &gt; threshold:         normalized_importance = word_importance / max_importance     else:         normalized_importance = word_importance          # Start HTML     html = ''          # Show the predicted sentiment     # Create a span for each word with color intensity based on importance     for word, importance in zip(words, normalized_importance):         # Calculate color intensity (0-1)         intensity = min(abs(importance), 1.0)                  # Skip very low importance words (show as neutral)         if abs(importance) &lt; threshold:             html += f'{word} '             continue                      # Determine color (red for negative, green for positive)         if importance &gt; 0:             # Positive sentiment (green)             color = f'rgba(0, 128, 0, {intensity})'         else:             # Negative sentiment (red)             color = f'rgba(255, 0, 0, {intensity})'                      # Create span with background color         html += f'{word} '          # Close HTML div     html += ''          # Display HTML     display(HTML(html))  # Example usage: review = \"The movie was terrible but the actors did a great job\" words, importances = get_word_importance_leave_one_out(best_model, review, tokenizer) print(f\"Word importances: {importances}\") visualize_word_importance_html(words, importances)  # Try another example review = \"I absolutely loved this film, it was amazing from start to finish\" words, importances = get_word_importance_leave_one_out(best_model, review, tokenizer) visualize_word_importance_html(words, importances)  # Try a negative example review = \"This was the worst movie I have ever seen, complete waste of time\" words, importances = get_word_importance_leave_one_out(best_model, review, tokenizer) visualize_word_importance_html(words, importances) <pre>Word importances: [-0.00511168 -0.01023081 -0.00419638 -0.05363719  0.00032213  0.00032213\n -0.00724988 -0.01255389  0.00083612  0.04623024  0.02968016]\n</pre> movie terrible actors did great job absolutely loved film amazing start finish worst movie seen complete waste time"},{"location":"chapter2/Session_2_3_LSTM_Classif/#lstm-text-classification-vs-tf-idf-logistic-regression","title":"LSTM Text Classification vs. TF-IDF + Logistic Regression\u00b6","text":"<p>In this notebook, we compare two different approaches to text classification:</p> <ol> <li>Logistic Regression on top of TF-IDF features</li> <li>LSTM (using Keras) trained end-to-end</li> </ol> <p>We'll explore:</p> <ul> <li>The architectures for each approach</li> <li>Their respective training processes</li> <li>Visualization of results and metrics</li> <li>How each model interprets words in the final decision</li> <li>Potential improvements for the LSTM model</li> </ul>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#1-environment-setup-imports","title":"1. Environment Setup &amp; Imports\u00b6","text":"<p>We'll install any dependencies (if needed) and import the necessary libraries.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#2-data-loading-preprocessing","title":"2. Data Loading &amp; Preprocessing\u00b6","text":"<p>For demonstration, we'll assume we have a text classification dataset named <code>df</code>. It contains two columns:</p> <ul> <li><code>text</code> : The text field</li> <li><code>label</code>: The target label (0 or 1, for binary classification, or more classes)</li> </ul> <p>We'll split the data into train and test sets, then further create a small dev set if needed.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#3-tf-idf-logistic-regression","title":"3. TF-IDF + Logistic Regression\u00b6","text":"<p>We'll:</p> <ol> <li>Vectorize our text using TF-IDF.</li> <li>Train a Logistic Regression model.</li> <li>Evaluate on the test set.</li> <li>Visualize results.</li> </ol>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#visualization-of-confusion-matrix","title":"Visualization of Confusion Matrix\u00b6","text":""},{"location":"chapter2/Session_2_3_LSTM_Classif/#4-lstm-classification","title":"4: LSTM Classification\u00b6","text":"<p>In this part, we'll train a Recurrent Neural Network (RNN) using an LSTM (Long Short-Term Memory) layer \u2014 a powerful deep learning technique especially designed for sequential data like text.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#why-not-use-tf-idf-for-lstms","title":"\ud83e\udd14 Why Not Use TF-IDF for LSTMs?\u00b6","text":"<p>TF-IDF transforms text into fixed-length sparse vectors, measuring the importance of each word. However, it ignores word order, which is crucial for understanding meaning in context.</p> <p>For example:</p> <ul> <li><code>\"the cat sat on the mat\"</code> and <code>\"mat sat the on cat the\"</code> will result in the same TF-IDF vector.</li> </ul> <p>This is a problem for models like LSTMs that are built to:</p> <ul> <li>Process sequences of words, not unordered bags.</li> <li>Learn how word meanings evolve across time.</li> <li>Use positional and contextual dependencies to understand sentences.</li> </ul>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#what-do-lstms-expect-as-input","title":"\u2705 What Do LSTMs Expect as Input?\u00b6","text":"<p>Instead of a TF-IDF vector, LSTMs take in:</p> <ul> <li>A sequence of word IDs (tokenized text),</li> <li>Which are then mapped to dense vectors (embeddings),</li> <li>And passed through the LSTM layer, which learns from the word order and meaning.</li> </ul>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#41-preprocessing-the-text-for-lstm","title":"4.1 Preprocessing the Text for LSTM\u00b6","text":""},{"location":"chapter2/Session_2_3_LSTM_Classif/#step-1-tokenize-text-using-tokenizer","title":"Step 1\ufe0f\u20e3 \u2014 Tokenize Text using <code>Tokenizer</code>\u00b6","text":"<ul> <li>The <code>Tokenizer</code> assigns a unique ID to each word in the training data.</li> <li>We use:<ul> <li><code>num_words=vocab_size</code> to limit to top N most frequent words.</li> <li><code>oov_token=\"&lt;OOV&gt;\"</code> to represent unknown or rare words.</li> <li><code>fit_on_texts(...)</code> to build the internal word-to-index dictionary.</li> </ul> </li> </ul>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#step-2-convert-sentences-to-sequences","title":"Step 2\ufe0f\u20e3 \u2014 Convert Sentences to Sequences\u00b6","text":"<p>Each sentence is turned into a sequence of word IDs:</p> <p><code>\"There is no relation at all between Fortier...\"</code> becomes <code>[48, 7, 55, 1, 31, 30, 198, 1, ...]</code></p> <ul> <li><code>&lt;OOV&gt;</code> is inserted for words not in the vocabulary.</li> <li>These sequences now preserve the order of words, which is key for LSTM models.</li> </ul>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#step-3-pad-sequences-to-equal-length","title":"Step 3\ufe0f\u20e3 \u2014 Pad Sequences to Equal Length\u00b6","text":"<p>Neural networks require inputs of consistent size. But:</p> <ul> <li>Some sentences are short, others are long.</li> <li>So we truncate longer ones and pad shorter ones to a fixed length.</li> </ul> <p>We use:</p> <ul> <li><code>max_len = 64</code></li> <li><code>pad_sequences(..., padding='post', truncating='post')</code> to ensure all sequences are the same length, with padding (0s) added at the end.</li> </ul>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#step-4-split-into-x-and-y","title":"Step 4\ufe0f\u20e3 \u2014 Split into <code>X</code> and <code>y</code>\u00b6","text":"<p>At this point, we have:</p> <ul> <li><code>X_train_seq</code>: Padded sequences of token IDs.</li> <li><code>y_train_lstm</code>: Corresponding binary labels (0 or 1).</li> </ul> <p>The same applies for the test set: <code>X_test_seq</code>, <code>y_test_lstm</code>.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#note-on-using-the-same-vocabulary-for-lstm-and-tf-idf","title":"\u270f\ufe0f Note on Using the Same Vocabulary for LSTM and TF-IDF\u00b6","text":"<p>To ensure a fair comparison between the Logistic Regression (TF-IDF) and LSTM models:</p> <ul> <li>We use the same vocabulary from the TF-IDF vectorizer.</li> <li>This means we ignore stopwords and low-frequency words consistently.</li> <li>The only addition is the <code>&lt;OOV&gt;</code> token used by LSTM to handle unknowns.</li> </ul>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#embeddings-the-input-of-the-lstm","title":"Embeddings: the input of the LSTM\u00b6","text":"<p>The LSTM (or RNNs) is fed with the tokenized sequences of the reviews. Nevertheless, if we feed the RNN with the raw token IDs (represented as integers above) the RNN will be just get as inputs the positions of the words in the vocabulary nothing more. To gain more information we use the embeddings.</p> <p>Let's first link embeddings to the TF-IDF pipeline.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#embeddings-in-the-tf-idf-pipeline","title":"\ud83e\udde0 Embeddings in the TF-IDF Pipeline\u00b6","text":"<p>In the TF-IDF + logistic regression model, the model learned the embeddings implicitly from the TF-IDF vectorizer. The embeddings are learned when we fit the vectorizer to the training data. In the end we associate, for each sentence, each word to a weight and the sentence is represented as a vector of the weights. We say that we have a \"sentence embedding\".</p> <p>In a TF-IDF + Logistic Regression model:</p> <ul> <li>Each word is assigned a single numerical weight.</li> <li>This weight reflects how important the word is in the review.</li> <li>The model uses this fixed value as the representation of the word.</li> <li>The logistic regression model uses the sentence embedding to make the prediction.</li> </ul> <p>See below:</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#embeddings-in-the-lstm-pipeline","title":"\ud83e\udde0 Embeddings in the LSTM Pipeline\u00b6","text":"<p>As we said, the LSTM could be fed with the tokenized sequences of the reviews: their positions in the vocabulary. To gain word importance and semantic we can add a layer between the tokenized sequences and the LSTM cells. This layer is called the embedding layer.</p> <p>This embedding layer is the matrix of weights. In the case of the TF-IDF pipeline it would be the matrix of the weights of the words in the vocabulary. A matrix with one column and as many rows as the number of words in the vocabulary.</p> <p>As we said, one value is quite limited to capture the semantic meaning of a word. So we use a dense vector to represent a word instead of a single number.</p> <p>Therefore, in an LSTM pipeline:</p> <ul> <li>We tokenize the sentences and have a sequence of token IDs which represent the words in the sentences.</li> <li>Each word is mapped to a dense vector (e.g. <code>[0.21, -0.67, 0.04, ...]</code>) instead of a single number.</li> <li>We feed the LSTM with the sequence of dense vectors instead of the token IDs.</li> </ul> <p>Therefore when we set up the LSTM pipeline we need to set up the embedding layer at first. We can see the embedding layer as a matrix of weights for the whole vocabulary. And one vector is associated to one word. This vector will be the word embedding said differently the vector carries the semantic meaning of the word.</p> <p>We will dive more into embeddings in the next session.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#more-concretely","title":"More concretely\u00b6","text":"<p>In the LSTM pipeline, we add a layer, called the embedding layer, which turns each token ID into a dense vector \u2014 like turning <code>actor</code> into <code>[0.21, -0.67, 0.04, ...]</code>. This dense vector for <code>actor</code>is part of the vocabulary matrix.</p> <p>Each word of the sequence is mapped to a dense vector, we feed the LSTM with the sequence and then we add classifier, concretely a dense layer with a sigmoid activation function.</p> <p>When we train the whole pipeline, we will train also the embedding layer.</p> <p>\ud83d\udccc Without embeddings, LSTM would be just memorizing token positions \u2014 embeddings help it understand meaning of the words.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#summary","title":"\u2705 Summary\u00b6","text":"Feature TF-IDF + Logistic Regression Tokenized Sequence + LSTM Input Format Sparse fixed-length vector Sequence of token IDs Captures Word Order? \u274c No \u2705 Yes Learns Contextual Meaning \u274c No \u2705 Yes (via Embeddings) Model Complexity Simpler, interpretable More expressive, less interpretable Good For Quick baselines Sequence modeling, complex patterns <p>Using embedding + LSTM lets us build context-aware models that understand both word meanings and their positions, which is essential for tasks like sentiment analysis, text generation, and sequence classification.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#42-build-the-lstm-model","title":"4.2 Build the LSTM Model\u00b6","text":"<p>Now that we have our sequences ready and padded, we can build an LSTM-based neural network using Keras.</p> <p>We\u2019ll use a simple architecture:</p> <ul> <li>An Embedding layer to learn word representations.</li> <li>A single LSTM layer to process the sequence.</li> <li>A Dense layer with a sigmoid activation to output binary predictions.</li> </ul> <p>Let\u2019s break this down:</p> <ul> <li><code>Embedding(input_dim, output_dim)</code>: This layer takes each word ID and maps it to a dense vector. Here, <code>input_dim</code> is the vocabulary size, and <code>output_dim</code> is the size of the embedding (we use 16).</li> <li><code>LSTM(units)</code>: Processes the sequence of embeddings, capturing patterns and dependencies across time.</li> <li><code>Bidirectional(LSTM(units))</code>: This layer processes the sequence in both directions (forward and backward), capturing patterns and dependencies across time.</li> <li><code>Flatten()</code>: This layer flattens the output of the LTSM layer into a 1D vector. As we asked the LSTM to return the sequences we need to flatten the output to have a 1D vector and have the information of the whole sequence.</li> <li><code>Dense(1, activation='sigmoid')</code>: Outputs a single probability for binary classification (positive or negative label).</li> </ul> <p>The optimizer is Adam with a learning rate of 0.01.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#43-train-the-lstm-model","title":"4.3 Train the LSTM Model\u00b6","text":"<p>Now we train the model using our preprocessed sequences and binary labels.</p> <p>We use:</p> <ul> <li><code>validation_split=0.2</code>: to hold out 20% of training data for validation.</li> <li><code>epochs=10</code>: to iterate 10 times over the dataset.</li> <li><code>batch_size=128</code>: A high batch size to speed up training and also to reduce overfitting.</li> </ul> <p>Note that LSTMs are more computationally expensive than logistic regression, so training can take longer \u2014 especially with small batches.</p> <p>To control the overfitting we can use the callback <code>EarlyStopping</code> to stop the training if the validation loss does not improve for 3 epochs.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#44-training-curves","title":"4.4 Training Curves\u00b6","text":"<p>Let's visualize the training and validation accuracy/loss.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#regularization","title":"\u2696\ufe0f Regularization\u00b6","text":"<p>Regularization is a technique used to prevent overfitting. It adds a penalty term to the loss function that encourages the model to have smaller weights. This helps prevent the model from fitting the noise in the training data.</p> <p>We can use L2 regularization or L1 regularization.</p> <p>Concretely we can add the regularization to the LSTM layer or the dense layer with <code>activity_regularizer=keras.regularizers.l2(0.01)</code> or <code>activity_regularizer=keras.regularizers.l1(0.01)</code>.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#dropout","title":"\ud83d\udd73\ufe0f Dropout\u00b6","text":"<p>Dropout is a technique used to prevent overfitting. It randomly drops out some neurons in the network during training. This helps prevent the model from fitting the noise in the training data.</p> <p>We can use dropout in the LSTM layer or in the dense layer. Concretely we can add the dropout to the LSTM layer or the dense layer with <code>dropout=0.15</code>.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#45-evaluate-on-test-data","title":"4.5 Evaluate on Test Data\u00b6","text":""},{"location":"chapter2/Session_2_3_LSTM_Classif/#how-to-interpret-changes-in-hyperparameters","title":"\ud83d\udd27 How to Interpret Changes in Hyperparameters\u00b6","text":"<p>When tuning a neural network like LSTM, understanding how each parameter affects the model is key. Here's a summary of what typically happens when you increase or decrease each of the main hyperparameters:</p> Hyperparameter If You Increase It If You Decrease It Batch Size Faster training, but less frequent updates (less noise). Can converge faster but may get stuck in local minima. More frequent updates (more noise), can explore better minima but training is slower and noisier. Embedding Dimension More capacity to represent semantic meaning. Better for complex tasks. Higher risk of overfitting. Less capacity to capture word meaning. May underfit. LSTM Layers Can model more complex patterns. Better for long-term dependencies. Slower training, more prone to overfitting. Less expressive. May underfit complex data. Learning Rate Faster convergence but may overshoot or diverge. Unstable training. Slower, more stable training. May get stuck or take too long to converge. Epochs More training time. Can help model generalize better. Risk of overfitting if too many. Undertraining \u2014 model might not learn enough patterns. Dropout Reduces overfitting. Helps generalization. If too high, model struggles to learn. Model might overfit. Better training accuracy but worse generalization. Regularization Coefficient (L2) Penalizes large weights. Encourages simpler models that generalize better. Less penalty on large weights. Can lead to overfitting if too low."},{"location":"chapter2/Session_2_3_LSTM_Classif/#takeaway","title":"\ud83e\uddea Takeaway\u00b6","text":"<p>Finding the best set of hyperparameters is a balancing act:</p> <ul> <li>Too simple? The model underfits \u2014 it doesn't capture important patterns.</li> <li>Too complex? The model overfits \u2014 it memorizes the training data but fails to generalize.</li> </ul> <p>Always validate your model on a development/test set, and use learning curves (loss/accuracy over time) to help diagnose underfitting vs overfitting.</p> <p>This kind of experimentation and interpretation is what makes deep learning a blend of science and art \ud83c\udfa8\ud83d\udd2c</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#5-word-level-interpretation-in-rnns","title":"5. Word-Level Interpretation in RNNs\u00b6","text":""},{"location":"chapter2/Session_2_3_LSTM_Classif/#51-leave-one-out-attribution-for-rnns","title":"5.1 Leave-One-Out Attribution for RNNs\u00b6","text":"<p>Unlike simpler models like TF-IDF + logistic regression where each word has a fixed coefficient/weight that directly indicates its contribution to the prediction, interpreting word importance in RNNs is more challenging for several reasons:</p> <ol> <li>Sequential processing: RNNs process words sequentially, so a word's impact depends on all preceding words.</li> <li>Hidden state complexity: The influence of each word is captured in the hidden state, which evolves throughout the sequence.</li> <li>Non-linearity: Multiple non-linear transformations make it difficult to trace a word's direct impact on the final prediction.</li> <li>Bidirectional influence: In bidirectional LSTMs, a word's importance depends on both preceding and following context.</li> </ol> <p>To address this challenge, we implemented a \"leave-one-out\" approach that:</p> <ol> <li>Takes a complete sentence and obtains its sentiment prediction</li> <li>Systematically removes each word one at a time</li> <li>Observes how the prediction changes without that word</li> <li>Words that cause large changes when removed are considered more influential</li> </ol> <p>Interpretation:</p> <ul> <li>Words highlighted in green contribute positively to sentiment (removing them decreases the positive sentiment score)</li> <li>Words highlighted in red contribute negatively (removing them increases the positive sentiment score)</li> <li>The intensity of the color indicates the magnitude of impact</li> </ul> <p>This method, while computationally more intensive than gradient-based approaches, provides more reliable interpretations for RNN architectures where gradient flow might be challenging to capture.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#52-gradient-based-attribution-for-rnns","title":"5.2 Gradient-Based Attribution for RNNs\u00b6","text":"<p>While the leave-one-out approach provides reliable interpretations, it is computationally expensive as it requires N+1 forward passes for a sentence of N words. Here we implement a more efficient gradient-based attribution method.</p>"},{"location":"chapter2/Session_2_3_LSTM_Classif/#how-input-gradient-attribution-works","title":"How Input \u00d7 Gradient Attribution Works:\u00b6","text":"<ol> <li>Forward Pass: Run the input text through the model to get embeddings and predictions.</li> <li>Gradient Calculation: Compute the gradient of the output (sentiment score) with respect to the word embeddings.</li> <li>Attribution: Multiply these gradients by the embedding values themselves.</li> <li>Aggregation: Sum across the embedding dimensions to get a single importance score per word.</li> </ol> <p>This approach is based on the concept that the gradient indicates how much a small change in the embedding would affect the output, and multiplying by the embedding values weights this by the actual magnitude of the input features.</p> <p>The method has several advantages:</p> <ul> <li>Efficiency: Requires only one forward and one backward pass</li> <li>Directness: Directly measures the sensitivity of the output to each word</li> <li>Captures Interactions: Accounts for how words interact with each other in the LSTM context</li> </ul> <p>However, gradient-based methods can sometimes face challenges with vanishing gradients or saturation in deep networks, which is why we implement a fallback to the leave-one-out method. It's much more complex to implement and we will try to do in another notebook.</p>"},{"location":"chapter3/","title":"Session 3: Word Embeddings","text":""},{"location":"chapter3/#course-materials","title":"\ud83c\udf93 Course Materials","text":""},{"location":"chapter3/#slides","title":"\ud83d\udcd1 Slides","text":"<p>Download Session 3 Slides (PDF)</p>"},{"location":"chapter3/#notebooks","title":"\ud83d\udcd3 Notebooks","text":"<ul> <li>Word2Vec from Scratch - with negative sampling</li> <li>Embedding Evaluation: Intrinsic and Extrinsic</li> <li>Classification with Embeddings</li> </ul>"},{"location":"chapter3/#session-3-word-embeddings","title":"\ud83d\ude80 Session 3: Word Embeddings","text":"<p>In this third session, we explore how words can be mathematically represented and why this is essential in any NLP pipeline. We trace the journey from traditional sparse one-hot encodings and TF-IDF vectors to powerful dense embeddings like Word2Vec and GloVe, and finally to context-aware models like ELMo and BERT.</p> <p>We also see how these embeddings are evaluated and how they can be applied to downstream NLP tasks like sentiment analysis, NER, or question answering.</p>"},{"location":"chapter3/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ol> <li>Understand the limitations of traditional word representations (e.g., sparsity, context insensitivity).</li> <li>Learn how dense vector embeddings solve these problems and how to train them.</li> <li>Explore Word2Vec architectures (Skip-gram and CBOW) and techniques like negative sampling.</li> <li>Evaluate embeddings both intrinsically (e.g., word similarity, analogy) and extrinsically (e.g., classification).</li> <li>Discover the next evolution: contextual embeddings with ELMo, including how to pretrain and fine-tune them.</li> </ol>"},{"location":"chapter3/#topics-covered","title":"\ud83d\udcda Topics Covered","text":""},{"location":"chapter3/#static-word-embeddings","title":"Static Word Embeddings","text":"<ul> <li>One-hot, TF-IDF: Why we moved beyond them.</li> <li>Word2Vec (Skip-gram, CBOW) and the training process.</li> <li>Negative Sampling: How to make training efficient.</li> <li>GloVe: A count-based alternative to Word2Vec.</li> <li>FastText: Subword-level embeddings to deal with rare words and misspellings.</li> </ul>"},{"location":"chapter3/#evaluating-word-embeddings","title":"Evaluating Word Embeddings","text":"<ul> <li>Intrinsic evaluations:</li> <li>Word similarity (e.g., cosine distance between \u201cking\u201d and \u201cqueen\u201d).</li> <li>Word analogy (\u201cman\u201d : \u201cwoman\u201d :: \u201cking\u201d : \u201cqueen\u201d).</li> <li>Extrinsic evaluations:</li> <li>How well embeddings help in downstream tasks like classification or POS tagging.</li> </ul>"},{"location":"chapter3/#contextual-word-embeddings","title":"Contextual Word Embeddings","text":"<ul> <li>Why static vectors fall short (e.g., \"bank\" in \u201criver bank\u201d vs. \u201cbank account\u201d).</li> <li>Introduction to ELMo (Peters et al., 2018).</li> <li>Bidirectional Language Modeling using LSTMs.</li> <li>How ELMo generates different embeddings for the same word in different contexts.</li> <li>Using ELMo for transfer learning in real-world NLP tasks (e.g., sentiment classification).</li> </ul>"},{"location":"chapter3/#key-takeaways","title":"\ud83e\udde0 Key Takeaways","text":"Aspect Static Embeddings Contextual Embeddings Meaning Based on Context? \u274c Same vector regardless \u2705 Different vectors per use Polysemy Handling \u274c No \u2705 Yes Requires Large Corpus? \u2705 Usually \u2705 Definitely Adaptable to Tasks? \u26a0\ufe0f Not easily \u2705 Via fine-tuning"},{"location":"chapter3/#bibliography-recommended-reading","title":"\ud83d\udcd6 Bibliography &amp; Recommended Reading","text":"<ul> <li> <p>Jay Alammar (2017): Visual Introduction to Word Embeddings \u2013 Blog Post   Excellent visuals to understand Word2Vec and GloVe.</p> </li> <li> <p>Sebastian Ruder (2017): On Word Embeddings \u2013 Part 2: Approximating Co-occurrence Matrices \u2013 Blog Post   Detailed breakdown of how different embedding models compare.</p> </li> <li> <p>Mikolov et al. (2013): Efficient Estimation of Word Representations in Vector Space \u2013 Paper   The original Word2Vec paper introducing Skip-gram and CBOW models.</p> </li> <li> <p>Pennington et al. (2014): GloVe: Global Vectors for Word Representation \u2013 Paper   Count-based embedding approach from Stanford NLP group.</p> </li> <li> <p>Joulin et al. (2016): Bag of Tricks for Efficient Text Classification (FastText) \u2013 Paper   A very practical take on embeddings using subword units.</p> </li> <li> <p>Peters et al. (2018): Deep Contextualized Word Representations \u2013 Paper   ELMo paper showing how dynamic embeddings outperform static ones on many tasks.</p> </li> </ul>"},{"location":"chapter3/#practical-components","title":"\ud83d\udcbb Practical Components","text":"<ul> <li>From Scratch Word2Vec: We walk through how Skip-Gram is trained using pairs of target/context words and how to integrate negative sampling.</li> <li>Embedding Visualizations: Use t-SNE or PCA to project high-dimensional embeddings and see how similar words cluster.</li> <li>Text Classification with Embeddings: Test embeddings in real classification tasks with logistic regression or LSTMs.</li> <li>Using Pretrained ELMo Embeddings: Fine-tune contextual embeddings on your own dataset.</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/","title":"Word2Vec Implementation from Scratch","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\nimport random\nfrom tqdm.notebook import tqdm\n\n# Download and load a text corpus (smaller dataset for faster processing)\n!wget -nc http://mattmahoney.net/dc/text8.zip\n!unzip -o text8.zip\n\n# Load the data\nwith open('text8', 'r') as f:\n    text = f.read()[:5000000]\n\nprint(f\"Total characters: {len(text)}\")\nprint(f\"First 500 characters: {text[:500]}\")\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from collections import Counter import re import random from tqdm.notebook import tqdm  # Download and load a text corpus (smaller dataset for faster processing) !wget -nc http://mattmahoney.net/dc/text8.zip !unzip -o text8.zip  # Load the data with open('text8', 'r') as f:     text = f.read()[:5000000]  print(f\"Total characters: {len(text)}\") print(f\"First 500 characters: {text[:500]}\") <pre>Fichier \u00ab\u00a0text8.zip\u00a0\u00bb d\u00e9j\u00e0 pr\u00e9sent\u00a0; pas de r\u00e9cup\u00e9ration.\n\nArchive:  text8.zip\n  inflating: text8                   \nTotal characters: 5000000\nFirst 500 characters:  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n</pre> In\u00a0[2]: Copied! <pre># Tokenize the text\nwords = text.split()\nprint(f\"Total words: {len(words)}\")\nprint(f\"First 20 words: {words[:20]}\")\n\n# Build vocabulary\nword_counts = Counter(words)\nprint(f\"Unique words: {len(word_counts)}\")\n\n# Plot word frequency distribution\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nword_freq = sorted(word_counts.values(), reverse=True)\nplt.plot(word_freq[:100])\nplt.title('Top 100 Word Frequencies')\nplt.xlabel('Word Rank')\nplt.ylabel('Frequency')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(word_freq)\nplt.title('All Word Frequencies (Log Scale)')\nplt.xlabel('Word Rank')\nplt.ylabel('Frequency')\nplt.yscale('log')\nplt.xscale('log')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</pre> # Tokenize the text words = text.split() print(f\"Total words: {len(words)}\") print(f\"First 20 words: {words[:20]}\")  # Build vocabulary word_counts = Counter(words) print(f\"Unique words: {len(word_counts)}\")  # Plot word frequency distribution plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) word_freq = sorted(word_counts.values(), reverse=True) plt.plot(word_freq[:100]) plt.title('Top 100 Word Frequencies') plt.xlabel('Word Rank') plt.ylabel('Frequency') plt.grid(True)  plt.subplot(1, 2, 2) plt.plot(word_freq) plt.title('All Word Frequencies (Log Scale)') plt.xlabel('Word Rank') plt.ylabel('Frequency') plt.yscale('log') plt.xscale('log') plt.grid(True) plt.tight_layout() plt.show() <pre>Total words: 846987\nFirst 20 words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\nUnique words: 47046\n</pre> In\u00a0[3]: Copied! <pre># Filter vocabulary based on word frequency\nmin_count = 5  # Minimum word frequency\nvocabulary = {word: count for word, count in word_counts.items() if count &gt;= min_count}\nprint(f\"Vocabulary size after filtering: {len(vocabulary)}\")\n\n# Analyze the coverage of our vocabulary\ntotal_word_count = len(words)\ncovered_word_count = sum(word_counts[word] for word in vocabulary)\ncoverage_percentage = (covered_word_count / total_word_count) * 100\n\nprint(f\"Original vocabulary size: {len(word_counts)}\")\nprint(f\"Filtered vocabulary size: {len(vocabulary)}\")\nprint(f\"Words retained: {covered_word_count:,} out of {total_word_count:,} ({coverage_percentage:.2f}%)\")\n\n# Plot the effect of different min_count thresholds\nthresholds = [1, 2, 5, 10, 20, 50, 100]\nvocab_sizes = []\ncoverage_percentages = []\n\nfor threshold in thresholds:\n    filtered_vocab = {word: count for word, count in word_counts.items() if count &gt;= threshold}\n    vocab_sizes.append(len(filtered_vocab))\n    covered = sum(word_counts[word] for word in filtered_vocab)\n    coverage_percentages.append((covered / total_word_count) * 100)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(thresholds, vocab_sizes, 'o-')\nplt.title('Vocabulary Size vs Min Count')\nplt.xlabel('Minimum Count Threshold')\nplt.ylabel('Vocabulary Size')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(thresholds, coverage_percentages, 'o-')\nplt.title('Text Coverage vs Min Count')\nplt.xlabel('Minimum Count Threshold')\nplt.ylabel('Coverage (%)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</pre> # Filter vocabulary based on word frequency min_count = 5  # Minimum word frequency vocabulary = {word: count for word, count in word_counts.items() if count &gt;= min_count} print(f\"Vocabulary size after filtering: {len(vocabulary)}\")  # Analyze the coverage of our vocabulary total_word_count = len(words) covered_word_count = sum(word_counts[word] for word in vocabulary) coverage_percentage = (covered_word_count / total_word_count) * 100  print(f\"Original vocabulary size: {len(word_counts)}\") print(f\"Filtered vocabulary size: {len(vocabulary)}\") print(f\"Words retained: {covered_word_count:,} out of {total_word_count:,} ({coverage_percentage:.2f}%)\")  # Plot the effect of different min_count thresholds thresholds = [1, 2, 5, 10, 20, 50, 100] vocab_sizes = [] coverage_percentages = []  for threshold in thresholds:     filtered_vocab = {word: count for word, count in word_counts.items() if count &gt;= threshold}     vocab_sizes.append(len(filtered_vocab))     covered = sum(word_counts[word] for word in filtered_vocab)     coverage_percentages.append((covered / total_word_count) * 100)  plt.figure(figsize=(12, 5)) plt.subplot(1, 2, 1) plt.plot(thresholds, vocab_sizes, 'o-') plt.title('Vocabulary Size vs Min Count') plt.xlabel('Minimum Count Threshold') plt.ylabel('Vocabulary Size') plt.grid(True)  plt.subplot(1, 2, 2) plt.plot(thresholds, coverage_percentages, 'o-') plt.title('Text Coverage vs Min Count') plt.xlabel('Minimum Count Threshold') plt.ylabel('Coverage (%)') plt.grid(True) plt.tight_layout() plt.show() <pre>Vocabulary size after filtering: 12515\nOriginal vocabulary size: 47046\nFiltered vocabulary size: 12515\nWords retained: 791,689 out of 846,987 (93.47%)\n</pre> In\u00a0[4]: Copied! <pre># Create word-to-index and index-to-word mappings\nword2idx = {word: i for i, word in enumerate(vocabulary.keys())}\nidx2word = {i: word for word, i in word2idx.items()}\nvocab_size = len(word2idx)\n\n# Filter the corpus to include only words in our vocabulary\nfiltered_words = [word for word in words if word in vocabulary]\nprint(f\"Words retained after filtering: {len(filtered_words)} ({len(filtered_words)/len(words):.2%})\")\n</pre> # Create word-to-index and index-to-word mappings word2idx = {word: i for i, word in enumerate(vocabulary.keys())} idx2word = {i: word for word, i in word2idx.items()} vocab_size = len(word2idx)  # Filter the corpus to include only words in our vocabulary filtered_words = [word for word in words if word in vocabulary] print(f\"Words retained after filtering: {len(filtered_words)} ({len(filtered_words)/len(words):.2%})\") <pre>Words retained after filtering: 791689 (93.47%)\n</pre> In\u00a0[5]: Copied! <pre># Demonstrate the context window\ndef visualize_context_window(text, window_size=2):\n    words = text.split()\n    examples = []\n    \n    for i in range(len(words)):\n        target = words[i]\n        # Calculate context range (window_size words on each side)\n        context_start = max(0, i - window_size)\n        context_end = min(len(words), i + window_size + 1)\n        \n        # Get context words (excluding the target)\n        context = words[context_start:i] + words[i+1:context_end]\n        examples.append((target, context))\n    \n    return examples\n\n# Example text\nsample_text = \"the quick brown fox jumps over the lazy dog\"\nexamples = visualize_context_window(sample_text, window_size=2)\n\n# Create a visualization\nplt.figure(figsize=(8, 10))\nplt.title(\"Context Window Visualization (Window Size = 2)\")\nplt.axis('off')\n\nfor i, (target, context) in enumerate(examples):\n    # Display target word\n    plt.text(0.3, -i/len(examples), target, fontsize=10, \n             ha='center', bbox=dict(facecolor='lightblue', alpha=0.5, pad=5))\n    \n    # Display context words\n    for j, word in enumerate(context):\n        x_offset = (j * 0.2)\n        plt.text(x_offset, -i/len(examples), word, fontsize=10, \n                 ha='center', bbox=dict(facecolor='lightyellow', alpha=0.5, pad=5))\n\nplt.tight_layout()\nplt.show()\n</pre> # Demonstrate the context window def visualize_context_window(text, window_size=2):     words = text.split()     examples = []          for i in range(len(words)):         target = words[i]         # Calculate context range (window_size words on each side)         context_start = max(0, i - window_size)         context_end = min(len(words), i + window_size + 1)                  # Get context words (excluding the target)         context = words[context_start:i] + words[i+1:context_end]         examples.append((target, context))          return examples  # Example text sample_text = \"the quick brown fox jumps over the lazy dog\" examples = visualize_context_window(sample_text, window_size=2)  # Create a visualization plt.figure(figsize=(8, 10)) plt.title(\"Context Window Visualization (Window Size = 2)\") plt.axis('off')  for i, (target, context) in enumerate(examples):     # Display target word     plt.text(0.3, -i/len(examples), target, fontsize=10,               ha='center', bbox=dict(facecolor='lightblue', alpha=0.5, pad=5))          # Display context words     for j, word in enumerate(context):         x_offset = (j * 0.2)         plt.text(x_offset, -i/len(examples), word, fontsize=10,                   ha='center', bbox=dict(facecolor='lightyellow', alpha=0.5, pad=5))  plt.tight_layout() plt.show() In\u00a0[6]: Copied! <pre>def generate_skipgram_pairs(corpus, window_size, word2idx):\n    \"\"\"\n    Generate training pairs for the Skip-gram model.\n    \n    Args:\n        corpus: List of tokenized words\n        window_size: Number of words to consider as context on each side\n        word2idx: Word to index mapping\n    \n    Returns:\n        pairs: List of (target_idx, context_idx) pairs\n    \"\"\"\n    pairs = []\n    \n    # Loop through each word in corpus\n    for i in tqdm(range(len(corpus)), desc=\"Generating Skip-gram pairs\"):\n        target = word2idx[corpus[i]]\n        \n        # Get context words within the window\n        context_start = max(0, i - window_size)\n        context_end = min(len(corpus), i + window_size + 1)\n        \n        for j in range(context_start, context_end):\n            if i != j:  # Skip the target word itself\n                if corpus[j] in word2idx:  # Check if context word is in vocabulary\n                    context = word2idx[corpus[j]]\n                    pairs.append((target, context))\n    \n    return np.array(pairs)\n\n# Generate Skip-gram training pairs\nwindow_size = 2  # Context window of 2 words on each side\nskipgram_pairs = generate_skipgram_pairs(filtered_words, window_size, word2idx)\n\nprint(f\"Generated {len(skipgram_pairs)} Skip-gram training pairs\")\nprint(f\"Sample pairs (word indices):\")\nfor i in range(5):\n    target_idx, context_idx = skipgram_pairs[i]\n    print(f\"  Target: {idx2word[target_idx]} ({target_idx}) \u2192 Context: {idx2word[context_idx]} ({context_idx})\")\n</pre> def generate_skipgram_pairs(corpus, window_size, word2idx):     \"\"\"     Generate training pairs for the Skip-gram model.          Args:         corpus: List of tokenized words         window_size: Number of words to consider as context on each side         word2idx: Word to index mapping          Returns:         pairs: List of (target_idx, context_idx) pairs     \"\"\"     pairs = []          # Loop through each word in corpus     for i in tqdm(range(len(corpus)), desc=\"Generating Skip-gram pairs\"):         target = word2idx[corpus[i]]                  # Get context words within the window         context_start = max(0, i - window_size)         context_end = min(len(corpus), i + window_size + 1)                  for j in range(context_start, context_end):             if i != j:  # Skip the target word itself                 if corpus[j] in word2idx:  # Check if context word is in vocabulary                     context = word2idx[corpus[j]]                     pairs.append((target, context))          return np.array(pairs)  # Generate Skip-gram training pairs window_size = 2  # Context window of 2 words on each side skipgram_pairs = generate_skipgram_pairs(filtered_words, window_size, word2idx)  print(f\"Generated {len(skipgram_pairs)} Skip-gram training pairs\") print(f\"Sample pairs (word indices):\") for i in range(5):     target_idx, context_idx = skipgram_pairs[i]     print(f\"  Target: {idx2word[target_idx]} ({target_idx}) \u2192 Context: {idx2word[context_idx]} ({context_idx})\") <pre>Generating Skip-gram pairs:   0%|          | 0/791689 [00:00&lt;?, ?it/s]</pre> <pre>Generated 3166750 Skip-gram training pairs\nSample pairs (word indices):\n  Target: anarchism (0) \u2192 Context: originated (1)\n  Target: anarchism (0) \u2192 Context: as (2)\n  Target: originated (1) \u2192 Context: anarchism (0)\n  Target: originated (1) \u2192 Context: as (2)\n  Target: originated (1) \u2192 Context: a (3)\n</pre> In\u00a0[7]: Copied! <pre>def generate_cbow_pairs(corpus, window_size, word2idx):\n    \"\"\"\n    Generate training pairs for the CBOW model.\n    \n    Args:\n        corpus: List of tokenized words\n        window_size: Number of words to consider as context on each side\n        word2idx: Word to index mapping\n    \n    Returns:\n        X: Context word indices (each row contains 2*window_size indices)\n        y: Target word indices\n    \"\"\"\n    X = []  # Context words\n    y = []  # Target words\n    \n    # Loop through each word in corpus\n    for i in tqdm(range(len(corpus)), desc=\"Generating CBOW pairs\"):\n        if corpus[i] not in word2idx:\n            continue\n            \n        target = word2idx[corpus[i]]\n        \n        # Get context words within the window\n        context_start = max(0, i - window_size)\n        context_end = min(len(corpus), i + window_size + 1)\n        \n        # Get context word indices\n        context_indices = []\n        for j in range(context_start, context_end):\n            if i != j and corpus[j] in word2idx:  # Skip target &amp; ensure word is in vocabulary\n                context_indices.append(word2idx[corpus[j]])\n        \n        # Only use examples with sufficient context\n        if len(context_indices) == 2 * window_size:  # Full context window\n            X.append(context_indices)\n            y.append(target)\n    \n    return np.array(X), np.array(y)\n\n# Generate CBOW training pairs\nwindow_size = 2  # Context window of 2 words on each side\nX_cbow, y_cbow = generate_cbow_pairs(filtered_words, window_size, word2idx)\n\nprint(f\"Generated {len(X_cbow)} CBOW training examples\")\nprint(f\"Shape of X_cbow: {X_cbow.shape}\")\nprint(f\"Shape of y_cbow: {y_cbow.shape}\")\n\n# Show sample CBOW training examples\nprint(\"\\nSample CBOW examples:\")\nfor i in range(5):\n    context_indices = X_cbow[i]\n    target_idx = y_cbow[i]\n    \n    context_words = [idx2word[idx] for idx in context_indices]\n    target_word = idx2word[target_idx]\n    \n    print(f\"  Context: {context_words} \u2192 Target: {target_word}\")\n</pre> def generate_cbow_pairs(corpus, window_size, word2idx):     \"\"\"     Generate training pairs for the CBOW model.          Args:         corpus: List of tokenized words         window_size: Number of words to consider as context on each side         word2idx: Word to index mapping          Returns:         X: Context word indices (each row contains 2*window_size indices)         y: Target word indices     \"\"\"     X = []  # Context words     y = []  # Target words          # Loop through each word in corpus     for i in tqdm(range(len(corpus)), desc=\"Generating CBOW pairs\"):         if corpus[i] not in word2idx:             continue                      target = word2idx[corpus[i]]                  # Get context words within the window         context_start = max(0, i - window_size)         context_end = min(len(corpus), i + window_size + 1)                  # Get context word indices         context_indices = []         for j in range(context_start, context_end):             if i != j and corpus[j] in word2idx:  # Skip target &amp; ensure word is in vocabulary                 context_indices.append(word2idx[corpus[j]])                  # Only use examples with sufficient context         if len(context_indices) == 2 * window_size:  # Full context window             X.append(context_indices)             y.append(target)          return np.array(X), np.array(y)  # Generate CBOW training pairs window_size = 2  # Context window of 2 words on each side X_cbow, y_cbow = generate_cbow_pairs(filtered_words, window_size, word2idx)  print(f\"Generated {len(X_cbow)} CBOW training examples\") print(f\"Shape of X_cbow: {X_cbow.shape}\") print(f\"Shape of y_cbow: {y_cbow.shape}\")  # Show sample CBOW training examples print(\"\\nSample CBOW examples:\") for i in range(5):     context_indices = X_cbow[i]     target_idx = y_cbow[i]          context_words = [idx2word[idx] for idx in context_indices]     target_word = idx2word[target_idx]          print(f\"  Context: {context_words} \u2192 Target: {target_word}\") <pre>Generating CBOW pairs:   0%|          | 0/791689 [00:00&lt;?, ?it/s]</pre> <pre>Generated 791685 CBOW training examples\nShape of X_cbow: (791685, 4)\nShape of y_cbow: (791685,)\n\nSample CBOW examples:\n  Context: ['anarchism', 'originated', 'a', 'term'] \u2192 Target: as\n  Context: ['originated', 'as', 'term', 'of'] \u2192 Target: a\n  Context: ['as', 'a', 'of', 'abuse'] \u2192 Target: term\n  Context: ['a', 'term', 'abuse', 'first'] \u2192 Target: of\n  Context: ['term', 'of', 'first', 'used'] \u2192 Target: abuse\n</pre> In\u00a0[8]: Copied! <pre>from tensorflow import keras\nfrom keras import layers\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\nimport time\n\nclass Word2VecNegativeSampling(keras.Model):\n    def __init__(self, vocab_size, embedding_dim, num_ns=4):\n        super(Word2VecNegativeSampling, self).__init__()\n        self.target_embedding = layers.Embedding(\n            vocab_size, \n            embedding_dim,\n            name=\"w2v_embedding\",\n        )\n        self.context_embedding = layers.Embedding(\n            vocab_size, \n            embedding_dim,\n            name=\"context_embedding\"\n        )\n        self.num_ns = num_ns\n        \n    def call(self, inputs):\n        target_inputs, context_inputs, negative_inputs = inputs\n        \n        # Get embeddings\n        # Shape: (batch_size, 1, embedding_dim)\n        target_embedding = self.target_embedding(target_inputs)\n        # Shape: (batch_size, 1, embedding_dim)\n        context_embedding = self.context_embedding(context_inputs)\n        # Shape: (batch_size, num_ns, embedding_dim)\n        negative_embedding = self.context_embedding(negative_inputs)\n        \n        # Remove the extra dimension from target and context\n        # Shape: (batch_size, embedding_dim)\n        target_embedding_flat = tf.squeeze(target_embedding, axis=1)\n        # Shape: (batch_size, embedding_dim)\n        context_embedding_flat = tf.squeeze(context_embedding, axis=1)\n        \n        # Compute positive dot product (target \u00b7 context)\n        # Shape: (batch_size, 1)\n        positive_dots = tf.reduce_sum(\n            tf.multiply(target_embedding_flat, context_embedding_flat), \n            axis=1, keepdims=True\n        )\n        \n        # Reshape target for negative samples\n        # Shape: (batch_size, 1, embedding_dim)\n        target_for_negatives = tf.expand_dims(target_embedding_flat, axis=1)\n        \n        # Compute negative dot products (target \u00b7 negative samples)\n        # Broadcasting handles the shapes: (batch_size, 1, embedding_dim) * (batch_size, num_ns, embedding_dim)\n        # Result shape: (batch_size, num_ns)\n        negative_dots = tf.reduce_sum(\n            tf.multiply(target_for_negatives, negative_embedding),\n            axis=2\n        )\n        \n        # Combine positive and negative dots\n        # Shape: (batch_size, 1 + num_ns)\n        dots = tf.concat([positive_dots, negative_dots], axis=1)\n        \n        return dots\n\n# Custom loss function to handle the combined logits\ndef w2v_negative_sampling_loss(y_true, y_pred):\n    \"\"\"\n    Word2Vec negative sampling loss.\n    y_pred contains dot products for positive (first column) and negative samples (remaining columns)\n    \"\"\"\n    # Create labels: 1 for positive sample, 0 for negative samples\n    batch_size = tf.shape(y_pred)[0]\n    # Shape: (batch_size, 1 + num_ns)\n    labels = tf.concat([\n        tf.ones((batch_size, 1), dtype=tf.float32),\n        tf.zeros((batch_size, tf.shape(y_pred)[1] - 1), dtype=tf.float32)\n    ], axis=1)\n    \n    # Apply sigmoid cross entropy\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=y_pred)\n    )\n\n# Function to prepare training data\ndef prepare_training_data(skipgram_pairs, vocab_size, num_ns=4, seed=42):\n    \"\"\"Generate properly formatted data for negative sampling\"\"\"\n    np.random.seed(seed)\n    \n    # Extract target and context words\n    targets, contexts = skipgram_pairs[:, 0], skipgram_pairs[:, 1]\n    \n    # Generate negative samples\n    negatives = []\n    for target, context in zip(targets, contexts):\n        negative_samples = []\n        while len(negative_samples) &lt; num_ns:\n            neg = np.random.randint(0, vocab_size)\n            if neg != target and neg != context:\n                negative_samples.append(neg)\n        negatives.append(negative_samples)\n    \n    # Convert to arrays and reshape\n    targets = np.array(targets, dtype=np.int32)[:, np.newaxis]\n    contexts = np.array(contexts, dtype=np.int32)[:, np.newaxis]\n    negatives = np.array(negatives, dtype=np.int32)\n    \n    return targets, contexts, negatives\n\n# Prepare the training data (assuming skipgram_pairs is already defined)\ntargets, contexts, negatives = prepare_training_data(\n    skipgram_pairs, vocab_size, num_ns=4\n)\n\nprint(f\"Prepared training data:\")\nprint(f\"  Targets shape: {targets.shape}\")\nprint(f\"  Contexts shape: {contexts.shape}\")\nprint(f\"  Negatives shape: {negatives.shape}\")\n\n# Create TensorFlow dataset\ndataset = tf.data.Dataset.from_tensor_slices(\n    ((targets, contexts, negatives), np.zeros(len(targets)))\n)\ndataset = dataset.shuffle(buffer_size=10000)\ndataset = dataset.batch(512)\ndataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n# Create and compile model\nembedding_dim = 16\nnum_ns = 4\nmodel = Word2VecNegativeSampling(vocab_size, embedding_dim, num_ns)\n\n# Compile with loss function\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n    loss=w2v_negative_sampling_loss\n)\n\nmodel.summary()\n</pre> from tensorflow import keras from keras import layers import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from collections import Counter from tqdm.notebook import tqdm import time  class Word2VecNegativeSampling(keras.Model):     def __init__(self, vocab_size, embedding_dim, num_ns=4):         super(Word2VecNegativeSampling, self).__init__()         self.target_embedding = layers.Embedding(             vocab_size,              embedding_dim,             name=\"w2v_embedding\",         )         self.context_embedding = layers.Embedding(             vocab_size,              embedding_dim,             name=\"context_embedding\"         )         self.num_ns = num_ns              def call(self, inputs):         target_inputs, context_inputs, negative_inputs = inputs                  # Get embeddings         # Shape: (batch_size, 1, embedding_dim)         target_embedding = self.target_embedding(target_inputs)         # Shape: (batch_size, 1, embedding_dim)         context_embedding = self.context_embedding(context_inputs)         # Shape: (batch_size, num_ns, embedding_dim)         negative_embedding = self.context_embedding(negative_inputs)                  # Remove the extra dimension from target and context         # Shape: (batch_size, embedding_dim)         target_embedding_flat = tf.squeeze(target_embedding, axis=1)         # Shape: (batch_size, embedding_dim)         context_embedding_flat = tf.squeeze(context_embedding, axis=1)                  # Compute positive dot product (target \u00b7 context)         # Shape: (batch_size, 1)         positive_dots = tf.reduce_sum(             tf.multiply(target_embedding_flat, context_embedding_flat),              axis=1, keepdims=True         )                  # Reshape target for negative samples         # Shape: (batch_size, 1, embedding_dim)         target_for_negatives = tf.expand_dims(target_embedding_flat, axis=1)                  # Compute negative dot products (target \u00b7 negative samples)         # Broadcasting handles the shapes: (batch_size, 1, embedding_dim) * (batch_size, num_ns, embedding_dim)         # Result shape: (batch_size, num_ns)         negative_dots = tf.reduce_sum(             tf.multiply(target_for_negatives, negative_embedding),             axis=2         )                  # Combine positive and negative dots         # Shape: (batch_size, 1 + num_ns)         dots = tf.concat([positive_dots, negative_dots], axis=1)                  return dots  # Custom loss function to handle the combined logits def w2v_negative_sampling_loss(y_true, y_pred):     \"\"\"     Word2Vec negative sampling loss.     y_pred contains dot products for positive (first column) and negative samples (remaining columns)     \"\"\"     # Create labels: 1 for positive sample, 0 for negative samples     batch_size = tf.shape(y_pred)[0]     # Shape: (batch_size, 1 + num_ns)     labels = tf.concat([         tf.ones((batch_size, 1), dtype=tf.float32),         tf.zeros((batch_size, tf.shape(y_pred)[1] - 1), dtype=tf.float32)     ], axis=1)          # Apply sigmoid cross entropy     return tf.reduce_mean(         tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=y_pred)     )  # Function to prepare training data def prepare_training_data(skipgram_pairs, vocab_size, num_ns=4, seed=42):     \"\"\"Generate properly formatted data for negative sampling\"\"\"     np.random.seed(seed)          # Extract target and context words     targets, contexts = skipgram_pairs[:, 0], skipgram_pairs[:, 1]          # Generate negative samples     negatives = []     for target, context in zip(targets, contexts):         negative_samples = []         while len(negative_samples) &lt; num_ns:             neg = np.random.randint(0, vocab_size)             if neg != target and neg != context:                 negative_samples.append(neg)         negatives.append(negative_samples)          # Convert to arrays and reshape     targets = np.array(targets, dtype=np.int32)[:, np.newaxis]     contexts = np.array(contexts, dtype=np.int32)[:, np.newaxis]     negatives = np.array(negatives, dtype=np.int32)          return targets, contexts, negatives  # Prepare the training data (assuming skipgram_pairs is already defined) targets, contexts, negatives = prepare_training_data(     skipgram_pairs, vocab_size, num_ns=4 )  print(f\"Prepared training data:\") print(f\"  Targets shape: {targets.shape}\") print(f\"  Contexts shape: {contexts.shape}\") print(f\"  Negatives shape: {negatives.shape}\")  # Create TensorFlow dataset dataset = tf.data.Dataset.from_tensor_slices(     ((targets, contexts, negatives), np.zeros(len(targets))) ) dataset = dataset.shuffle(buffer_size=10000) dataset = dataset.batch(512) dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Create and compile model embedding_dim = 16 num_ns = 4 model = Word2VecNegativeSampling(vocab_size, embedding_dim, num_ns)  # Compile with loss function model.compile(     optimizer=keras.optimizers.Adam(learning_rate=0.01),     loss=w2v_negative_sampling_loss )  model.summary() <pre>Prepared training data:\n  Targets shape: (3166750, 1)\n  Contexts shape: (3166750, 1)\n  Negatives shape: (3166750, 4)\n</pre> <pre>Model: \"word2_vec_negative_sampling\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 w2v_embedding (Embedding)       \u2502 ?                      \u2502   0 (unbuilt) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 context_embedding (Embedding)   \u2502 ?                      \u2502   0 (unbuilt) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 0 (0.00 B)\n</pre> <pre> Trainable params: 0 (0.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[9]: Copied! <pre># Train model with early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='loss', patience=5, restore_best_weights=True\n)\n\nprint(\"Beginning training...\")\nhistory = model.fit(\n    dataset, \n    steps_per_epoch=1,\n    epochs=1,\n    verbose=1,\n    callbacks=[early_stopping]\n)\n\nskipgram_initial_embeddings = model.target_embedding.get_weights()[0]\n\nhistory = model.fit(\n    dataset, \n    epochs=20,\n    verbose=1,\n    callbacks=[early_stopping]\n)\n\n# Plot loss history\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['loss'])\nplt.title('Word2Vec Negative Sampling Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.show()\n</pre> # Train model with early stopping early_stopping = tf.keras.callbacks.EarlyStopping(     monitor='loss', patience=5, restore_best_weights=True )  print(\"Beginning training...\") history = model.fit(     dataset,      steps_per_epoch=1,     epochs=1,     verbose=1,     callbacks=[early_stopping] )  skipgram_initial_embeddings = model.target_embedding.get_weights()[0]  history = model.fit(     dataset,      epochs=20,     verbose=1,     callbacks=[early_stopping] )  # Plot loss history plt.figure(figsize=(10, 6)) plt.plot(history.history['loss']) plt.title('Word2Vec Negative Sampling Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.grid(True) plt.show() <pre>Beginning training...\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 389ms/step - loss: 0.6932\nEpoch 1/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 2ms/step - loss: 0.2820\nEpoch 2/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 2ms/step - loss: 0.2471\nEpoch 3/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 15s 2ms/step - loss: 0.2424\nEpoch 4/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 15s 2ms/step - loss: 0.2408\nEpoch 5/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 2ms/step - loss: 0.2402\nEpoch 6/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 2ms/step - loss: 0.2398\nEpoch 7/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 2ms/step - loss: 0.2396\nEpoch 8/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 2ms/step - loss: 0.2397\nEpoch 9/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 2ms/step - loss: 0.2394\nEpoch 10/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13s 2ms/step - loss: 0.2394\nEpoch 11/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 2ms/step - loss: 0.2395\nEpoch 12/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 2ms/step - loss: 0.2395\nEpoch 13/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 2ms/step - loss: 0.2398\nEpoch 14/20\n6186/6186 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 2ms/step - loss: 0.2396\n</pre> In\u00a0[10]: Copied! <pre># Extract learned embeddings\nskipgram_learned_embeddings = model.target_embedding.get_weights()[0]\nprint(f\"Learned embeddings shape: {skipgram_learned_embeddings.shape}\")\n\ndef find_similar_words(word, embeddings, word2idx, idx2word, top_n=5):\n    \"\"\"Find most similar words based on cosine similarity\"\"\"\n    if word not in word2idx:\n        return []\n        \n    word_idx = word2idx[word]\n    word_vec = embeddings[word_idx]\n    \n    # Normalize vectors for cosine similarity\n    norm_word = np.linalg.norm(word_vec)\n    if norm_word &gt; 0:\n        word_vec = word_vec / norm_word\n    \n    # Normalize all embeddings\n    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n    norms = np.maximum(norms, 1e-10)  # Avoid division by zero\n    normalized_embeddings = embeddings / norms\n    \n    # Calculate similarities\n    similarities = np.dot(normalized_embeddings, word_vec)\n    \n    # Get top n similar words\n    most_similar = []\n    for idx in similarities.argsort()[-top_n-1:][::-1]:\n        if idx != word_idx:\n            most_similar.append((idx2word[idx], similarities[idx]))\n            \n    return most_similar\n\n# Test some example words\ntest_words = [\"king\", \"france\", \"computer\", \"dog\"]\nfor word in test_words:\n    if word in word2idx:\n        similar_words = find_similar_words(word, skipgram_initial_embeddings, word2idx, idx2word)\n        print(f\"\\nWords similar to '{word} - initial embeddings':\")\n        for similar_word, similarity in similar_words:\n            print(f\"  {similar_word}: {similarity:.4f}\")\n        similar_words = find_similar_words(word, skipgram_learned_embeddings, word2idx, idx2word)\n        print(f\"\\nWords similar to '{word} - learned embeddings':\")\n        for similar_word, similarity in similar_words:\n            print(f\"  {similar_word}: {similarity:.4f}\")\n        print('---')\n</pre> # Extract learned embeddings skipgram_learned_embeddings = model.target_embedding.get_weights()[0] print(f\"Learned embeddings shape: {skipgram_learned_embeddings.shape}\")  def find_similar_words(word, embeddings, word2idx, idx2word, top_n=5):     \"\"\"Find most similar words based on cosine similarity\"\"\"     if word not in word2idx:         return []              word_idx = word2idx[word]     word_vec = embeddings[word_idx]          # Normalize vectors for cosine similarity     norm_word = np.linalg.norm(word_vec)     if norm_word &gt; 0:         word_vec = word_vec / norm_word          # Normalize all embeddings     norms = np.linalg.norm(embeddings, axis=1, keepdims=True)     norms = np.maximum(norms, 1e-10)  # Avoid division by zero     normalized_embeddings = embeddings / norms          # Calculate similarities     similarities = np.dot(normalized_embeddings, word_vec)          # Get top n similar words     most_similar = []     for idx in similarities.argsort()[-top_n-1:][::-1]:         if idx != word_idx:             most_similar.append((idx2word[idx], similarities[idx]))                  return most_similar  # Test some example words test_words = [\"king\", \"france\", \"computer\", \"dog\"] for word in test_words:     if word in word2idx:         similar_words = find_similar_words(word, skipgram_initial_embeddings, word2idx, idx2word)         print(f\"\\nWords similar to '{word} - initial embeddings':\")         for similar_word, similarity in similar_words:             print(f\"  {similar_word}: {similarity:.4f}\")         similar_words = find_similar_words(word, skipgram_learned_embeddings, word2idx, idx2word)         print(f\"\\nWords similar to '{word} - learned embeddings':\")         for similar_word, similarity in similar_words:             print(f\"  {similar_word}: {similarity:.4f}\")         print('---') <pre>Learned embeddings shape: (12515, 16)\n\nWords similar to 'king - initial embeddings':\n  semantic: 0.7762\n  atheistic: 0.7647\n  commentator: 0.7542\n  pot: 0.7325\n  advanced: 0.7275\n\nWords similar to 'king - learned embeddings':\n  bishop: 0.8909\n  john: 0.8590\n  res: 0.8484\n  edward: 0.8416\n  henry: 0.8413\n---\n\nWords similar to 'france - initial embeddings':\n  estate: 0.8324\n  optional: 0.7671\n  carried: 0.7653\n  ufos: 0.7577\n  serious: 0.7473\n\nWords similar to 'france - learned embeddings':\n  serbia: 0.8250\n  baptiste: 0.8215\n  clifford: 0.8102\n  de: 0.8012\n  italy: 0.7880\n---\n\nWords similar to 'computer - initial embeddings':\n  syrian: 0.8098\n  discusses: 0.8062\n  venues: 0.7890\n  distant: 0.7887\n  tiger: 0.7569\n\nWords similar to 'computer - learned embeddings':\n  internet: 0.8837\n  eleftherios: 0.8740\n  pages: 0.8211\n  convention: 0.8198\n  code: 0.8186\n---\n\nWords similar to 'dog - initial embeddings':\n  frogs: 0.7953\n  impairments: 0.7868\n  dealing: 0.7792\n  hitherto: 0.7478\n  feed: 0.7392\n\nWords similar to 'dog - learned embeddings':\n  prairie: 0.8507\n  got: 0.8158\n  rapper: 0.8102\n  graduating: 0.7931\n  johnson: 0.7864\n---\n</pre> In\u00a0[11]: Copied! <pre>class CBOWNegativeSampling(keras.Model):\n    def __init__(self, vocab_size, embedding_dim, context_size=4, num_ns=4):\n        super(CBOWNegativeSampling, self).__init__()\n        self.word_embedding = layers.Embedding(\n            vocab_size, \n            embedding_dim,\n            name=\"cbow_embedding\",\n        )\n        self.target_embedding = layers.Embedding(\n            vocab_size, \n            embedding_dim,\n            name=\"target_embedding\"\n        )\n        self.context_size = context_size\n        self.num_ns = num_ns\n        \n    def call(self, inputs):\n        context_inputs, target_inputs, negative_inputs = inputs\n        \n        # Get embeddings\n        # Shape: (batch_size, context_size, embedding_dim)\n        context_embedding = self.word_embedding(context_inputs)\n        # Shape: (batch_size, 1, embedding_dim)\n        target_embedding = self.target_embedding(target_inputs)\n        # Shape: (batch_size, num_ns, embedding_dim)\n        negative_embedding = self.target_embedding(negative_inputs)\n        \n        # Average the context embeddings\n        # Shape: (batch_size, embedding_dim)\n        context_embedding_avg = tf.reduce_mean(context_embedding, axis=1)\n        \n        # Remove the extra dimension from target\n        # Shape: (batch_size, embedding_dim)\n        target_embedding_flat = tf.squeeze(target_embedding, axis=1)\n        \n        # Compute positive dot product (context_avg \u00b7 target)\n        # Shape: (batch_size, 1)\n        positive_dots = tf.reduce_sum(\n            tf.multiply(context_embedding_avg, target_embedding_flat), \n            axis=1, keepdims=True\n        )\n        \n        # Reshape context_avg for negative samples\n        # Shape: (batch_size, 1, embedding_dim)\n        context_for_negatives = tf.expand_dims(context_embedding_avg, axis=1)\n        \n        # Compute negative dot products (context_avg \u00b7 negative samples)\n        # Broadcasting handles the shapes\n        # Result shape: (batch_size, num_ns)\n        negative_dots = tf.reduce_sum(\n            tf.multiply(context_for_negatives, negative_embedding),\n            axis=2\n        )\n        \n        # Combine positive and negative dots\n        # Shape: (batch_size, 1 + num_ns)\n        dots = tf.concat([positive_dots, negative_dots], axis=1)\n        \n        return dots\n\n# Custom loss function (same as Word2Vec)\ndef cbow_negative_sampling_loss(y_true, y_pred):\n    \"\"\"\n    CBOW negative sampling loss.\n    y_pred contains dot products for positive (first column) and negative samples (remaining columns)\n    \"\"\"\n    # Create labels: 1 for positive sample, 0 for negative samples\n    batch_size = tf.shape(y_pred)[0]\n    # Shape: (batch_size, 1 + num_ns)\n    labels = tf.concat([\n        tf.ones((batch_size, 1), dtype=tf.float32),\n        tf.zeros((batch_size, tf.shape(y_pred)[1] - 1), dtype=tf.float32)\n    ], axis=1)\n    \n    # Apply sigmoid cross entropy\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=y_pred)\n    )\n\n# Function to prepare CBOW training data\ndef prepare_cbow_training_data(corpus, word2idx, context_size=2, num_ns=4, seed=42):\n    \"\"\"Generate training data for CBOW with negative sampling\"\"\"\n    np.random.seed(seed)\n    \n    # Create context-target pairs\n    contexts = []\n    targets = []\n    \n    # Loop through each word in the corpus\n    for i in tqdm(range(context_size, len(corpus) - context_size), desc=\"Generating CBOW pairs\"):\n        if corpus[i] not in word2idx:\n            continue\n            \n        # Get target word\n        target = word2idx[corpus[i]]\n        \n        # Get context words (context_size words on each side)\n        context = []\n        for j in range(i - context_size, i + context_size + 1):\n            if j != i and corpus[j] in word2idx:  # Skip target itself\n                context.append(word2idx[corpus[j]])\n        \n        # Only use examples with full context\n        if len(context) == context_size * 2:  # Full context window\n            contexts.append(context)\n            targets.append(target)\n    \n    # Convert to arrays\n    contexts = np.array(contexts, dtype=np.int32)\n    targets = np.array(targets, dtype=np.int32)[:, np.newaxis]\n    \n    # Generate negative samples\n    negatives = []\n    vocab_size = len(word2idx)\n    \n    for target in targets:\n        negative_samples = []\n        while len(negative_samples) &lt; num_ns:\n            neg = np.random.randint(0, vocab_size)\n            if neg != target[0]:  # Don't include the target word\n                negative_samples.append(neg)\n        negatives.append(negative_samples)\n    \n    negatives = np.array(negatives, dtype=np.int32)\n    \n    return contexts, targets, negatives\n\n# Generate CBOW training data\ncontext_size = 2  # 2 words on each side = 4 total context words\nnum_ns = 4  # Number of negative samples per positive example\n\ncontexts, targets, negatives = prepare_cbow_training_data(\n    filtered_words, word2idx, context_size=context_size, num_ns=num_ns\n)\n\nprint(f\"Prepared CBOW training data:\")\nprint(f\"  Contexts shape: {contexts.shape} (each row contains {context_size*2} context words)\")\nprint(f\"  Targets shape: {targets.shape}\")\nprint(f\"  Negatives shape: {negatives.shape}\")\n\n# Create TensorFlow dataset\ndataset = tf.data.Dataset.from_tensor_slices(\n    ((contexts, targets, negatives), np.zeros(len(targets)))\n)\ndataset = dataset.shuffle(buffer_size=10000)\ndataset = dataset.batch(512)\ndataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n# Create and compile model\nembedding_dim = 16\nmodel = CBOWNegativeSampling(\n    vocab_size, embedding_dim, context_size=context_size*2, num_ns=num_ns\n)\n\n# Compile with loss function\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n    loss=cbow_negative_sampling_loss\n)\n\n# Print model summary\nmodel.summary()\n</pre> class CBOWNegativeSampling(keras.Model):     def __init__(self, vocab_size, embedding_dim, context_size=4, num_ns=4):         super(CBOWNegativeSampling, self).__init__()         self.word_embedding = layers.Embedding(             vocab_size,              embedding_dim,             name=\"cbow_embedding\",         )         self.target_embedding = layers.Embedding(             vocab_size,              embedding_dim,             name=\"target_embedding\"         )         self.context_size = context_size         self.num_ns = num_ns              def call(self, inputs):         context_inputs, target_inputs, negative_inputs = inputs                  # Get embeddings         # Shape: (batch_size, context_size, embedding_dim)         context_embedding = self.word_embedding(context_inputs)         # Shape: (batch_size, 1, embedding_dim)         target_embedding = self.target_embedding(target_inputs)         # Shape: (batch_size, num_ns, embedding_dim)         negative_embedding = self.target_embedding(negative_inputs)                  # Average the context embeddings         # Shape: (batch_size, embedding_dim)         context_embedding_avg = tf.reduce_mean(context_embedding, axis=1)                  # Remove the extra dimension from target         # Shape: (batch_size, embedding_dim)         target_embedding_flat = tf.squeeze(target_embedding, axis=1)                  # Compute positive dot product (context_avg \u00b7 target)         # Shape: (batch_size, 1)         positive_dots = tf.reduce_sum(             tf.multiply(context_embedding_avg, target_embedding_flat),              axis=1, keepdims=True         )                  # Reshape context_avg for negative samples         # Shape: (batch_size, 1, embedding_dim)         context_for_negatives = tf.expand_dims(context_embedding_avg, axis=1)                  # Compute negative dot products (context_avg \u00b7 negative samples)         # Broadcasting handles the shapes         # Result shape: (batch_size, num_ns)         negative_dots = tf.reduce_sum(             tf.multiply(context_for_negatives, negative_embedding),             axis=2         )                  # Combine positive and negative dots         # Shape: (batch_size, 1 + num_ns)         dots = tf.concat([positive_dots, negative_dots], axis=1)                  return dots  # Custom loss function (same as Word2Vec) def cbow_negative_sampling_loss(y_true, y_pred):     \"\"\"     CBOW negative sampling loss.     y_pred contains dot products for positive (first column) and negative samples (remaining columns)     \"\"\"     # Create labels: 1 for positive sample, 0 for negative samples     batch_size = tf.shape(y_pred)[0]     # Shape: (batch_size, 1 + num_ns)     labels = tf.concat([         tf.ones((batch_size, 1), dtype=tf.float32),         tf.zeros((batch_size, tf.shape(y_pred)[1] - 1), dtype=tf.float32)     ], axis=1)          # Apply sigmoid cross entropy     return tf.reduce_mean(         tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=y_pred)     )  # Function to prepare CBOW training data def prepare_cbow_training_data(corpus, word2idx, context_size=2, num_ns=4, seed=42):     \"\"\"Generate training data for CBOW with negative sampling\"\"\"     np.random.seed(seed)          # Create context-target pairs     contexts = []     targets = []          # Loop through each word in the corpus     for i in tqdm(range(context_size, len(corpus) - context_size), desc=\"Generating CBOW pairs\"):         if corpus[i] not in word2idx:             continue                      # Get target word         target = word2idx[corpus[i]]                  # Get context words (context_size words on each side)         context = []         for j in range(i - context_size, i + context_size + 1):             if j != i and corpus[j] in word2idx:  # Skip target itself                 context.append(word2idx[corpus[j]])                  # Only use examples with full context         if len(context) == context_size * 2:  # Full context window             contexts.append(context)             targets.append(target)          # Convert to arrays     contexts = np.array(contexts, dtype=np.int32)     targets = np.array(targets, dtype=np.int32)[:, np.newaxis]          # Generate negative samples     negatives = []     vocab_size = len(word2idx)          for target in targets:         negative_samples = []         while len(negative_samples) &lt; num_ns:             neg = np.random.randint(0, vocab_size)             if neg != target[0]:  # Don't include the target word                 negative_samples.append(neg)         negatives.append(negative_samples)          negatives = np.array(negatives, dtype=np.int32)          return contexts, targets, negatives  # Generate CBOW training data context_size = 2  # 2 words on each side = 4 total context words num_ns = 4  # Number of negative samples per positive example  contexts, targets, negatives = prepare_cbow_training_data(     filtered_words, word2idx, context_size=context_size, num_ns=num_ns )  print(f\"Prepared CBOW training data:\") print(f\"  Contexts shape: {contexts.shape} (each row contains {context_size*2} context words)\") print(f\"  Targets shape: {targets.shape}\") print(f\"  Negatives shape: {negatives.shape}\")  # Create TensorFlow dataset dataset = tf.data.Dataset.from_tensor_slices(     ((contexts, targets, negatives), np.zeros(len(targets))) ) dataset = dataset.shuffle(buffer_size=10000) dataset = dataset.batch(512) dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Create and compile model embedding_dim = 16 model = CBOWNegativeSampling(     vocab_size, embedding_dim, context_size=context_size*2, num_ns=num_ns )  # Compile with loss function model.compile(     optimizer=keras.optimizers.Adam(learning_rate=0.01),     loss=cbow_negative_sampling_loss )  # Print model summary model.summary() <pre>Generating CBOW pairs:   0%|          | 0/791685 [00:00&lt;?, ?it/s]</pre> <pre>Prepared CBOW training data:\n  Contexts shape: (791685, 4) (each row contains 4 context words)\n  Targets shape: (791685, 1)\n  Negatives shape: (791685, 4)\n</pre> <pre>Model: \"cbow_negative_sampling\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 cbow_embedding (Embedding)      \u2502 ?                      \u2502   0 (unbuilt) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 target_embedding (Embedding)    \u2502 ?                      \u2502   0 (unbuilt) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 0 (0.00 B)\n</pre> <pre> Trainable params: 0 (0.00 B)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[12]: Copied! <pre># Add callbacks\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='loss', patience=5, restore_best_weights=True\n)\n\n# Train the model\nprint(\"\\nTraining CBOW model with negative sampling...\")\nhistory = model.fit(\n    dataset,\n    epochs=1,\n    steps_per_epoch=1,\n    verbose=1,\n    callbacks=[early_stopping]\n)\n\ncbow_initial_embeddings = model.word_embedding.get_weights()[0]\n\nhistory = model.fit(\n    dataset,\n    epochs=20,\n    verbose=1,\n    callbacks=[early_stopping]\n)\n\ncbow_learned_embeddings = model.word_embedding.get_weights()[0]\n\n# Plot training history\nplt.figure(figsize=(10, 6))\nplt.plot(history.history['loss'])\nplt.title('CBOW with Negative Sampling - Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.show()\n</pre> # Add callbacks early_stopping = keras.callbacks.EarlyStopping(     monitor='loss', patience=5, restore_best_weights=True )  # Train the model print(\"\\nTraining CBOW model with negative sampling...\") history = model.fit(     dataset,     epochs=1,     steps_per_epoch=1,     verbose=1,     callbacks=[early_stopping] )  cbow_initial_embeddings = model.word_embedding.get_weights()[0]  history = model.fit(     dataset,     epochs=20,     verbose=1,     callbacks=[early_stopping] )  cbow_learned_embeddings = model.word_embedding.get_weights()[0]  # Plot training history plt.figure(figsize=(10, 6)) plt.plot(history.history['loss']) plt.title('CBOW with Negative Sampling - Training Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.grid(True) plt.show() <pre>\nTraining CBOW model with negative sampling...\n1/1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 390ms/step - loss: 0.6931\nEpoch 1/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.3169\nEpoch 2/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.2334\nEpoch 3/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.2063\nEpoch 4/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.1885\nEpoch 5/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1769\nEpoch 6/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1692\nEpoch 7/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1634\nEpoch 8/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1589\nEpoch 9/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1554\nEpoch 10/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1524\nEpoch 11/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.1499\nEpoch 12/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.1477\nEpoch 13/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.1457\nEpoch 14/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1441\nEpoch 15/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1425\nEpoch 16/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 2ms/step - loss: 0.1412\nEpoch 17/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.1400\nEpoch 18/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 4ms/step - loss: 0.1387\nEpoch 19/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.1376\nEpoch 20/20\n1547/1547 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 2ms/step - loss: 0.1368\n</pre> In\u00a0[13]: Copied! <pre># Find similar words using trained embeddings\ndef find_similar_words(word, embeddings, word2idx, idx2word, top_n=5):\n    \"\"\"Find most similar words based on cosine similarity\"\"\"\n    if word not in word2idx:\n        return []\n        \n    word_idx = word2idx[word]\n    word_vec = embeddings[word_idx]\n    \n    # Normalize vectors for cosine similarity\n    norm_word = np.linalg.norm(word_vec)\n    if norm_word &gt; 0:\n        word_vec = word_vec / norm_word\n    \n    # Normalize all embeddings\n    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n    norms = np.maximum(norms, 1e-10)  # Avoid division by zero\n    normalized_embeddings = embeddings / norms\n    \n    # Calculate similarities\n    similarities = np.dot(normalized_embeddings, word_vec)\n    \n    # Get top n similar words\n    most_similar = []\n    for idx in similarities.argsort()[-top_n-1:][::-1]:\n        if idx != word_idx:\n            most_similar.append((idx2word[idx], similarities[idx]))\n            \n    return most_similar\n\n# Test some example words\ntest_words = [\"king\", \"france\", \"computer\", \"dog\"]\nfor word in test_words:\n    if word in word2idx:\n        similar_words = find_similar_words(word, cbow_initial_embeddings, word2idx, idx2word)\n        print(f\"\\nWords similar to '{word} - initial embeddings':\")\n        for similar_word, similarity in similar_words:\n            print(f\"  {similar_word}: {similarity:.4f}\")\n        similar_words = find_similar_words(word, cbow_learned_embeddings, word2idx, idx2word)\n        print(f\"\\nWords similar to '{word} - learned embeddings':\")\n        for similar_word, similarity in similar_words:\n            print(f\"  {similar_word}: {similarity:.4f}\")\n        print('---')\n</pre> # Find similar words using trained embeddings def find_similar_words(word, embeddings, word2idx, idx2word, top_n=5):     \"\"\"Find most similar words based on cosine similarity\"\"\"     if word not in word2idx:         return []              word_idx = word2idx[word]     word_vec = embeddings[word_idx]          # Normalize vectors for cosine similarity     norm_word = np.linalg.norm(word_vec)     if norm_word &gt; 0:         word_vec = word_vec / norm_word          # Normalize all embeddings     norms = np.linalg.norm(embeddings, axis=1, keepdims=True)     norms = np.maximum(norms, 1e-10)  # Avoid division by zero     normalized_embeddings = embeddings / norms          # Calculate similarities     similarities = np.dot(normalized_embeddings, word_vec)          # Get top n similar words     most_similar = []     for idx in similarities.argsort()[-top_n-1:][::-1]:         if idx != word_idx:             most_similar.append((idx2word[idx], similarities[idx]))                  return most_similar  # Test some example words test_words = [\"king\", \"france\", \"computer\", \"dog\"] for word in test_words:     if word in word2idx:         similar_words = find_similar_words(word, cbow_initial_embeddings, word2idx, idx2word)         print(f\"\\nWords similar to '{word} - initial embeddings':\")         for similar_word, similarity in similar_words:             print(f\"  {similar_word}: {similarity:.4f}\")         similar_words = find_similar_words(word, cbow_learned_embeddings, word2idx, idx2word)         print(f\"\\nWords similar to '{word} - learned embeddings':\")         for similar_word, similarity in similar_words:             print(f\"  {similar_word}: {similarity:.4f}\")         print('---')  <pre>\nWords similar to 'king - initial embeddings':\n  allosaurus: 0.8045\n  tendencies: 0.7713\n  eighty: 0.7701\n  drew: 0.7664\n  asteraceae: 0.7544\n\nWords similar to 'king - learned embeddings':\n  marched: 0.8228\n  father: 0.8155\n  benedict: 0.8062\n  son: 0.8026\n  oliver: 0.7886\n---\n\nWords similar to 'france - initial embeddings':\n  kandahar: 0.7864\n  candidate: 0.7847\n  logically: 0.7753\n  educated: 0.7746\n  revisionism: 0.7686\n\nWords similar to 'france - learned embeddings':\n  merchants: 0.8654\n  youngest: 0.8219\n  salisbury: 0.7975\n  sailed: 0.7945\n  mauritania: 0.7905\n---\n\nWords similar to 'computer - initial embeddings':\n  imports: 0.8938\n  supplying: 0.8409\n  transatlantic: 0.8304\n  moved: 0.8095\n  followed: 0.7946\n\nWords similar to 'computer - learned embeddings':\n  anime: 0.8789\n  inventors: 0.8317\n  ary: 0.8098\n  animation: 0.7912\n  portable: 0.7875\n---\n\nWords similar to 'dog - initial embeddings':\n  critic: 0.8114\n  lift: 0.7804\n  near: 0.7781\n  postulated: 0.7679\n  immortal: 0.7678\n\nWords similar to 'dog - learned embeddings':\n  mario: 0.8316\n  lisa: 0.8145\n  dead: 0.7892\n  germans: 0.7813\n  mysteries: 0.7751\n---\n</pre> In\u00a0[19]: Copied! <pre>def find_similar_words(word, embedding_matrix, word2idx, idx2word, top_n=10):\n    \"\"\"\n    Find the most similar words to a given word using cosine similarity.\n    \n    Args:\n        word: Query word\n        embedding_matrix: Matrix of word embeddings\n        word2idx: Word to index mapping\n        idx2word: Index to word mapping\n        top_n: Number of similar words to return\n    \n    Returns:\n        List of (word, similarity) tuples\n    \"\"\"\n    if word not in word2idx:\n        return []\n    \n    word_idx = word2idx[word]\n    word_embedding = embedding_matrix[word_idx]\n    \n    # Calculate cosine similarity\n    word_embedding = word_embedding / np.linalg.norm(word_embedding)  # Normalize\n    similarities = np.dot(embedding_matrix, word_embedding) / (\n        np.linalg.norm(embedding_matrix, axis=1) + 1e-8)  # Avoid division by zero\n    \n    # Get top similar words (excluding the query word)\n    similar_indices = similarities.argsort()[-top_n-1:-1][::-1]\n    similar_words = [(idx2word[idx], similarities[idx]) for idx in similar_indices]\n    \n    return similar_words\n\ndef evaluate_word_similarity(test_words, embedding_matrices, names, word2idx, idx2word):\n    \"\"\"\n    Evaluate and compare word similarities across different embedding matrices.\n    \n    Args:\n        test_words: List of words to test\n        embedding_matrices: List of embedding matrices to compare\n        names: Names for each embedding matrix\n        word2idx, idx2word: Mappings between words and indices\n    \"\"\"\n    results = []\n    \n    for word in test_words:\n        if word not in word2idx:\n            print(f\"'{word}' not in vocabulary, skipping\")\n            continue\n            \n        word_results = {'word': word}\n        \n        for embedding_matrix, name in zip(embedding_matrices, names):\n            similar_words = find_similar_words(word, embedding_matrix, word2idx, idx2word)\n            word_results[name] = ', '.join([f\"{w} ({s:.2f})\" for w, s in similar_words[:5]])\n        \n        results.append(word_results)\n    \n    return pd.DataFrame(results)\n\ndef create_similarity_html(similarity_results):\n    html = \"&lt;table style='width:100%; border-collapse:collapse;'&gt;\"\n    \n    # Header row\n    html += \"&lt;tr style='background-color:#f2f2f2;'&gt;\"\n    for col in similarity_results.columns:\n        html += f\"&lt;th style='padding:8px; text-align:center; border:1px solid #ddd;'&gt;{col}&lt;/th&gt;\"\n    html += \"&lt;/tr&gt;\"\n    \n    # Data rows\n    for _, row in similarity_results.iterrows():\n        html += \"&lt;tr&gt;\"\n        for i, col in enumerate(similarity_results.columns):\n            value = row[col]\n            if i &gt; 0:  # For similarity columns\n                items = value.split(', ')\n                value = \"&lt;br&gt;\".join(items)\n            \n            html += f\"&lt;td style='padding:8px; border:1px solid #ddd; vertical-align:top;'&gt;{value}&lt;/td&gt;\"\n        html += \"&lt;/tr&gt;\"\n    \n    html += \"&lt;/table&gt;\"\n    return html\n\n\n# Test words for similarity\ntest_words = [\n    # Common nouns\n    \"king\", \"woman\", \"man\", \"queen\", \"child\", \n    # Countries and cities\n    \"france\", \"paris\", \"germany\", \"berlin\", \"london\",\n    # Verbs\n    \"walk\", \"run\", \"eat\", \"think\", \"play\",\n    # Adjectives\n    \"good\", \"bad\", \"beautiful\", \"strong\", \"happy\",\n    # Time-related\n    \"today\", \"yesterday\", \"year\", \"month\", \"week\"\n]\n\n# Compare initial vs trained embeddings\nsimilarity_results = evaluate_word_similarity(\n    test_words,\n    [skipgram_initial_embeddings, skipgram_learned_embeddings, cbow_initial_embeddings, cbow_learned_embeddings],\n    [\"Initial Random\", \"Skip-gram Trained\", \"CBOW Initial\", \"CBOW Trained\"],\n    word2idx, idx2word\n)\n\n# Display the results\nprint(\"Word Similarity Comparison:\")\n# Display the custom HTML table\nfrom IPython.display import HTML\ndisplay(HTML(create_similarity_html(similarity_results)))\n</pre> def find_similar_words(word, embedding_matrix, word2idx, idx2word, top_n=10):     \"\"\"     Find the most similar words to a given word using cosine similarity.          Args:         word: Query word         embedding_matrix: Matrix of word embeddings         word2idx: Word to index mapping         idx2word: Index to word mapping         top_n: Number of similar words to return          Returns:         List of (word, similarity) tuples     \"\"\"     if word not in word2idx:         return []          word_idx = word2idx[word]     word_embedding = embedding_matrix[word_idx]          # Calculate cosine similarity     word_embedding = word_embedding / np.linalg.norm(word_embedding)  # Normalize     similarities = np.dot(embedding_matrix, word_embedding) / (         np.linalg.norm(embedding_matrix, axis=1) + 1e-8)  # Avoid division by zero          # Get top similar words (excluding the query word)     similar_indices = similarities.argsort()[-top_n-1:-1][::-1]     similar_words = [(idx2word[idx], similarities[idx]) for idx in similar_indices]          return similar_words  def evaluate_word_similarity(test_words, embedding_matrices, names, word2idx, idx2word):     \"\"\"     Evaluate and compare word similarities across different embedding matrices.          Args:         test_words: List of words to test         embedding_matrices: List of embedding matrices to compare         names: Names for each embedding matrix         word2idx, idx2word: Mappings between words and indices     \"\"\"     results = []          for word in test_words:         if word not in word2idx:             print(f\"'{word}' not in vocabulary, skipping\")             continue                      word_results = {'word': word}                  for embedding_matrix, name in zip(embedding_matrices, names):             similar_words = find_similar_words(word, embedding_matrix, word2idx, idx2word)             word_results[name] = ', '.join([f\"{w} ({s:.2f})\" for w, s in similar_words[:5]])                  results.append(word_results)          return pd.DataFrame(results)  def create_similarity_html(similarity_results):     html = \"\"          # Header row     html += \"\"     for col in similarity_results.columns:         html += f\"{col}\"     html += \"\"          # Data rows     for _, row in similarity_results.iterrows():         html += \"\"         for i, col in enumerate(similarity_results.columns):             value = row[col]             if i &gt; 0:  # For similarity columns                 items = value.split(', ')                 value = \"\".join(items)                          html += f\"{value}\"         html += \"\"          html += \"\"     return html   # Test words for similarity test_words = [     # Common nouns     \"king\", \"woman\", \"man\", \"queen\", \"child\",      # Countries and cities     \"france\", \"paris\", \"germany\", \"berlin\", \"london\",     # Verbs     \"walk\", \"run\", \"eat\", \"think\", \"play\",     # Adjectives     \"good\", \"bad\", \"beautiful\", \"strong\", \"happy\",     # Time-related     \"today\", \"yesterday\", \"year\", \"month\", \"week\" ]  # Compare initial vs trained embeddings similarity_results = evaluate_word_similarity(     test_words,     [skipgram_initial_embeddings, skipgram_learned_embeddings, cbow_initial_embeddings, cbow_learned_embeddings],     [\"Initial Random\", \"Skip-gram Trained\", \"CBOW Initial\", \"CBOW Trained\"],     word2idx, idx2word )  # Display the results print(\"Word Similarity Comparison:\") # Display the custom HTML table from IPython.display import HTML display(HTML(create_similarity_html(similarity_results))) <pre>'yesterday' not in vocabulary, skipping\nWord Similarity Comparison:\n</pre> wordInitial RandomSkip-gram TrainedCBOW InitialCBOW Trainedkingsemantic (0.78)atheistic (0.76)commentator (0.75)pot (0.73)advanced (0.73)bishop (0.89)john (0.86)res (0.85)edward (0.84)henry (0.84)allosaurus (0.80)tendencies (0.77)eighty (0.77)drew (0.77)asteraceae (0.75)marched (0.82)father (0.82)benedict (0.81)son (0.80)oliver (0.79)womanhermes (0.89)shores (0.81)halogens (0.77)positively (0.77)antecedent (0.75)surprised (0.82)love (0.82)eros (0.80)blocked (0.80)her (0.80)qur (0.81)issues (0.80)voltage (0.76)sickness (0.76)rent (0.74)ptolemy (0.84)eduard (0.80)love (0.80)me (0.79)eloquence (0.78)manperforming (0.84)navy (0.80)increase (0.78)mass (0.76)word (0.76)hatred (0.84)coming (0.82)contribution (0.81)seriously (0.79)ill (0.79)kj (0.79)posted (0.78)zur (0.76)brilliant (0.75)acknowledge (0.74)get (0.89)battles (0.83)strict (0.82)walks (0.79)teacher (0.79)queengrew (0.83)allegedly (0.82)mnd (0.82)subjects (0.79)outer (0.77)captain (0.82)nicholas (0.82)camp (0.81)paris (0.80)young (0.79)nontheism (0.76)colonial (0.76)limits (0.76)spaced (0.74)decimal (0.73)ulysses (0.88)william (0.84)oliver (0.82)joseph (0.82)pierce (0.82)childsd (0.78)uzala (0.74)surrendered (0.74)whole (0.74)discovery (0.73)herself (0.85)taxes (0.82)torn (0.82)messages (0.80)warned (0.78)real (0.83)school (0.82)sent (0.76)temporary (0.75)choir (0.74)eschatological (0.83)invented (0.82)olympias (0.82)objections (0.81)ernst (0.79)franceestate (0.83)optional (0.77)carried (0.77)ufos (0.76)serious (0.75)serbia (0.83)baptiste (0.82)clifford (0.81)de (0.80)italy (0.79)kandahar (0.79)candidate (0.78)logically (0.78)educated (0.77)revisionism (0.77)merchants (0.87)youngest (0.82)salisbury (0.80)sailed (0.79)mauritania (0.79)parishandling (0.83)languages (0.77)share (0.76)vertigo (0.76)program (0.75)illinois (0.85)founded (0.84)benedict (0.83)julius (0.83)rhode (0.83)gravitation (0.84)defeats (0.81)collectively (0.79)to (0.79)defines (0.77)illinois (0.87)sociologist (0.85)statue (0.82)century (0.82)chicago (0.82)germanysclerosis (0.80)synthesized (0.76)tree (0.76)pearl (0.75)readily (0.74)harbor (0.82)occupied (0.81)tent (0.80)local (0.79)navajo (0.79)athenians (0.79)getting (0.79)hydraulic (0.78)bourgeois (0.77)ambition (0.77)louisiana (0.83)virginia (0.83)border (0.82)crashes (0.82)marched (0.82)berlinunpopular (0.80)memory (0.77)phylogenetic (0.76)certainly (0.73)procedure (0.73)weimar (0.87)publication (0.81)das (0.79)norway (0.78)liberation (0.78)smithsonian (0.78)launched (0.76)subdivision (0.74)councils (0.74)alternatives (0.74)corinth (0.81)commemoration (0.81)rating (0.81)administration (0.81)tribune (0.80)londonformulas (0.83)tactic (0.76)coincide (0.75)derleth (0.75)remained (0.75)august (0.90)shortly (0.86)chapter (0.83)ad (0.82)lightfoot (0.81)brakes (0.82)ken (0.79)aerospace (0.77)meditation (0.77)executed (0.76)thomson (0.89)maine (0.86)slams (0.82)gettysburg (0.81)tony (0.81)walkquarter (0.81)moral (0.77)animator (0.77)create (0.75)couples (0.75)probable (0.84)wishing (0.81)servants (0.80)men (0.79)temporal (0.79)simpsons (0.80)sink (0.78)supreme (0.77)bligh (0.75)europeans (0.74)abbey (0.91)federation (0.82)intellectuals (0.82)chaos (0.82)wiesenthal (0.81)runauthor (0.92)politically (0.87)richardson (0.83)logan (0.81)sparsely (0.78)greatly (0.91)summit (0.89)low (0.81)stores (0.79)aisles (0.78)minds (0.78)winnie (0.75)motions (0.75)faire (0.74)metro (0.74)goto (0.82)camera (0.81)car (0.81)game (0.80)mtv (0.80)eatsimilarly (0.84)uruguay (0.82)musician (0.80)proudhon (0.79)youthful (0.75)arise (0.84)coke (0.79)pleas (0.78)mind (0.77)mel (0.77)rectus (0.79)lieutenant (0.76)exchanged (0.76)abbreviated (0.75)asceticism (0.75)distilled (0.83)leaf (0.81)progresses (0.81)disbelief (0.80)spiritual (0.79)thinkarches (0.83)joint (0.76)compilation (0.75)going (0.74)academy (0.73)anyone (0.87)ghetto (0.85)shocked (0.82)prove (0.81)dispute (0.81)definitive (0.84)leaving (0.81)opposite (0.81)survives (0.77)narrative (0.76)parade (0.87)nothing (0.83)expression (0.82)stirner (0.82)lorenzo (0.80)playorange (0.82)crowd (0.78)problematic (0.76)sentiments (0.76)negative (0.75)hour (0.85)drivers (0.84)event (0.83)meant (0.80)shift (0.79)flying (0.83)radio (0.80)addressed (0.78)faa (0.78)hydride (0.77)trucks (0.89)singles (0.88)match (0.86)climb (0.79)step (0.78)goodcollections (0.77)ecology (0.77)coloured (0.77)cartoons (0.75)pdas (0.74)effectively (0.88)without (0.86)act (0.83)use (0.81)termed (0.80)recommend (0.77)joachim (0.76)niger (0.75)emigration (0.74)wrongly (0.73)measure (0.77)eros (0.77)want (0.77)trouble (0.76)recognise (0.76)badmathbf (0.81)circa (0.78)urgell (0.77)blocking (0.76)bruno (0.75)aware (0.88)architectures (0.83)faithful (0.83)consequences (0.82)farmer (0.82)pigs (0.85)calculations (0.82)akm (0.80)papyrus (0.76)visiting (0.76)dvd (0.81)voltage (0.79)animator (0.78)keen (0.77)desire (0.75)beautifulreturned (0.79)thousand (0.78)photon (0.74)scientists (0.73)beta (0.73)devon (0.85)fine (0.85)turned (0.85)beneath (0.83)glands (0.82)anatoly (0.81)charleston (0.78)shores (0.75)consisting (0.73)sketch (0.73)stretching (0.83)unnecessary (0.81)resolve (0.78)crushed (0.77)active (0.77)strongthreatening (0.85)therein (0.80)virulent (0.79)investors (0.76)exceptionally (0.76)case (0.86)nuclear (0.86)fisherman (0.83)power (0.83)seabed (0.82)naval (0.84)princeps (0.82)object (0.78)dramatic (0.78)cow (0.77)binding (0.87)agreements (0.86)aikidoka (0.83)hearing (0.82)autistic (0.80)happymouch (0.79)derived (0.78)eumc (0.77)album (0.76)euro (0.76)burnside (0.87)missouri (0.83)hitler (0.82)permitted (0.82)colorado (0.82)privileges (0.85)bi (0.81)satisfies (0.79)balloon (0.79)historians (0.77)sanctioned (0.80)resumed (0.80)zurich (0.79)rape (0.79)myself (0.77)todaymarx (0.76)reverence (0.76)invisible (0.76)comics (0.74)commentary (0.72)some (0.85)well (0.85)benzene (0.85)ie (0.84)thought (0.84)recorded (0.88)timothy (0.83)maj (0.79)appreciate (0.76)automorphism (0.76)sahrawi (0.82)loosely (0.79)austroasiatic (0.78)valid (0.78)inconclusive (0.75)yearbuilding (0.80)great (0.79)electromagnetism (0.77)typography (0.74)director (0.74)next (0.87)round (0.84)completed (0.83)per (0.82)september (0.82)ladder (0.85)hypothetical (0.82)intuitive (0.81)revival (0.77)akshara (0.77)days (0.84)propagation (0.80)mild (0.80)acquiring (0.80)minutes (0.79)monthmotive (0.78)tells (0.77)abortion (0.72)expressions (0.72)fullback (0.71)breach (0.86)hall (0.85)exceeded (0.83)wavelength (0.82)mining (0.80)creating (0.87)gide (0.86)routledge (0.83)formerly (0.77)iii (0.76)anatoly (0.83)brings (0.79)banquet (0.78)mali (0.76)hits (0.74)weekoverall (0.83)beating (0.82)lucius (0.78)rare (0.76)waiting (0.75)successes (0.80)celebrating (0.78)month (0.76)flood (0.76)headquartered (0.76)convince (0.80)brooke (0.77)famous (0.76)invasions (0.76)pamphlet (0.76)assessments (0.86)passion (0.84)fascination (0.84)marries (0.81)masters (0.80) In\u00a0[20]: Copied! <pre>def word_analogy(word1, word2, word3, embedding_matrix, word2idx, idx2word, top_n=5):\n    \"\"\"\n    Solve word analogies like \"king - man + woman = queen\".\n    \n    Args:\n        word1, word2, word3: Words in the analogy \"word1 - word2 + word3 = ?\"\n        embedding_matrix: Matrix of word embeddings\n        word2idx: Word to index mapping\n        idx2word: Index to word mapping\n        top_n: Number of results to return\n    \n    Returns:\n        List of (word, similarity) tuples\n    \"\"\"\n    for word in [word1, word2, word3]:\n        if word not in word2idx:\n            print(f\"Word '{word}' not in vocabulary\")\n            return []\n    \n    # Get word vectors\n    vec1 = embedding_matrix[word2idx[word1]]\n    vec2 = embedding_matrix[word2idx[word2]]\n    vec3 = embedding_matrix[word2idx[word3]]\n    \n    # Calculate target vector: vec1 - vec2 + vec3\n    target_vec = vec1 - vec2 + vec3\n    target_vec = target_vec / np.linalg.norm(target_vec)  # Normalize\n    \n    # Calculate similarities\n    similarities = np.dot(embedding_matrix, target_vec) / (\n        np.linalg.norm(embedding_matrix, axis=1) + 1e-8)\n    \n    # Get top results (excluding input words)\n    results = []\n    sorted_indices = similarities.argsort()[::-1]\n    \n    for idx in sorted_indices:\n        word = idx2word[idx]\n        if word != word1 and word != word2 and word != word3:\n            results.append((word, similarities[idx]))\n            if len(results) &gt;= top_n:\n                break\n    \n    return results\n\ndef evaluate_word_analogies(analogies, embedding_matrices, names, word2idx, idx2word):\n    \"\"\"\n    Evaluate word analogies across different embedding matrices.\n    \n    Args:\n        analogies: List of (word1, word2, word3, expected) tuples\n        embedding_matrices: List of embedding matrices to compare\n        names: Names for each embedding matrix\n        word2idx, idx2word: Mappings between words and indices\n    \"\"\"\n    results = []\n    \n    for word1, word2, word3, expected in analogies:\n        analogy_str = f\"{word1} - {word2} + {word3} = ?\"\n        result = {'analogy': analogy_str, 'expected': expected}\n        \n        for embedding_matrix, name in zip(embedding_matrices, names):\n            # Skip initial random embeddings for analogies (they won't be meaningful)\n            if 'Initial' in name:\n                result[name] = 'N/A'\n                continue\n                \n            answers = word_analogy(word1, word2, word3, embedding_matrix, word2idx, idx2word)\n            \n            if answers:\n                # Check if expected word is in top results\n                expected_in_top = any(word == expected for word, _ in answers)\n                top_answers = ', '.join([f\"{w} ({s:.2f})\" for w, s in answers[:3]])\n                result[name] = top_answers + (f\" \u2713\" if expected_in_top else \"\")\n            else:\n                result[name] = \"Words not in vocabulary\"\n        \n        results.append(result)\n    \n    return pd.DataFrame(results)\n\n# Test analogies\nanalogies = [\n    # Capital cities\n    (\"france\", \"paris\", \"italy\", \"rome\"),\n    (\"germany\", \"berlin\", \"england\", \"london\"),\n    # Gender relationships\n    (\"king\", \"man\", \"woman\", \"queen\"),\n    (\"man\", \"he\", \"she\", \"woman\"),\n    # Verb tenses\n    (\"walking\", \"walk\", \"run\", \"running\"),\n    (\"good\", \"better\", \"bad\", \"worse\"),\n    # Semantic relationships\n    (\"small\", \"smaller\", \"big\", \"bigger\"),\n    (\"eat\", \"food\", \"drink\", \"water\"),\n    # Comparative and superlative\n    (\"good\", \"best\", \"bad\", \"worst\"),\n    (\"big\", \"biggest\", \"small\", \"smallest\")\n]\n\n# Evaluate analogies on trained embeddings\nanalogy_results = evaluate_word_analogies(\n    analogies,\n    [skipgram_learned_embeddings, cbow_learned_embeddings],\n    [\"Skip-gram\", \"CBOW\"],\n    word2idx, idx2word\n)\n\n# Display the results\nprint(\"Word Analogy Evaluation:\")\ndisplay(analogy_results)\n</pre> def word_analogy(word1, word2, word3, embedding_matrix, word2idx, idx2word, top_n=5):     \"\"\"     Solve word analogies like \"king - man + woman = queen\".          Args:         word1, word2, word3: Words in the analogy \"word1 - word2 + word3 = ?\"         embedding_matrix: Matrix of word embeddings         word2idx: Word to index mapping         idx2word: Index to word mapping         top_n: Number of results to return          Returns:         List of (word, similarity) tuples     \"\"\"     for word in [word1, word2, word3]:         if word not in word2idx:             print(f\"Word '{word}' not in vocabulary\")             return []          # Get word vectors     vec1 = embedding_matrix[word2idx[word1]]     vec2 = embedding_matrix[word2idx[word2]]     vec3 = embedding_matrix[word2idx[word3]]          # Calculate target vector: vec1 - vec2 + vec3     target_vec = vec1 - vec2 + vec3     target_vec = target_vec / np.linalg.norm(target_vec)  # Normalize          # Calculate similarities     similarities = np.dot(embedding_matrix, target_vec) / (         np.linalg.norm(embedding_matrix, axis=1) + 1e-8)          # Get top results (excluding input words)     results = []     sorted_indices = similarities.argsort()[::-1]          for idx in sorted_indices:         word = idx2word[idx]         if word != word1 and word != word2 and word != word3:             results.append((word, similarities[idx]))             if len(results) &gt;= top_n:                 break          return results  def evaluate_word_analogies(analogies, embedding_matrices, names, word2idx, idx2word):     \"\"\"     Evaluate word analogies across different embedding matrices.          Args:         analogies: List of (word1, word2, word3, expected) tuples         embedding_matrices: List of embedding matrices to compare         names: Names for each embedding matrix         word2idx, idx2word: Mappings between words and indices     \"\"\"     results = []          for word1, word2, word3, expected in analogies:         analogy_str = f\"{word1} - {word2} + {word3} = ?\"         result = {'analogy': analogy_str, 'expected': expected}                  for embedding_matrix, name in zip(embedding_matrices, names):             # Skip initial random embeddings for analogies (they won't be meaningful)             if 'Initial' in name:                 result[name] = 'N/A'                 continue                              answers = word_analogy(word1, word2, word3, embedding_matrix, word2idx, idx2word)                          if answers:                 # Check if expected word is in top results                 expected_in_top = any(word == expected for word, _ in answers)                 top_answers = ', '.join([f\"{w} ({s:.2f})\" for w, s in answers[:3]])                 result[name] = top_answers + (f\" \u2713\" if expected_in_top else \"\")             else:                 result[name] = \"Words not in vocabulary\"                  results.append(result)          return pd.DataFrame(results)  # Test analogies analogies = [     # Capital cities     (\"france\", \"paris\", \"italy\", \"rome\"),     (\"germany\", \"berlin\", \"england\", \"london\"),     # Gender relationships     (\"king\", \"man\", \"woman\", \"queen\"),     (\"man\", \"he\", \"she\", \"woman\"),     # Verb tenses     (\"walking\", \"walk\", \"run\", \"running\"),     (\"good\", \"better\", \"bad\", \"worse\"),     # Semantic relationships     (\"small\", \"smaller\", \"big\", \"bigger\"),     (\"eat\", \"food\", \"drink\", \"water\"),     # Comparative and superlative     (\"good\", \"best\", \"bad\", \"worst\"),     (\"big\", \"biggest\", \"small\", \"smallest\") ]  # Evaluate analogies on trained embeddings analogy_results = evaluate_word_analogies(     analogies,     [skipgram_learned_embeddings, cbow_learned_embeddings],     [\"Skip-gram\", \"CBOW\"],     word2idx, idx2word )  # Display the results print(\"Word Analogy Evaluation:\") display(analogy_results) <pre>Word Analogy Evaluation:\n</pre> analogy expected Skip-gram CBOW 0 france - paris + italy = ? rome emirates (0.80), expelled (0.77), farmers (0.76) rescued (0.85), ruler (0.83), motto (0.83) 1 germany - berlin + england = ? london constantinople (0.81), ornaments (0.80), until... tent (0.81), then (0.80), nile (0.79) 2 king - man + woman = ? queen henry (0.83), connecticut (0.82), signer (0.80) rider (0.84), satrap (0.83), martyr (0.79) 3 man - he + she = ? woman aphrodite (0.86), loved (0.86), adonis (0.84) get (0.82), sura (0.77), going (0.76) 4 walking - walk + run = ? running indicator (0.74), slopes (0.70), photos (0.69) autobiographical (0.69), filmed (0.66), kalash... 5 good - better + bad = ? worse architectures (0.92), maia (0.86), addicted (0... obsession (0.78), gibson (0.77), chair (0.77) 6 small - smaller + big = ? bigger fourth (0.79), couple (0.79), fly (0.78) stewart (0.82), leopold (0.80), clarke (0.78) 7 eat - food + drink = ? water drinking (0.82), coke (0.80), lead (0.78) gothic (0.89), blockers (0.83), composers (0.82) 8 good - best + bad = ? worst tired (0.85), admiration (0.84), entity (0.81) cease (0.84), glorious (0.84), altruist (0.83) 9 big - biggest + small = ? smallest long (0.86), photon (0.80), aminoacids (0.79) recounts (0.82), christie (0.78), granite (0.71) In\u00a0[21]: Copied! <pre>def plot_semantic_clusters(words_by_category, embedding_matrix, word2idx, title):\n    \"\"\"\n    Plot words from different semantic categories in 2D space.\n    \n    Args:\n        words_by_category: Dictionary mapping categories to lists of words\n        embedding_matrix: Word embedding matrix\n        word2idx: Word to index mapping\n        title: Plot title\n    \"\"\"\n    from sklearn.manifold import TSNE\n    import matplotlib.colors as mcolors\n    \n    # Colors for different categories\n    colors = list(mcolors.TABLEAU_COLORS)\n    \n    # Collect embeddings and labels for words that are in vocabulary\n    embeddings = []\n    labels = []\n    categories = []\n    \n    for category, words in words_by_category.items():\n        for word in words:\n            if word in word2idx:\n                embeddings.append(embedding_matrix[word2idx[word]])\n                labels.append(word)\n                categories.append(category)\n    \n    if not embeddings:\n        print(\"No words found in vocabulary\")\n        return\n    \n    # Convert to array\n    embeddings = np.array(embeddings)\n    \n    # Use t-SNE to reduce to 2 dimensions\n    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n    reduced_embeddings = tsne.fit_transform(embeddings)\n    \n    # Plot\n    plt.figure(figsize=(12, 8))\n    category_handles = []\n    \n    # Get unique categories and sort them\n    unique_categories = sorted(set(categories))\n    \n    for i, category in enumerate(unique_categories):\n        color = colors[i % len(colors)]\n        indices = [j for j, cat in enumerate(categories) if cat == category]\n        \n        scatter = plt.scatter(\n            reduced_embeddings[indices, 0],\n            reduced_embeddings[indices, 1],\n            label=category,\n            color=color,\n            alpha=0.7\n        )\n        category_handles.append(scatter)\n        \n        # Add labels for each point\n        for j in indices:\n            plt.annotate(\n                labels[j],\n                (reduced_embeddings[j, 0], reduced_embeddings[j, 1]),\n                xytext=(5, 2),\n                textcoords='offset points',\n                fontsize=9\n            )\n    \n    plt.title(title)\n    plt.legend(handles=category_handles, loc='best')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.show()\n\n# Define semantic categories for testing\nsemantic_categories = {\n    'Countries': ['france', 'germany', 'italy', 'spain', 'england', 'russia', 'china', 'japan', 'canada', 'australia'],\n    'Cities': ['paris', 'berlin', 'rome', 'madrid', 'london', 'moscow', 'beijing', 'tokyo', 'toronto', 'sydney'],\n    'Numbers': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'],\n    'Family': ['father', 'mother', 'son', 'daughter', 'brother', 'sister', 'uncle', 'aunt', 'grandfather', 'grandmother'],\n    'Colors': ['red', 'blue', 'green', 'yellow', 'black', 'white', 'purple', 'orange', 'brown', 'pink'],\n    'Transportation': ['car', 'bus', 'train', 'plane', 'boat', 'bicycle', 'truck', 'motorcycle', 'ship', 'helicopter']\n}\n\n# Plot for initial vs trained embeddings\nprint(\"Initial Random Embeddings Clustering:\")\nplot_semantic_clusters(semantic_categories, skipgram_initial_embeddings, word2idx, \"Initial Random Embeddings - Semantic Categories\")\n\nprint(\"\\nSkip-gram Trained Embeddings Clustering:\")\nplot_semantic_clusters(semantic_categories, skipgram_learned_embeddings, word2idx, \"Skip-gram Trained Embeddings - Semantic Categories\")\n\nprint(\"\\nCBOW Trained Embeddings Clustering:\")\nplot_semantic_clusters(semantic_categories, cbow_learned_embeddings, word2idx, \"CBOW Trained Embeddings - Semantic Categories\")\n</pre> def plot_semantic_clusters(words_by_category, embedding_matrix, word2idx, title):     \"\"\"     Plot words from different semantic categories in 2D space.          Args:         words_by_category: Dictionary mapping categories to lists of words         embedding_matrix: Word embedding matrix         word2idx: Word to index mapping         title: Plot title     \"\"\"     from sklearn.manifold import TSNE     import matplotlib.colors as mcolors          # Colors for different categories     colors = list(mcolors.TABLEAU_COLORS)          # Collect embeddings and labels for words that are in vocabulary     embeddings = []     labels = []     categories = []          for category, words in words_by_category.items():         for word in words:             if word in word2idx:                 embeddings.append(embedding_matrix[word2idx[word]])                 labels.append(word)                 categories.append(category)          if not embeddings:         print(\"No words found in vocabulary\")         return          # Convert to array     embeddings = np.array(embeddings)          # Use t-SNE to reduce to 2 dimensions     tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))     reduced_embeddings = tsne.fit_transform(embeddings)          # Plot     plt.figure(figsize=(12, 8))     category_handles = []          # Get unique categories and sort them     unique_categories = sorted(set(categories))          for i, category in enumerate(unique_categories):         color = colors[i % len(colors)]         indices = [j for j, cat in enumerate(categories) if cat == category]                  scatter = plt.scatter(             reduced_embeddings[indices, 0],             reduced_embeddings[indices, 1],             label=category,             color=color,             alpha=0.7         )         category_handles.append(scatter)                  # Add labels for each point         for j in indices:             plt.annotate(                 labels[j],                 (reduced_embeddings[j, 0], reduced_embeddings[j, 1]),                 xytext=(5, 2),                 textcoords='offset points',                 fontsize=9             )          plt.title(title)     plt.legend(handles=category_handles, loc='best')     plt.grid(True, linestyle='--', alpha=0.7)     plt.show()  # Define semantic categories for testing semantic_categories = {     'Countries': ['france', 'germany', 'italy', 'spain', 'england', 'russia', 'china', 'japan', 'canada', 'australia'],     'Cities': ['paris', 'berlin', 'rome', 'madrid', 'london', 'moscow', 'beijing', 'tokyo', 'toronto', 'sydney'],     'Numbers': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'],     'Family': ['father', 'mother', 'son', 'daughter', 'brother', 'sister', 'uncle', 'aunt', 'grandfather', 'grandmother'],     'Colors': ['red', 'blue', 'green', 'yellow', 'black', 'white', 'purple', 'orange', 'brown', 'pink'],     'Transportation': ['car', 'bus', 'train', 'plane', 'boat', 'bicycle', 'truck', 'motorcycle', 'ship', 'helicopter'] }  # Plot for initial vs trained embeddings print(\"Initial Random Embeddings Clustering:\") plot_semantic_clusters(semantic_categories, skipgram_initial_embeddings, word2idx, \"Initial Random Embeddings - Semantic Categories\")  print(\"\\nSkip-gram Trained Embeddings Clustering:\") plot_semantic_clusters(semantic_categories, skipgram_learned_embeddings, word2idx, \"Skip-gram Trained Embeddings - Semantic Categories\")  print(\"\\nCBOW Trained Embeddings Clustering:\") plot_semantic_clusters(semantic_categories, cbow_learned_embeddings, word2idx, \"CBOW Trained Embeddings - Semantic Categories\") <pre>Initial Random Embeddings Clustering:\n</pre> <pre>\nSkip-gram Trained Embeddings Clustering:\n</pre> <pre>\nCBOW Trained Embeddings Clustering:\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#word2vec-implementation-from-scratch","title":"Word2Vec Implementation from Scratch\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#table-of-contents","title":"\ud83d\udcda Table of Contents\u00b6","text":"<ol> <li>Introduction</li> <li>Data Preparation</li> <li>Creating Training Data</li> <li>Skip Gram with Negative Sampling</li> <li>CBOW with Negative Sampling</li> <li>Intrinsic Evaluation</li> </ol> <p>Each section lives in its own notebook and links back here when needed. Let\u2019s dive in!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#1-introduction-to-word2vec","title":"1. Introduction to Word2Vec\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#11-what-are-word-embeddings","title":"1.1 \ud83d\udd24 What Are Word Embeddings?\u00b6","text":"<p>Word embeddings are a way to represent words as dense vectors in a continuous space, where semantically similar words are close together.</p> <p>Unlike one-hot encoding (where each word is just a binary blip in a huge sparse space), embeddings capture meaning and relationships between words.</p> <p>Key features:</p> <ul> <li>Dense vectors: Typically 50-300 dimensions (we'll use 16 for faster training)</li> <li>Learned from data: Capture semantic relationships automatically</li> <li>Contextual similarity: Words used in similar contexts have similar vectors</li> <li>Algebraic properties: Support meaningful vector operations (e.g., <code>king - man + woman \u2248 queen</code>)</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#12-word2vec-architectures","title":"1.2 \ud83e\udde0 Word2Vec Architectures\u00b6","text":"<p>Word2Vec (Mikolov et al., 2013) introduced two neural models for learning these embeddings: Skip-gram and CBOW. Let's break them down:</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#skip-gram","title":"\ud83d\udd01 Skip-gram\u00b6","text":"<ul> <li>Objective: Predict context words from the target word</li> <li>Input: A single target word</li> <li>Output: Probability distribution over possible context words</li> <li>Training pairs: (target_word, context_word)</li> <li>Strengths:<ul> <li>Performs well with small training datasets</li> <li>Better at representing rare words</li> <li>Can capture multiple meanings per word</li> </ul> </li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#cbow-continuous-bag-of-words","title":"\ud83e\uddfa CBOW (Continuous Bag of Words)\u00b6","text":"<ul> <li>Objective: Predict target word from context words</li> <li>Input: Multiple context words (averaged together)</li> <li>Output: Probability distribution over possible target words</li> <li>Training pairs: (context_words, target_word)</li> <li>Strengths:<ul> <li>Trains faster than Skip-gram</li> <li>Slightly better accuracy for frequent words</li> <li>Smoother word representations</li> </ul> </li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#13-why-build-word2vec-ourselves","title":"1.3 \ud83e\udd14 Why Build Word2Vec Ourselves?\u00b6","text":"<p>While pre-trained embeddings like GloVe or Word2Vec are readily available, building Word2Vec from scratch offers several benefits:</p> <ol> <li>Understanding the mechanics: See exactly how embeddings arise from word co-occurrence patterns</li> <li>Customization: Tailor the architecture and hyperparameters to specific needs</li> <li>Domain adaptation: Train on domain-specific text for specialized embeddings</li> <li>Educational value: Gain insights into neural network training and optimization</li> </ol> <p>In the following notebooks, we'll implement both Skip-gram and CBOW architectures and evaluate their performance on various intrinsic tasks. We'll use a smaller embedding dimension (16 instead of the typical 300) to speed up training while still capturing meaningful semantic relationships.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#2-preparing-the-data","title":"2. \ud83e\uddf9 Preparing the Data\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#21-loading-and-preprocessing-text","title":"2.1 \ud83d\udcdd Loading and Preprocessing Text\u00b6","text":"<p>The quality of word embeddings depends heavily on the quality and quantity of training data. For this implementation, we're using the Text8 dataset, which contains cleaned Wikipedia text.</p> <p>The preprocessing steps are crucial:</p> <ol> <li>Tokenization: Split the text into words</li> <li>Case normalization: Convert all text to lowercase (already done in Text8)</li> <li>Special character handling: Remove or replace special characters</li> <li>Handling rare words: Decide how to treat infrequent terms</li> </ol> <p>Let's examine our dataset and perform the necessary preprocessing:</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#22-building-the-vocabulary","title":"2.2 \ud83e\udde0 Building the Vocabulary\u00b6","text":"<p>Natural language isn't fair \u2014 a few words get used a lot, and most words barely show up. This phenomenon is known as Zipf\u2019s Law: word frequency follows a power-law distribution.</p> <p>\ud83d\udd0d You can see this in the plot above \u2014 a small number of words (like \"the\", \"and\", \"of\") dominate, while the majority appear rarely.</p> <p>This skewed distribution creates a few challenges:</p> <ol> <li>Common words flood the training process but don\u2019t add much semantic insight</li> <li>Rare words don\u2019t appear enough to learn meaningful embeddings</li> <li>A huge vocabulary makes training slower and memory-heavy</li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#our-strategy","title":"\ud83d\udee0\ufe0f Our Strategy\u00b6","text":"<p>To make training more efficient and meaningful, we\u2019ll:</p> <ul> <li>\ud83d\udeab Remove very rare words \u2014 filter out those below a minimum frequency</li> <li>\ud83e\uddf9 (Optionally) Subsample very frequent words \u2014 reduce their impact during training</li> <li>\ud83d\udd22 Build a fast lookup table \u2014 map each word to a unique index for efficient computation</li> </ul> <p>With this cleaned-up vocabulary, we\u2019re ready to turn raw text into training-ready data!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#23-filtering-the-vocabulary","title":"2.3 \ud83d\udd0d Filtering the Vocabulary\u00b6","text":"<p>To keep things manageable, we\u2019ve set a minimum frequency threshold of 5 \u2014 meaning any word that appears fewer than 5 times gets filtered out.</p> <p>This has a big impact: our vocabulary size drops significantly, but we still keep good coverage of the dataset.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#why-this-matters","title":"\u2696\ufe0f Why This Matters\u00b6","text":"<p>There\u2019s a tradeoff here:</p> <ul> <li>Higher threshold \u2192 Smaller vocab = faster training, but more \"unknown\" words</li> <li>Lower threshold \u2192 Better coverage, but slower training and noisy embeddings for rare words</li> </ul> <p>In real-world applications, thresholds like 20\u2013100 are common. But for our learning-focused setup, 5 hits a nice sweet spot.</p> <p>\ud83d\udcca Even with this filter, our vocabulary still covers 90%+ of the text \u2014 proof that a small set of words carries most of the meaning!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#24-creating-word-mappings","title":"2.4 \ud83d\uddc2\ufe0f Creating Word Mappings\u00b6","text":"<p>After filtering, we\u2019ve reduced the vocabulary from 47,046 words down to 12,515 \u2014 that\u2019s a ~73% reduction!</p> <p>Yet we\u2019ve still retained 93.47% of the total words in the text (791,689 out of 846,987). \ud83c\udfaf This shows just how concentrated language usage is \u2014 most of the meaning comes from a relatively small set of words.</p> <p>To work with our Word2Vec model, we now need two key mappings:</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#word2idx","title":"\ud83d\udd22 <code>word2idx</code>\u00b6","text":"<p>Maps each word to a unique integer index \u2705 Used to turn words into numbers for training \u2705 Enables fast lookup during model computation</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#idx2word","title":"\ud83d\udd01 <code>idx2word</code>\u00b6","text":"<p>Maps each index back to the original word \u2705 Useful for inspecting predictions and visualizations \u2705 Helps convert model outputs into readable text</p> <p>Why this matters:</p> <ul> <li>Vocabulary size determines the size of the model's output layer</li> <li>It directly affects training speed, memory usage, and overall efficiency</li> </ul> <p>We'll use these mappings to generate training pairs for both the Skip-gram and CBOW architectures.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#3-creating-training-examples","title":"3. \ud83c\udfd7\ufe0f Creating Training Examples\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#31-understanding-the-context-window","title":"3.1 \ud83c\udfaf Understanding the Context Window\u00b6","text":"<p>Word2Vec learns meaning from context \u2014 the idea is simple:</p> <p>Words that appear in similar surroundings tend to have similar meanings.</p> <p>To capture this, we use a sliding context window to extract training pairs from our text.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#how-it-works","title":"\ud83e\uddf0 How it works:\u00b6","text":"<ol> <li>For each word in the corpus (the target word)</li> <li>We look at a window of surrounding words (the context words)</li> <li>The window size controls how many words before and after are included</li> <li>For every target word, we pair it with each context word within that window</li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#example-window-size-2","title":"\ud83d\udccf Example (Window Size = 2)\u00b6","text":"<p>Text: <code>\"the cat sat on the mat\"</code> Target word: <code>\"sat\"</code> Context window: <code>[\"the\", \"cat\", \"on\", \"the\"]</code> Training pairs: (\"sat\", \"the\") (\"sat\", \"cat\") (\"sat\", \"on\") (\"sat\", \"the\")</p> <p>We\u2019ll generate these pairs for every word in our (filtered) corpus. This is the training data that powers both Skip-gram and CBOW models.</p> <p>\ud83d\udc40 Up next: we\u2019ll implement the code that creates these training examples.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#32-generating-skip-gram-training-pairs","title":"3.2 \ud83d\udd01 Generating Skip-gram Training Pairs\u00b6","text":"<p>In the Skip-gram model, our goal is to predict context words from a given target word.</p> <p>That means for every target word, we create multiple training pairs \u2014 one for each context word that appears within a sliding window.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#how-it-works","title":"\ud83e\uddf1 How It Works:\u00b6","text":"<ol> <li>Slide a context window over the corpus</li> <li>For each word at the center (our target)...</li> <li>Grab the words around it within the window (these are the context words)</li> <li>Create training pairs in the form: <code>(target_word, context_word)</code></li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#example","title":"\ud83d\udccc Example\u00b6","text":"<p>If our window size is 2 and the center word is <code>\"sat\"</code>, we generate: (\"sat\", \"the\") (\"sat\", \"cat\") (\"sat\", \"on\") (\"sat\", \"the\")</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#pros-tradeoffs","title":"\u2696\ufe0f Pros &amp; Tradeoffs\u00b6","text":"<p>\u2705 Great for rare words and capturing subtle semantic differences \u2757 But... it produces lots of training examples, which can slow down training</p> <p>Let\u2019s implement this logic in code and prepare the dataset for training our Skip-gram model.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#33-generating-cbow-training-pairs","title":"3.3 \ud83e\uddfa Generating CBOW Training Pairs\u00b6","text":"<p>The CBOW (Continuous Bag of Words) model flips the Skip-gram idea on its head:</p> <p>Instead of predicting context from a word, we predict the target word from its context.</p> <p>This means we group the surrounding words together and try to guess the word in the middle.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#how-it-works","title":"\ud83e\uddf1 How It Works:\u00b6","text":"<ol> <li>Slide a context window over the text</li> <li>For each position, treat the center word as the target</li> <li>Take the surrounding words as the context</li> <li>Create a training example like: <code>(context_words, target_word)</code></li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#example","title":"\ud83d\udccc Example\u00b6","text":"<p>With a window size of 2 and the center word <code>\"sat\"</code>: Context: [\"the\", \"cat\", \"on\", \"the\"] Target: \"sat\" Training pair: ([\"the\", \"cat\", \"on\", \"the\"], \"sat\")</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#pros-tradeoffs","title":"\u2696\ufe0f Pros &amp; Tradeoffs\u00b6","text":"<p>\u2705 Faster training than Skip-gram \u2705 Performs well on frequent words \u2757 Might struggle with rare or ambiguous words</p> <p>Let\u2019s now implement the CBOW training data generation logic in code.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#34-comparing-skip-gram-vs-cbow","title":"3.4 \u2696\ufe0f Comparing Skip-gram vs. CBOW\u00b6","text":"<p>Before we move on, let\u2019s pause and compare the two training approaches side by side:</p> \ud83d\udca1 Aspect \ud83d\udd01 Skip-gram \ud83e\uddfa CBOW Prediction Direction Target \u2192 Context Context \u2192 Target Input One word Multiple words Output One context word at a time One target word Training Examples More (one per context word) Fewer (one per target) Training Speed Slower Faster Good for Rare Words \u2705 Yes \ud83d\udeab Not as good Captures Word Senses \u2705 Better \ud83d\udeab Limited"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#so-which-one-should-you-use","title":"\ud83e\udde0 So Which One Should You Use?\u00b6","text":"<p>Both models have their strengths:</p> <ul> <li>Skip-gram: Great for small datasets, rare words, and learning multiple meanings of words</li> <li>CBOW: Faster and often slightly better for frequent words and large corpora</li> </ul> <p>In real-world use (e.g., the original Word2Vec paper), Skip-gram is usually preferred for its superior performance on semantic tasks \u2014 even if it takes a bit longer to train.</p> <p>Up next: we\u2019ll define the neural architectures that turn these training pairs into powerful word embeddings!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#4-skip-gram-model-implementation","title":"4. \u2699\ufe0f Skip-gram Model Implementation\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#41-the-skip-gram-architecture-with-negative-sampling","title":"4.1 \ud83e\udde0 The Skip-gram Architecture with Negative Sampling\u00b6","text":"<p>Now that we\u2019ve generated our training pairs, it\u2019s time to move into modeling \u2014 starting with Skip-gram, enhanced with negative sampling.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#quick-recap","title":"\ud83d\udd04 Quick Recap\u00b6","text":"<p>Earlier, we generated Skip-gram training pairs in the vanilla style \u2014 where each (target, context) pair would be used to predict the full probability distribution over the entire vocabulary.</p> <p>That\u2019s perfectly valid, but it's computationally expensive, especially with vocabularies of 10,000+ words.</p> <p>So instead, we\u2019ll now switch to a more efficient and scalable approach:</p> <p>Skip-gram with Negative Sampling</p> <p>This requires rethinking how we generate training data and how we structure the model \u2014 but it\u2019ll make everything much faster and more practical, even on large datasets.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-is-negative-sampling","title":"\ud83d\udca1 What Is Negative Sampling?\u00b6","text":"<p>Instead of trying to predict the entire vocabulary, we reframe the problem as a binary classification task:</p> <ul> <li>Is this word a true context word for the target? \u2705</li> <li>Or is it just a random (negative) sample? \u274c</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#how-it-works","title":"\ud83e\uddea How It Works:\u00b6","text":"<ol> <li>For each positive pair <code>(target, context)</code></li> <li>Sample a few negative pairs: words that are not actual context</li> <li>Train the model to:<ul> <li>Push positive pairs closer in embedding space</li> <li>Push negative pairs apart</li> </ul> </li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#why-use-it","title":"\u2699\ufe0f Why Use It?\u00b6","text":"<ul> <li>\ud83d\ude80 Faster training: Only a few dot products per example instead of tens of thousands</li> <li>\ud83d\udcc8 Better results: Especially for rare words and semantic similarity</li> <li>\ud83d\udcbc Scalable: Works well even for massive corpora</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#training-objective","title":"\ud83e\uddee Training Objective\u00b6","text":"<p>We optimize the following loss for each training pair:</p> <p>$$ \\log(\\sigma(v_{target} \\cdot v_{context})) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)}[\\log(\\sigma(-v_{target} \\cdot v_{w_i}))] $$</p> <p>Where:</p> <ul> <li>$\\sigma$ = sigmoid function</li> <li>$v_{target}$ = embedding of the center (input) word</li> <li>$v_{context}$ = embedding of a true context word</li> <li>$v_{w_i}$ = embeddings of negative samples</li> <li>$k$ = number of negative samples</li> <li>$P_n(w)$ = noise distribution (usually unigram freq raised to the 3/4 power)</li> </ul> <p>\ud83d\udee0\ufe0f Note: Since our earlier training pairs were built for the vanilla Skip-gram model, we\u2019ll now generate a new version of the training dataset tailored for negative sampling. The implementation will be a bit trickier \u2014 but way more efficient.</p> <p>Let\u2019s dive in.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#42-implementing-the-skip-gram-model-with-negative-sampling","title":"4.2 \ud83d\udee0\ufe0f Implementing the Skip-gram Model with Negative Sampling\u00b6","text":"<p>Now that we understand the theory behind Skip-gram with negative sampling, let\u2019s implement it in TensorFlow/Keras!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#input-format","title":"\ud83d\udce5 Input Format\u00b6","text":"<p>Our model takes in three components:</p> <ol> <li>Target word index (center word)</li> <li>Positive context word index</li> <li>Negative word indices (randomly sampled \"noise\" words)</li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#model-structure","title":"\ud83e\uddf1 Model Structure\u00b6","text":"<p>We\u2019ve built a custom Keras model with the following components:</p> <ul> <li><code>target_embedding</code>: Learns vector representations for input (center) words</li> <li><code>context_embedding</code>: Learns vector representations for context words (used for both positive and negative samples)</li> <li>Dot products are computed between target and context embeddings:<ul> <li>Positive dot product for real context words</li> <li>Negative dot products for sampled non-context words</li> </ul> </li> </ul> <p>These are passed to a custom loss function that encourages the model to:</p> <ul> <li>Assign high similarity to real <code>(target, context)</code> pairs</li> <li>Assign low similarity to randomly sampled negatives</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#loss-function","title":"\ud83e\uddea Loss Function\u00b6","text":"<p>We're using a custom negative sampling loss, which:</p> <ul> <li>Applies sigmoid cross-entropy</li> <li>Labels the first value in each output vector as <code>1</code> (positive pair)</li> <li>Labels all others as <code>0</code> (negatives)</li> </ul> <p>This simplifies our objective into a set of binary classification tasks per training example.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#preparing-training-data","title":"\ud83e\uddf0 Preparing Training Data\u00b6","text":"<p>Because negative sampling changes our training format, we regenerate the dataset using a helper function:</p> <pre>targets, contexts, negatives = prepare_training_data(skipgram_pairs, vocab_size, num_ns=4)\n</pre> <p>We then wrap it all in a <code>tf.data.Dataset</code> pipeline:</p> <ul> <li>Batching</li> <li>Shuffling</li> <li>Prefetching for performance</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#embedding-dimension","title":"\ud83e\udde0 Embedding Dimension\u00b6","text":"<p>We\u2019re using a small embedding size of 16 dimensions to keep training fast during experimentation. (In practice, 100\u2013300 dimensions are more common.)</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#model-compilation","title":"\u2705 Model Compilation\u00b6","text":"<p>We compile the model using:</p> <ul> <li>Adam optimizer for efficient gradient updates</li> <li>Our custom loss function tailored for negative sampling</li> </ul> <p>Once compiled, we can start training!</p> <p>Let\u2019s check out the model summary to see the architecture.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#43-training-the-skip-gram-model","title":"4.3 \ud83d\ude82 Training the Skip-gram Model\u00b6","text":"<p>We\u2019ve now trained our Skip-gram model using negative sampling, which reframed word prediction as a series of binary classification problems.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#training-insights","title":"\ud83d\udcc9 Training Insights\u00b6","text":"<p>Training ran for 20 full epochs, processing over 6,000 batches per epoch. The model\u2019s loss decreased steadily from <code>0.6932</code> to around <code>0.2374</code>, which shows consistent learning over time.</p> <p>Even though the loss might seem small and the \"accuracy\" isn\u2019t tracked here, that\u2019s expected \u2014 this task isn\u2019t about exact predictions, but about learning a good embedding space.</p> <p>Why? Because context can be fuzzy \u2014 there are usually many valid context words for any target. What matters is whether similar words end up with similar vectors.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-the-model-learns","title":"\ud83e\udde0 What the Model Learns\u00b6","text":"<p>Through backpropagation, the model shifts embeddings so that:</p> <ul> <li>Words that appear in similar contexts are pulled closer together</li> <li>Irrelevant or unrelated words are pushed apart</li> </ul> <p>This way, even without explicit semantic labels, the model organizes the vocabulary in a way that reflects meaning.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#44-extracting-and-interpreting-word-embeddings","title":"4.4 \ud83d\udce6 Extracting and Interpreting Word Embeddings\u00b6","text":"<p>After training, we extract the learned word vectors from the model\u2019s <code>Embedding</code> layer. Each word is now represented as a 16-dimensional vector (you\u2019ll typically see 100\u2013300 in real applications, but we\u2019ve chosen 16 for faster training).</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-do-these-vectors-mean","title":"\ud83d\udd0d What Do These Vectors Mean?\u00b6","text":"<ul> <li>Each word\u2019s vector encodes semantic and syntactic relationships</li> <li>Similar vectors \u2192 similar meanings, measured via cosine similarity</li> <li>These vectors also support semantic arithmetic like: <p><code>king - man + woman \u2248 queen</code></p> </li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#before-vs-after-training","title":"\ud83d\udcca Before vs. After Training\u00b6","text":"<p>Let\u2019s look at a few examples of cosine similarity before and after training:</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#king","title":"\ud83c\udff0 <code>\"king\"</code>\u00b6","text":"<ul> <li>Before: semantic, atheistic, commentator</li> <li>After: bishop, john, edward, henry</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#france","title":"\ud83c\udf0d <code>\"france\"</code>\u00b6","text":"<ul> <li>Before: estate, ufos, optional</li> <li>After: serbia, italy, baptiste</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#computer","title":"\ud83d\udcbb <code>\"computer\"</code>\u00b6","text":"<ul> <li>Before: syrian, venues, tiger</li> <li>After: internet, code, pages</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#dog","title":"\ud83d\udc36 <code>\"dog\"</code>\u00b6","text":"<ul> <li>Before: frogs, impairments, feed</li> <li>After: prairie, rapper, johnson</li> </ul> <p>As you can see, the post-training neighbors are much more semantically relevant \u2014 indicating that our model has learned useful representations!</p> <p>These embeddings are now ready for visualization, clustering, or use in downstream NLP tasks.</p> <p>Up next: let\u2019s evaluate them further with intrinsic tasks and maybe even visualize the embedding space!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#5-cbow-model-implementation","title":"5. \ud83e\uddfa CBOW Model Implementation\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#51-the-cbow-architecture-with-negative-sampling","title":"5.1 \ud83e\udde0 The CBOW Architecture (with Negative Sampling)\u00b6","text":"<p>The Continuous Bag of Words (CBOW) model takes a different approach from Skip-gram:</p> <p>It predicts a target word from its surrounding context.</p> <p>Just like with Skip-gram, we\u2019ll enhance the CBOW model with negative sampling to make it efficient and scalable.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#how-cbow-works-recap","title":"\ud83d\udd01 How CBOW Works (Recap)\u00b6","text":"<ul> <li>For each word in the corpus, we:<ol> <li>Take the surrounding context words as input</li> <li>Use them to predict the center word (the target)</li> </ol> </li> <li>Instead of treating the context individually, we average their embeddings into a single vector</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#model-structure","title":"\ud83e\uddf1 Model Structure\u00b6","text":"<p>We\u2019ve implemented CBOW with the following architecture:</p> <ul> <li><code>word_embedding</code>: Learns embeddings for context words</li> <li><code>target_embedding</code>: Learns embeddings for target words</li> <li>For each example:<ul> <li>Compute the average of all context embeddings</li> <li>Dot it with the target word embedding (positive sample)</li> <li>Dot it with several negative samples (noise words)</li> </ul> </li> </ul> <p>These dot products are passed into a custom loss to distinguish positive vs. negative examples.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#input-format","title":"\ud83d\udce5 Input Format\u00b6","text":"<p>Each training instance includes:</p> <ol> <li>Context word indices (multiple, e.g., 4 words)</li> <li>Target word index (center word)</li> <li>Negative sample indices (randomly chosen non-target words)</li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#loss-function","title":"\ud83e\uddea Loss Function\u00b6","text":"<p>We use the same custom binary cross-entropy loss as in Skip-gram:</p> <ul> <li>The first dot product is treated as a positive example (<code>label = 1</code>)</li> <li>The remaining dot products are negatives (<code>label = 0</code>)</li> <li>The loss encourages:<ul> <li>High similarity with the correct target</li> <li>Low similarity with random words</li> </ul> </li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#preparing-training-data","title":"\ud83e\uddf0 Preparing Training Data\u00b6","text":"<p>To build the training set, we slide a window across the corpus:</p> <pre>contexts, targets, negatives = prepare_cbow_training_data(\n    filtered_words, word2idx, context_size=2, num_ns=4\n)\n</pre> <p>Each row of <code>contexts</code> contains 4 word indices (2 before, 2 after), and for each pair, we generate 4 negative samples.</p> <p>We package it all into a <code>tf.data.Dataset</code> pipeline for efficient batching and shuffling.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#embedding-dimension","title":"\ud83d\udcd0 Embedding Dimension\u00b6","text":"<p>As before, we\u2019re using an embedding size of 16 for quick training. Larger dimensions (like 100\u2013300) are often used in production.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#model-compilation","title":"\u2705 Model Compilation\u00b6","text":"<p>We compile the model using:</p> <ul> <li>Adam optimizer for stable and adaptive learning</li> <li>Our custom CBOW negative sampling loss</li> </ul> <p>Once compiled, we\u2019re ready to train!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#52-training-the-cbow-model","title":"5.2 \ud83d\ude80 Training the CBOW Model\u00b6","text":"<p>We\u2019ve now trained our CBOW model using negative sampling, where the model learns to predict the center word from its surrounding context.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#training-insights","title":"\u26a1 Training Insights\u00b6","text":"<p>CBOW trains significantly faster than Skip-gram \u2014 in our case, nearly 3\u00d7 faster:</p> <ul> <li>Just 1,547 batches per epoch, compared to over 6,000 in Skip-gram</li> <li>Training completed in under 80 seconds for all 20 epochs</li> </ul> <p>The loss decreased from ~0.6931 (random guess) to ~0.1368, showing smooth and effective learning across the board.</p> <p>\u2705 CBOW is particularly efficient when training on large corpora and works well with frequent words.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-the-model-learns","title":"\ud83e\udde0 What the Model Learns\u00b6","text":"<p>Here, the model learns to average the embeddings of surrounding words and use that signal to adjust the target word's vector:</p> <ul> <li>If a group of context words tends to co-occur with a target, the target gets pulled closer to them</li> <li>Noise samples get pushed further away, improving discrimination</li> </ul> <p>This results in embeddings that capture shared semantics across similar contexts.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#53-extracting-and-interpreting-word-embeddings","title":"5.3 \ud83d\udce6 Extracting and Interpreting Word Embeddings\u00b6","text":"<p>As with Skip-gram, we extract the final vectors from the model\u2019s learned embeddings. Each word is now represented as a 16-dimensional vector, trained to reflect the context in which that word appears.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-do-these-vectors-represent","title":"\ud83d\udd0d What Do These Vectors Represent?\u00b6","text":"<ul> <li>Encoded semantic relationships derived from context</li> <li>Similar meanings = similar vectors</li> <li>Supports analogy-style reasoning (e.g., <code>paris - france + italy \u2248 rome</code>)</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#before-vs-after-training","title":"\ud83d\udcca Before vs. After Training\u00b6","text":"<p>Here\u2019s a look at how well the CBOW model captured meaningful associations \u2014 measured via cosine similarity:</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#king","title":"\ud83c\udff0 <code>\"king\"</code>\u00b6","text":"<ul> <li>Before: allosaurus, tendencies, eighty</li> <li>After: marched, father, benedict, son</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#france","title":"\ud83c\udf0d <code>\"france\"</code>\u00b6","text":"<ul> <li>Before: kandahar, candidate, revisionism</li> <li>After: merchants, salisbury, mauritania</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#computer","title":"\ud83d\udcbb <code>\"computer\"</code>\u00b6","text":"<ul> <li>Before: imports, supplying, transatlantic</li> <li>After: anime, inventors, animation, portable</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#dog","title":"\ud83d\udc36 <code>\"dog\"</code>\u00b6","text":"<ul> <li>Before: critic, lift, postulated</li> <li>After: mario, lisa, dead, mysteries</li> </ul> <p>You can clearly see how the model reshapes the embedding space \u2014 similar words move closer, and noise disappears from the top neighbors. But it looks like the model does not capture the semantic relationships as well as Skip-gram.</p> <p>These embeddings are now ready to be explored further via visualization, clustering, or downstream NLP tasks.</p> <p>Let\u2019s now compare Skip-gram and CBOW embeddings more directly!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#6-intrinsic-evaluation-of-word-embeddings","title":"6. \ud83e\uddea Intrinsic Evaluation of Word Embeddings\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#61-what-is-intrinsic-evaluation","title":"6.1 \ud83e\udd14 What Is Intrinsic Evaluation?\u00b6","text":"<p>Now that we've trained our Skip-gram and CBOW models, it's time to inspect what they\u2019ve actually learned.</p> <p>Intrinsic evaluation focuses on testing the embeddings themselves, rather than plugging them into downstream tasks like classification or translation.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-are-we-evaluating","title":"\ud83d\udd0d What Are We Evaluating?\u00b6","text":"<p>We\u2019ll look at whether our embeddings capture:</p> <ol> <li>Word Similarity \u2013 Do semantically related words have similar vectors?</li> <li>Word Analogies \u2013 Can the model solve analogies like <code>\"king - man + woman \u2248 queen\"</code>?</li> <li>Semantic Clusters \u2013 Do related words group together in embedding space?</li> <li>Visual Patterns \u2013 Do we see interpretable structures when reducing to 2D?</li> </ol> <p>These tests give us direct insight into how well our model has captured the structure of language.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#why-this-matters","title":"\ud83d\udca1 Why This Matters\u00b6","text":"<p>Intrinsic evaluations help us understand:</p> <ul> <li>Whether embeddings reflect real semantic meaning</li> <li>How different architectures (Skip-gram vs. CBOW) perform</li> <li>What types of relationships the model is good at learning</li> <li>How much the model improved over its random initialization</li> </ul> <p>It\u2019s like opening up the model\u2019s brain and asking: \u201cHow well do you understand the relationships between words?\u201d</p> <p>Let\u2019s now implement functions for each of these evaluation methods and compare the results across models.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#62-word-similarity-evaluation","title":"6.2 \ud83d\udd0d Word Similarity Evaluation\u00b6","text":"<p>The table above compares the top 5 most similar words (based on cosine similarity) for each of our test words across four embedding sets:</p> <ol> <li>Initial Random Embeddings: Before any training (baseline)</li> <li>Skip-gram Trained Embeddings</li> <li>CBOW Initial Embeddings</li> <li>CBOW Trained Embeddings</li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#key-takeaways","title":"\ud83e\udde0 Key Takeaways\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#1-initial-vs-trained-embeddings","title":"1. Initial vs. Trained Embeddings\u00b6","text":"<ul> <li>The initial embeddings (both for Skip-gram and CBOW) show no meaningful semantic grouping \u2014 top matches are random or nonsensical (e.g., <code>\"woman\" \u2192 halogens</code>, <code>\"queen\" \u2192 mnd</code>).</li> <li>After training, both models clearly organize the space: <code>\"king\"</code> is now surrounded by names like <code>\"edward\"</code> and <code>\"henry\"</code>, <code>\"france\"</code> by <code>\"serbia\"</code> and <code>\"mauritania\"</code>, and so on.</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#2-skip-gram-vs-cbow","title":"2. Skip-gram vs. CBOW\u00b6","text":"<ul> <li>Skip-gram often captures sharper, more specific associations \u2014 <code>\"france\" \u2192 serbia (0.83) baptiste (0.82), italy (0.79)\"</code>, <code>\"berlin\" \u2192 \"weimar\" (0.87), \"das\" (0.79), \"norway\" (0.78), \"liberation\" (0.78)\"</code>.</li> <li>CBOW finds more broadly related concepts, with smoother clusters and more general co-occurrence patterns.</li> <li>In some cases, CBOW even returns slightly more abstract matches  (e.g., <code>\"year\" \u2192 days, minutes, propagation\"</code>).</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#overall-evaluation","title":"\u2705 Overall Evaluation\u00b6","text":"<p>Both models have successfully transformed random vectors into semantically meaningful embeddings, even with low training data. We could imagine much better results with more training data and more epochs. The Skip-gram model leans toward precise relationships, while CBOW captures softer semantic themes.</p> <p>This confirms that even with a small embedding dimension and moderate training time, our models learn to encode real-world language patterns, all by just observing word co-occurrence.</p> <p>Up next: let\u2019s push further with analogies and visualizations!</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#63-word-analogy-evaluation","title":"6.3 \ud83e\udde0 Word Analogy Evaluation\u00b6","text":"<p>Word analogy tasks go beyond simple similarity \u2014 they test whether embeddings capture relational structures in a way that allows consistent vector arithmetic.</p> <p>The classic example is:</p> <p>\"king - man + woman = ?\" \u2192 Ideally: <code>\"queen\"</code></p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-are-we-testing","title":"\ud83d\udd04 What Are We Testing?\u00b6","text":"<p>These evaluations check if:</p> <ol> <li>Semantic relationships are encoded as vector differences</li> <li>The same \"offset\" (e.g., gender, country-capital) applies across different word pairs</li> <li>The model can retrieve the correct word via nearest-neighbor search in the embedding space</li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#key-observations-from-our-results","title":"\ud83d\udcca Key Observations from Our Results\u00b6","text":""},{"location":"chapter3/Session_3_1_Word2Vec_Training/#1-models-learn-some-relational-structure","title":"\u2705 1. Models Learn Some Relational Structure\u00b6","text":"<ul> <li>Both Skip-gram and CBOW capture certain relationships, though not always the expected word appears at the top.</li> <li>Examples like <code>\"eat - food + drink\"</code> returning <code>\"drinking\"</code> (Skip-gram) suggest partial relational understanding.</li> <li>Analogies involving verb tenses or comparatives are harder, but sometimes the models retrieve related concepts (e.g., <code>\"good - best + bad\"</code> \u2192 <code>\"tired\"</code> or <code>\"cease\"</code>).</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#2-skip-gram-vs-cbow","title":"\ud83c\udd9a 2. Skip-gram vs. CBOW\u00b6","text":"<ul> <li>Skip-gram tends to perform better on analogies, especially for precise relationships like gender or verb forms.</li> <li>CBOW retrieves more abstract or loosely related terms, occasionally hitting the right direction but missing specificity.</li> <li>Neither model perfectly solves all analogies \u2014 which is expected given the data size and low embedding dimensionality (16d).</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#3-limitations-to-keep-in-mind","title":"\u26a0\ufe0f 3. Limitations to Keep in Mind\u00b6","text":"<ul> <li>The vocabulary is limited, so many ideal targets (e.g., <code>\"rome\"</code>, <code>\"queen\"</code>, <code>\"running\"</code>) may be missing or underrepresented.</li> <li>Embedding quality improves with larger corpora, longer training, and higher dimensions.</li> <li>Still, these results give us a powerful diagnostic tool to evaluate what the model is capturing.</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#conclusion","title":"\ud83c\udfc1 Conclusion\u00b6","text":"<p>While we\u2019re not seeing state-of-the-art analogy solving here, this evaluation demonstrates that even small Word2Vec models can encode meaningful relationships.</p> <p>\u2705 Some analogies are partially captured \u2705 Embeddings reflect directional meaning shifts \u2705 We now have a way to benchmark embedding quality intrinsically</p> <p>As a next step, we can visualize the embedding space to get an intuitive sense of the structure that these analogies hint at.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#64-semantic-clustering-analysis","title":"6.4 \ud83e\udded Semantic Clustering Analysis\u00b6","text":"<p>The plots above visualize how word embeddings evolve before and after training, using t-SNE to project them into 2D space. Each word belongs to a predefined semantic category (e.g., cities, family, colors), and colors indicate groupings.</p>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#what-were-looking-for","title":"\ud83d\udc40 What We're Looking For:\u00b6","text":"<ol> <li>Category Clustering \u2013 Do words from the same group stay close together?</li> <li>Semantic Proximity \u2013 Are related categories near each other in the space?</li> <li>Transformation Through Training \u2013 Does the embedding space become more meaningful?</li> </ol>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#initial-random-embeddings","title":"\ud83e\udea8 Initial Random Embeddings\u00b6","text":"<ul> <li>Words are randomly scattered, with no visible structure</li> <li>No evidence of clustering by meaning or category</li> <li>Semantic categories are completely blended and unorganized</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#skip-gram-trained-embeddings","title":"\ud83e\udde0 Skip-gram Trained Embeddings\u00b6","text":"<ul> <li>Clear clustering by semantic category (e.g., countries, numbers, colors, transportation)</li> <li>Related categories (e.g., cities and countries) are positioned near each other</li> <li>Numbers form a tight group: <code>\"one\"</code>, <code>\"two\"</code>, <code>\"three\"</code>, <code>\"four\"</code>, <code>\"five\"</code>, <code>\"six\"</code>, <code>\"seven\"</code>, <code>\"eight\"</code>, <code>\"nine\"</code>, <code>\"ten\"</code></li> <li>Shows sharper boundaries between categories, reflecting Skip-gram\u2019s tendency for more precise local relationships</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#cbow-trained-embeddings","title":"\ud83c\udf10 CBOW Trained Embeddings\u00b6","text":"<ul> <li>Strong semantic grouping, though slightly more blended transitions between clusters</li> <li>Categories like colors, family, and transportation still stand out</li> <li>Cities and countries are often close \u2014 but with softer separation than Skip-gram</li> <li>This reflects CBOW\u2019s nature: it smooths across larger context windows, producing more globally coherent embeddings</li> </ul>"},{"location":"chapter3/Session_3_1_Word2Vec_Training/#takeaway","title":"\u2705 Takeaway\u00b6","text":"<p>These plots visually confirm that:</p> <ul> <li>Training drastically transforms the embedding space from random noise to meaningful structure</li> <li>Both models learn semantics from context alone</li> <li>Proximity now encodes meaning, and category membership becomes visually detectable</li> </ul> <p>This clustering is a strong sign that our Word2Vec models have successfully learned to represent semantic relationships in vector space.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/","title":"\ud83d\udcda Embedding Evaluation: Intrinsic and Extrinsic","text":"In\u00a0[1]: Copied! <pre># Import necessary libraries\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom huggingface_hub import hf_hub_download\nfrom staticvectors import StaticVectors\nimport fasttext\n\n# Load Word2Vec model\nprint(\"Loading Word2Vec model...\")\nword2vec_model = StaticVectors(\"neuml/word2vec\")\n\n# Load FastText model\nprint(\"Loading FastText model...\")\nfasttext_model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\nfasttext_model = fasttext.load_model(fasttext_model_path)\n\n# Load ELMo embeddings from TensorFlow Hub\nprint(\"Loading ELMo embeddings from TensorFlow Hub...\")\nelmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")\n\nprint(\"All models loaded successfully!\")\n</pre> # Import necessary libraries import numpy as np import tensorflow as tf import tensorflow_hub as hub from huggingface_hub import hf_hub_download from staticvectors import StaticVectors import fasttext  # Load Word2Vec model print(\"Loading Word2Vec model...\") word2vec_model = StaticVectors(\"neuml/word2vec\")  # Load FastText model print(\"Loading FastText model...\") fasttext_model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\") fasttext_model = fasttext.load_model(fasttext_model_path)  # Load ELMo embeddings from TensorFlow Hub print(\"Loading ELMo embeddings from TensorFlow Hub...\") elmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")  print(\"All models loaded successfully!\") <pre>Loading Word2Vec model...\nLoading FastText model...\nLoading ELMo embeddings from TensorFlow Hub...\nAll models loaded successfully!\n</pre> In\u00a0[2]: Copied! <pre>def get_word2vec_embedding(word):\n    try:\n        # StaticVectors expects a list of words and returns a list of embeddings\n        embeddings = word2vec_model.embeddings([word])\n        return embeddings[0]\n    except Exception as e:\n        print(f\"Error getting Word2Vec embedding for {word}: {e}\")\n        return None\n\ndef get_fasttext_embedding(word):\n    try:\n        return fasttext_model[word]\n    except Exception as e:\n        print(f\"Error getting FastText embedding for {word}: {e}\")\n        return None  # FastText handles OOV, but just in case\n\n# First, let's update the get_elmo_embedding function to make it more robust\ndef get_elmo_embedding(sentence, target_word):\n    \"\"\"\n    Extract contextual embedding for a target word using ELMo via TensorFlow Hub.\n    \n    Args:\n        sentence (str): A sentence containing the target word\n        target_word (str): The word to get embedding for\n        \n    Returns:\n        numpy.ndarray: The ELMo embedding for the target word\n    \"\"\"\n    # Tokenize the sentence (more robust tokenization)\n    words = sentence.lower().split()\n    target_word = target_word.lower()\n    \n    # Find the position of the target word\n    try:\n        target_positions = [i for i, word in enumerate(words) if word.strip('.,:;!?') == target_word]\n        if not target_positions:\n            # Try with exact match as fallback\n            target_positions = [i for i, word in enumerate(words) if word == target_word]\n            if not target_positions:\n                print(f\"Warning: '{target_word}' not found in sentence: '{sentence}'\")\n                return None\n        target_pos = target_positions[0]  # Use the first occurrence\n    except Exception as e:\n        print(f\"Error finding '{target_word}' in '{sentence}': {e}\")\n        return None\n    \n    # Get ELMo embeddings for the sentence\n    try:\n        embeddings = elmo_model.signatures[\"default\"](\n            tf.constant([sentence])\n        )\n        \n        # Extract the embedding for the target word\n        # The ELMo model returns word vectors with shape [batch_size, max_length, embedding_size]\n        word_embedding = embeddings[\"word_emb\"][0, target_pos, :].numpy()\n        \n        return word_embedding\n    except Exception as e:\n        print(f\"Error getting ELMo embedding: {e}\")\n        return None\n</pre> def get_word2vec_embedding(word):     try:         # StaticVectors expects a list of words and returns a list of embeddings         embeddings = word2vec_model.embeddings([word])         return embeddings[0]     except Exception as e:         print(f\"Error getting Word2Vec embedding for {word}: {e}\")         return None  def get_fasttext_embedding(word):     try:         return fasttext_model[word]     except Exception as e:         print(f\"Error getting FastText embedding for {word}: {e}\")         return None  # FastText handles OOV, but just in case  # First, let's update the get_elmo_embedding function to make it more robust def get_elmo_embedding(sentence, target_word):     \"\"\"     Extract contextual embedding for a target word using ELMo via TensorFlow Hub.          Args:         sentence (str): A sentence containing the target word         target_word (str): The word to get embedding for              Returns:         numpy.ndarray: The ELMo embedding for the target word     \"\"\"     # Tokenize the sentence (more robust tokenization)     words = sentence.lower().split()     target_word = target_word.lower()          # Find the position of the target word     try:         target_positions = [i for i, word in enumerate(words) if word.strip('.,:;!?') == target_word]         if not target_positions:             # Try with exact match as fallback             target_positions = [i for i, word in enumerate(words) if word == target_word]             if not target_positions:                 print(f\"Warning: '{target_word}' not found in sentence: '{sentence}'\")                 return None         target_pos = target_positions[0]  # Use the first occurrence     except Exception as e:         print(f\"Error finding '{target_word}' in '{sentence}': {e}\")         return None          # Get ELMo embeddings for the sentence     try:         embeddings = elmo_model.signatures[\"default\"](             tf.constant([sentence])         )                  # Extract the embedding for the target word         # The ELMo model returns word vectors with shape [batch_size, max_length, embedding_size]         word_embedding = embeddings[\"word_emb\"][0, target_pos, :].numpy()                  return word_embedding     except Exception as e:         print(f\"Error getting ELMo embedding: {e}\")         return None In\u00a0[3]: Copied! <pre>from sklearn.metrics.pairwise import cosine_similarity\n\n# Static word\nword = \"point\"\n\n# Contexts for ELMo\nsentence1 = \"I will show you a valid point of reference and talk to the point\"\nsentence2 = \"Where have you placed the point\"\n\n# --- Get embeddings ---\nw2v_vec = get_word2vec_embedding(word)\nft_vec = get_fasttext_embedding(word)\nelmo_vec1 = get_elmo_embedding(sentence1, word)\nelmo_vec2 = get_elmo_embedding(sentence2, word)\n\n# --- Display results ---\nprint(f\"Word2Vec '{word}' shape: {w2v_vec.shape} | Sample: {w2v_vec[:5]}\")\nprint(f\"FastText '{word}' shape: {ft_vec.shape} | Sample: {ft_vec[:5]}\")\nprint(f\"ELMo '{word}' (context 1) shape: {elmo_vec1.shape} | Sample: {elmo_vec1[:5]}\")\nprint(f\"ELMo '{word}' (context 2) shape: {elmo_vec2.shape} | Sample: {elmo_vec2[:5]}\")\n\n# --- Compare contextual differences ---\nsimilarity = cosine_similarity([elmo_vec1], [elmo_vec2])[0][0]\nprint(f\"\\nCosine similarity between ELMo embeddings (context 1 vs 2): {similarity:.4f}\")\n</pre> from sklearn.metrics.pairwise import cosine_similarity  # Static word word = \"point\"  # Contexts for ELMo sentence1 = \"I will show you a valid point of reference and talk to the point\" sentence2 = \"Where have you placed the point\"  # --- Get embeddings --- w2v_vec = get_word2vec_embedding(word) ft_vec = get_fasttext_embedding(word) elmo_vec1 = get_elmo_embedding(sentence1, word) elmo_vec2 = get_elmo_embedding(sentence2, word)  # --- Display results --- print(f\"Word2Vec '{word}' shape: {w2v_vec.shape} | Sample: {w2v_vec[:5]}\") print(f\"FastText '{word}' shape: {ft_vec.shape} | Sample: {ft_vec[:5]}\") print(f\"ELMo '{word}' (context 1) shape: {elmo_vec1.shape} | Sample: {elmo_vec1[:5]}\") print(f\"ELMo '{word}' (context 2) shape: {elmo_vec2.shape} | Sample: {elmo_vec2[:5]}\")  # --- Compare contextual differences --- similarity = cosine_similarity([elmo_vec1], [elmo_vec2])[0][0] print(f\"\\nCosine similarity between ELMo embeddings (context 1 vs 2): {similarity:.4f}\") <pre>Word2Vec 'point' shape: (300,) | Sample: [ 0.05756511 -0.00306746  0.02935592  0.08577431 -0.05274891]\nFastText 'point' shape: (300,) | Sample: [ 0.03026813  0.06170474  0.05471474  0.08540645 -0.05659813]\nELMo 'point' (context 1) shape: (512,) | Sample: [ 0.12824385  0.14562231 -0.20140205  0.04809816 -0.50635725]\nELMo 'point' (context 2) shape: (512,) | Sample: [ 0.12824386  0.14562228 -0.2014019   0.04809807 -0.50635725]\n\nCosine similarity between ELMo embeddings (context 1 vs 2): 1.0000\n</pre> In\u00a0[4]: Copied! <pre>import pandas as pd\nfrom datasets import load_dataset\nfrom scipy.stats import spearmanr\nimport nltk\nnltk.download(\"words\")\n\n# Load WordSim-353 from Hugging Face\nprint(\"Loading WordSim-353 from Hugging Face...\")\ndataset = load_dataset(\"almogtavor/WordSim353\")\nwordsim = pd.DataFrame(dataset['train'])\n\n# Rename columns to match expected format\nwordsim.columns = [\"word1\", \"word2\", \"human_score\"]\nprint(f\"\u2705 Loaded {len(wordsim)} word pairs from WordSim-353\")\nwordsim.head()\n</pre> import pandas as pd from datasets import load_dataset from scipy.stats import spearmanr import nltk nltk.download(\"words\")  # Load WordSim-353 from Hugging Face print(\"Loading WordSim-353 from Hugging Face...\") dataset = load_dataset(\"almogtavor/WordSim353\") wordsim = pd.DataFrame(dataset['train'])  # Rename columns to match expected format wordsim.columns = [\"word1\", \"word2\", \"human_score\"] print(f\"\u2705 Loaded {len(wordsim)} word pairs from WordSim-353\") wordsim.head() <pre>[nltk_data] Downloading package words to /Users/agomberto/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n</pre> <pre>Loading WordSim-353 from Hugging Face...\n</pre> <pre>python(71809) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n</pre> <pre>\u2705 Loaded 353 word pairs from WordSim-353\n</pre> Out[4]: word1 word2 human_score 0 admission ticket 5.5360 1 alcohol chemistry 4.1250 2 aluminum metal 6.6250 3 announcement effort 2.0625 4 announcement news 7.1875 In\u00a0[5]: Copied! <pre>import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef compute_similarity(row, model_type):\n    w1, w2 = row[\"word1\"], row[\"word2\"]\n\n    if model_type == \"word2vec\":\n        vec1, vec2 = get_word2vec_embedding(w1), get_word2vec_embedding(w2)\n    elif model_type == \"fasttext\":\n        vec1, vec2 = get_fasttext_embedding(w1), get_fasttext_embedding(w2)\n    elif model_type == \"elmo\":\n        sent1 = f\"This is {w1}.\"\n        sent2 = f\"This is {w2}.\"\n        try:\n            vec1 = get_elmo_embedding(sent1, w1)\n            vec2 = get_elmo_embedding(sent2, w2)\n        except:\n            return np.nan\n    else:\n        return np.nan\n\n    if vec1 is None or vec2 is None:\n        return np.nan\n\n    return cosine_similarity([vec1], [vec2])[0][0]\n</pre> import numpy as np from sklearn.metrics.pairwise import cosine_similarity import matplotlib.pyplot as plt import seaborn as sns  def compute_similarity(row, model_type):     w1, w2 = row[\"word1\"], row[\"word2\"]      if model_type == \"word2vec\":         vec1, vec2 = get_word2vec_embedding(w1), get_word2vec_embedding(w2)     elif model_type == \"fasttext\":         vec1, vec2 = get_fasttext_embedding(w1), get_fasttext_embedding(w2)     elif model_type == \"elmo\":         sent1 = f\"This is {w1}.\"         sent2 = f\"This is {w2}.\"         try:             vec1 = get_elmo_embedding(sent1, w1)             vec2 = get_elmo_embedding(sent2, w2)         except:             return np.nan     else:         return np.nan      if vec1 is None or vec2 is None:         return np.nan      return cosine_similarity([vec1], [vec2])[0][0] In\u00a0[6]: Copied! <pre># Create a copy of WordSim for evaluations\nresults = wordsim.copy()\n\nprint(\"\ud83d\udd0d Evaluating Word2Vec...\")\nresults[\"w2v_sim\"] = results.apply(lambda row: compute_similarity(row, \"word2vec\"), axis=1)\nprint(results[\"w2v_sim\"])\n\nprint(\"\ud83d\udd0d Evaluating FastText...\")\nresults[\"ft_sim\"] = results.apply(lambda row: compute_similarity(row, \"fasttext\"), axis=1)\nprint(results[\"ft_sim\"])\n\nprint(\"\ud83d\udd0d Evaluating ELMo (context-free sentences)...\")\nresults[\"elmo_sim\"] = results.apply(lambda row: compute_similarity(row, \"elmo\"), axis=1)\nprint(results[\"elmo_sim\"])\n\n# Drop rows with missing vectors\nresults_clean = results.dropna()\n</pre> # Create a copy of WordSim for evaluations results = wordsim.copy()  print(\"\ud83d\udd0d Evaluating Word2Vec...\") results[\"w2v_sim\"] = results.apply(lambda row: compute_similarity(row, \"word2vec\"), axis=1) print(results[\"w2v_sim\"])  print(\"\ud83d\udd0d Evaluating FastText...\") results[\"ft_sim\"] = results.apply(lambda row: compute_similarity(row, \"fasttext\"), axis=1) print(results[\"ft_sim\"])  print(\"\ud83d\udd0d Evaluating ELMo (context-free sentences)...\") results[\"elmo_sim\"] = results.apply(lambda row: compute_similarity(row, \"elmo\"), axis=1) print(results[\"elmo_sim\"])  # Drop rows with missing vectors results_clean = results.dropna() <pre>\ud83d\udd0d Evaluating Word2Vec...\n0      0.436883\n1      0.088762\n2      0.637576\n3      0.231306\n4      0.467333\n         ...   \n348    0.258439\n349    0.362721\n350    0.260148\n351    0.417516\n352    0.096587\nName: w2v_sim, Length: 353, dtype: float32\n\ud83d\udd0d Evaluating FastText...\n0      0.448249\n1      0.264830\n2      0.612454\n3      0.226267\n4      0.487779\n         ...   \n348    0.373049\n349    0.552444\n350    0.238878\n351    0.443536\n352    0.244587\nName: ft_sim, Length: 353, dtype: float32\n\ud83d\udd0d Evaluating ELMo (context-free sentences)...\n0      0.573292\n1      0.637792\n2      0.665230\n3      0.660678\n4      0.639659\n         ...   \n348    0.563930\n349    0.474886\n350    0.591700\n351    0.635695\n352    0.569608\nName: elmo_sim, Length: 353, dtype: float32\n</pre> In\u00a0[7]: Copied! <pre># Calculate Spearman correlations\nw2v_corr = spearmanr(results_clean[\"human_score\"], results_clean[\"w2v_sim\"]).correlation\nft_corr = spearmanr(results_clean[\"human_score\"], results_clean[\"ft_sim\"]).correlation\nelmo_corr = spearmanr(results_clean[\"human_score\"], results_clean[\"elmo_sim\"]).correlation\n</pre> # Calculate Spearman correlations w2v_corr = spearmanr(results_clean[\"human_score\"], results_clean[\"w2v_sim\"]).correlation ft_corr = spearmanr(results_clean[\"human_score\"], results_clean[\"ft_sim\"]).correlation elmo_corr = spearmanr(results_clean[\"human_score\"], results_clean[\"elmo_sim\"]).correlation In\u00a0[8]: Copied! <pre># Function to normalize scores to 0-10 range for better comparison with human scores\ndef normalize_scores(scores):\n    min_val, max_val = min(scores), max(scores)\n    return 10 * (scores - min_val) / (max_val - min_val)\n\n# Normalize similarity scores\nresults_clean[\"w2v_sim_norm\"] = normalize_scores(results_clean[\"w2v_sim\"])\nresults_clean[\"ft_sim_norm\"] = normalize_scores(results_clean[\"ft_sim\"])\nresults_clean[\"elmo_sim_norm\"] = normalize_scores(results_clean[\"elmo_sim\"])\n</pre> # Function to normalize scores to 0-10 range for better comparison with human scores def normalize_scores(scores):     min_val, max_val = min(scores), max(scores)     return 10 * (scores - min_val) / (max_val - min_val)  # Normalize similarity scores results_clean[\"w2v_sim_norm\"] = normalize_scores(results_clean[\"w2v_sim\"]) results_clean[\"ft_sim_norm\"] = normalize_scores(results_clean[\"ft_sim\"]) results_clean[\"elmo_sim_norm\"] = normalize_scores(results_clean[\"elmo_sim\"])  In\u00a0[9]: Copied! <pre># Select interesting examples for demonstration\n# 1. High similarity pairs\nhigh_sim = results_clean.nlargest(5, \"human_score\")\n# 2. Low similarity pairs\nlow_sim = results_clean.nsmallest(5, \"human_score\")\n# 3. Polysemous examples (e.g., words like 'bank', 'cell', etc.)\npolysemous = results_clean[results_clean[\"word1\"].isin([\"bank\", \"cell\", \"bat\", \"pitcher\", \"crane\"]) | \n                          results_clean[\"word2\"].isin([\"bank\", \"cell\", \"bat\", \"pitcher\", \"crane\"])]\n# 4. Cases where models disagree with humans\ndisagreement = results_clean.copy()\ndisagreement[\"w2v_diff\"] = abs(disagreement[\"human_score\"] - disagreement[\"w2v_sim_norm\"])\ndisagreement[\"ft_diff\"] = abs(disagreement[\"human_score\"] - disagreement[\"ft_sim_norm\"])\ndisagreement[\"elmo_diff\"] = abs(disagreement[\"human_score\"] - disagreement[\"elmo_sim_norm\"])\ndisagreement[\"avg_diff\"] = (disagreement[\"w2v_diff\"] + disagreement[\"ft_diff\"] + disagreement[\"elmo_diff\"]) / 3\ndisagreement = disagreement.nlargest(5, \"avg_diff\")\n\n# Combine examples\nexamples = pd.concat([high_sim, low_sim, polysemous, disagreement]).drop_duplicates()\n\n# Print summary of results\nprint(\"\\n\ud83d\udd0e Example Word Pairs and Model Predictions:\")\npd.set_option('display.max_rows', None)\nexample_display = examples[[\"word1\", \"word2\", \"human_score\", \"w2v_sim_norm\", \"ft_sim_norm\", \"elmo_sim_norm\"]]\nexample_display.columns = [\"Word 1\", \"Word 2\", \"Human Score\", \"Word2Vec\", \"FastText\", \"ELMo\"]\nprint(example_display.sort_values(\"Human Score\", ascending=False))\n</pre> # Select interesting examples for demonstration # 1. High similarity pairs high_sim = results_clean.nlargest(5, \"human_score\") # 2. Low similarity pairs low_sim = results_clean.nsmallest(5, \"human_score\") # 3. Polysemous examples (e.g., words like 'bank', 'cell', etc.) polysemous = results_clean[results_clean[\"word1\"].isin([\"bank\", \"cell\", \"bat\", \"pitcher\", \"crane\"]) |                            results_clean[\"word2\"].isin([\"bank\", \"cell\", \"bat\", \"pitcher\", \"crane\"])] # 4. Cases where models disagree with humans disagreement = results_clean.copy() disagreement[\"w2v_diff\"] = abs(disagreement[\"human_score\"] - disagreement[\"w2v_sim_norm\"]) disagreement[\"ft_diff\"] = abs(disagreement[\"human_score\"] - disagreement[\"ft_sim_norm\"]) disagreement[\"elmo_diff\"] = abs(disagreement[\"human_score\"] - disagreement[\"elmo_sim_norm\"]) disagreement[\"avg_diff\"] = (disagreement[\"w2v_diff\"] + disagreement[\"ft_diff\"] + disagreement[\"elmo_diff\"]) / 3 disagreement = disagreement.nlargest(5, \"avg_diff\")  # Combine examples examples = pd.concat([high_sim, low_sim, polysemous, disagreement]).drop_duplicates()  # Print summary of results print(\"\\n\ud83d\udd0e Example Word Pairs and Model Predictions:\") pd.set_option('display.max_rows', None) example_display = examples[[\"word1\", \"word2\", \"human_score\", \"w2v_sim_norm\", \"ft_sim_norm\", \"elmo_sim_norm\"]] example_display.columns = [\"Word 1\", \"Word 2\", \"Human Score\", \"Word2Vec\", \"FastText\", \"ELMo\"] print(example_display.sort_values(\"Human Score\", ascending=False)) <pre>\n\ud83d\udd0e Example Word Pairs and Model Predictions:\n        Word 1      Word 2  Human Score   Word2Vec   FastText       ELMo\n333      tiger       tiger      10.0000  10.000000  10.000000  10.000000\n33         car  automobile       9.3690   5.925843   7.082528   6.023292\n193      money        cash       9.0315   6.232118   6.856684   5.467494\n186     midday        noon       8.9375   5.621416   6.555274   4.570495\n192      money        cash       8.8750   6.232118   6.856684   5.467494\n92      dollar        buck       8.5625   2.718457   4.343258   3.851626\n123       fuck         sex       8.2500   2.397155   4.885740   1.801877\n13      asylum    madhouse       8.0625   2.682504   4.406788   4.486266\n194      money    currency       7.9375   1.777553   3.682916   4.988157\n17        bank       money       6.6250   2.768471   4.106092   4.784755\n35        cell       phone       6.2500   3.013003   6.191161   3.653709\n191      money        bank       6.0000   2.768471   4.106092   4.784755\n71         cup   tableware       6.0000   2.117868   2.498245   0.782586\n22        bird       crane       4.8125   3.175153   4.428065   3.381423\n58       crane   implement       2.1250   0.437181   0.742416   2.478549\n155       king     cabbage       0.6250   1.366680   2.084829   2.865413\n99       drink      mother       0.5625   1.745229   1.591457   4.075823\n97       drink         ear       0.3750   1.125240   1.141692   3.356437\n258  professor    cucumber       0.3125   0.764474   1.071865   2.760647\n96       drink         car       0.3125   2.614916   2.822372   4.857550\n</pre> In\u00a0[10]: Copied! <pre># Global evaluation\nprint(f\"\\n\ud83d\udcca Spearman Correlations with Human Scores:\")\nprint(f\"Word2Vec:  {w2v_corr:.4f}\")\nprint(f\"FastText:  {ft_corr:.4f}\")\nprint(f\"ELMo:      {elmo_corr:.4f}\")\n</pre> # Global evaluation print(f\"\\n\ud83d\udcca Spearman Correlations with Human Scores:\") print(f\"Word2Vec:  {w2v_corr:.4f}\") print(f\"FastText:  {ft_corr:.4f}\") print(f\"ELMo:      {elmo_corr:.4f}\") <pre>\n\ud83d\udcca Spearman Correlations with Human Scores:\nWord2Vec:  0.6876\nFastText:  0.7506\nELMo:      0.4521\n</pre> In\u00a0[11]: Copied! <pre># Visualization\nplt.figure(figsize=(15, 5))\n\n# Plot 1: Correlation with human scores\nplt.subplot(1, 2, 1)\ncorrelations = [w2v_corr, ft_corr, elmo_corr]\nmodels = [\"Word2Vec\", \"FastText\", \"ELMo\"]\nplt.bar(models, correlations, color=[\"skyblue\", \"lightgreen\", \"coral\"])\nplt.title(\"Correlation with Human Similarity Judgments\")\nplt.ylabel(\"Spearman Correlation\")\nplt.ylim(0, 1)\nfor i, v in enumerate(correlations):\n    plt.text(i, v + 0.05, f\"{v:.4f}\", ha='center')\n\n# Plot 2: Scatter plot comparing human vs model scores for a sample\nplt.subplot(1, 2, 2)\nplt.scatter(results_clean[\"human_score\"], results_clean[\"w2v_sim_norm\"], alpha=0.5, label=\"Word2Vec\")\nplt.scatter(results_clean[\"human_score\"], results_clean[\"ft_sim_norm\"], alpha=0.5, label=\"FastText\")\nplt.scatter(results_clean[\"human_score\"], results_clean[\"elmo_sim_norm\"], alpha=0.5, label=\"ELMo\")\nplt.plot([0, 10], [0, 10], 'k--', alpha=0.3)  # Diagonal line for perfect correlation\nplt.xlabel(\"Human Similarity Score\")\nplt.ylabel(\"Model Predicted Score (Normalized)\")\nplt.title(\"Human vs Model Similarity Scores\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> # Visualization plt.figure(figsize=(15, 5))  # Plot 1: Correlation with human scores plt.subplot(1, 2, 1) correlations = [w2v_corr, ft_corr, elmo_corr] models = [\"Word2Vec\", \"FastText\", \"ELMo\"] plt.bar(models, correlations, color=[\"skyblue\", \"lightgreen\", \"coral\"]) plt.title(\"Correlation with Human Similarity Judgments\") plt.ylabel(\"Spearman Correlation\") plt.ylim(0, 1) for i, v in enumerate(correlations):     plt.text(i, v + 0.05, f\"{v:.4f}\", ha='center')  # Plot 2: Scatter plot comparing human vs model scores for a sample plt.subplot(1, 2, 2) plt.scatter(results_clean[\"human_score\"], results_clean[\"w2v_sim_norm\"], alpha=0.5, label=\"Word2Vec\") plt.scatter(results_clean[\"human_score\"], results_clean[\"ft_sim_norm\"], alpha=0.5, label=\"FastText\") plt.scatter(results_clean[\"human_score\"], results_clean[\"elmo_sim_norm\"], alpha=0.5, label=\"ELMo\") plt.plot([0, 10], [0, 10], 'k--', alpha=0.3)  # Diagonal line for perfect correlation plt.xlabel(\"Human Similarity Score\") plt.ylabel(\"Model Predicted Score (Normalized)\") plt.title(\"Human vs Model Similarity Scores\") plt.legend() plt.tight_layout() plt.show() In\u00a0[12]: Copied! <pre># Additional analysis: Distribution of error by word type\nprint(\"\\n\ud83d\udcc8 Error Analysis by Word Type:\")\n# Find abstract vs concrete words (simplified approach)\nconcrete_words = [\"car\", \"book\", \"water\", \"tree\", \"house\", \"food\", \"computer\", \"dog\", \"chair\", \"phone\"]\nabstract_words = [\"love\", \"time\", \"freedom\", \"idea\", \"peace\", \"happiness\", \"success\", \"truth\", \"theory\", \"beauty\"]\n\ndef word_type(row):\n    if row[\"word1\"] in concrete_words or row[\"word2\"] in concrete_words:\n        return \"Concrete\"\n    elif row[\"word1\"] in abstract_words or row[\"word2\"] in abstract_words:\n        return \"Abstract\"\n    else:\n        return \"Other\"\n\nresults_clean[\"word_type\"] = results_clean.apply(word_type, axis=1)\nresults_clean[\"w2v_error\"] = abs(results_clean[\"human_score\"] - results_clean[\"w2v_sim_norm\"])\nresults_clean[\"ft_error\"] = abs(results_clean[\"human_score\"] - results_clean[\"ft_sim_norm\"])\nresults_clean[\"elmo_error\"] = abs(results_clean[\"human_score\"] - results_clean[\"elmo_sim_norm\"])\n\n# Average error by word type\nerrors_by_type = results_clean.groupby(\"word_type\")[[\"w2v_error\", \"ft_error\", \"elmo_error\"]].mean()\nprint(errors_by_type)\n</pre> # Additional analysis: Distribution of error by word type print(\"\\n\ud83d\udcc8 Error Analysis by Word Type:\") # Find abstract vs concrete words (simplified approach) concrete_words = [\"car\", \"book\", \"water\", \"tree\", \"house\", \"food\", \"computer\", \"dog\", \"chair\", \"phone\"] abstract_words = [\"love\", \"time\", \"freedom\", \"idea\", \"peace\", \"happiness\", \"success\", \"truth\", \"theory\", \"beauty\"]  def word_type(row):     if row[\"word1\"] in concrete_words or row[\"word2\"] in concrete_words:         return \"Concrete\"     elif row[\"word1\"] in abstract_words or row[\"word2\"] in abstract_words:         return \"Abstract\"     else:         return \"Other\"  results_clean[\"word_type\"] = results_clean.apply(word_type, axis=1) results_clean[\"w2v_error\"] = abs(results_clean[\"human_score\"] - results_clean[\"w2v_sim_norm\"]) results_clean[\"ft_error\"] = abs(results_clean[\"human_score\"] - results_clean[\"ft_sim_norm\"]) results_clean[\"elmo_error\"] = abs(results_clean[\"human_score\"] - results_clean[\"elmo_sim_norm\"])  # Average error by word type errors_by_type = results_clean.groupby(\"word_type\")[[\"w2v_error\", \"ft_error\", \"elmo_error\"]].mean() print(errors_by_type) <pre>\n\ud83d\udcc8 Error Analysis by Word Type:\n           w2v_error  ft_error  elmo_error\nword_type                                 \nAbstract    0.937094  0.736520    1.326223\nConcrete    1.645436  1.124287    1.621554\nOther       1.460719  1.157303    1.489836\n</pre> In\u00a0[13]: Copied! <pre># Load dataset from HuggingFace\nprint(\"Loading Word Analogy dataset...\")\nanalogy_dataset = load_dataset(\"tomasmcz/word2vec_analogy\")\nprint(f\"\u2705 Loaded {len(analogy_dataset['train'])} analogy examples\")\n\n# Display sample analogies\nprint(\"\\n\ud83d\udcdd Example analogies:\")\nfor i in range(5):\n    ex = analogy_dataset['train'][i]\n    print(f\"{ex['word_a']} : {ex['word_b']} :: {ex['word_c']} : {ex['word_d']}\")\n</pre> # Load dataset from HuggingFace print(\"Loading Word Analogy dataset...\") analogy_dataset = load_dataset(\"tomasmcz/word2vec_analogy\") print(f\"\u2705 Loaded {len(analogy_dataset['train'])} analogy examples\")  # Display sample analogies print(\"\\n\ud83d\udcdd Example analogies:\") for i in range(5):     ex = analogy_dataset['train'][i]     print(f\"{ex['word_a']} : {ex['word_b']} :: {ex['word_c']} : {ex['word_d']}\")  <pre>Loading Word Analogy dataset...\n\u2705 Loaded 19544 analogy examples\n\n\ud83d\udcdd Example analogies:\nAthens : Greece :: Baghdad : Iraq\nAthens : Greece :: Bangkok : Thailand\nAthens : Greece :: Beijing : China\nAthens : Greece :: Berlin : Germany\nAthens : Greece :: Bern : Switzerland\n</pre> In\u00a0[14]: Copied! <pre>from tqdm import tqdm\n\ndef evaluate_analogy(model_type, dataset, sample_size=1000, show_progress=True):\n    \"\"\"\n    Evaluate embedding models on word analogy task.\n    \n    Args:\n        model_type: \"word2vec\", \"fasttext\", or \"elmo\"\n        dataset: The analogy dataset\n        sample_size: Number of examples to sample for evaluation\n        show_progress: Whether to show a progress bar\n        \n    Returns:\n        dict: Evaluation results\n    \"\"\"\n    # Sample from the dataset to keep evaluation time reasonable\n    if sample_size and sample_size &lt; len(dataset):\n        # Use dataset.select() with Python integers\n        indices = [int(i) for i in np.random.choice(len(dataset), sample_size, replace=False)]\n        evaluation_set = dataset.select(indices)\n    else:\n        evaluation_set = dataset\n    \n    total = len(evaluation_set)\n    correct = 0\n    skipped = 0\n    \n    # Keep track of examples where model succeeded or failed\n    correct_examples = []\n    incorrect_examples = []\n    \n    # Process each analogy\n    iterator = tqdm(range(len(evaluation_set))) if show_progress else range(len(evaluation_set))\n    for i in iterator:\n        example = evaluation_set[i]\n        word_a, word_b, word_c, word_d = example['word_a'], example['word_b'], example['word_c'], example['word_d']\n        \n        # Get embeddings based on model type\n        if model_type == \"word2vec\":\n            vec_a = get_word2vec_embedding(word_a)\n            vec_b = get_word2vec_embedding(word_b) \n            vec_c = get_word2vec_embedding(word_c)\n            \n        elif model_type == \"fasttext\":\n            vec_a = get_fasttext_embedding(word_a)\n            vec_b = get_fasttext_embedding(word_b)\n            vec_c = get_fasttext_embedding(word_c)\n            \n        elif model_type == \"elmo\":\n            # For ELMo, create minimal contexts for each word\n            template = \"The word {} is important.\"\n            try:\n                vec_a = get_elmo_embedding(template.format(word_a), word_a)\n                vec_b = get_elmo_embedding(template.format(word_b), word_b)\n                vec_c = get_elmo_embedding(template.format(word_c), word_c)\n                vec_d = get_elmo_embedding(template.format(word_d), word_d)\n                \n                # For ELMo, check if d is closer to (b-a+c) than a, b, c are\n                if vec_a is None or vec_b is None or vec_c is None or vec_d is None:\n                    skipped += 1\n                    continue\n                    \n                target_vec = vec_b - vec_a + vec_c\n                target_vec = target_vec / np.linalg.norm(target_vec)\n                \n                # Normalize vectors\n                vec_a_norm = vec_a / np.linalg.norm(vec_a)\n                vec_b_norm = vec_b / np.linalg.norm(vec_b)\n                vec_c_norm = vec_c / np.linalg.norm(vec_c)\n                vec_d_norm = vec_d / np.linalg.norm(vec_d)\n                \n                # Calculate similarities\n                similarity_d = np.dot(target_vec, vec_d_norm)\n                similarities = [\n                    np.dot(target_vec, vec_a_norm),\n                    np.dot(target_vec, vec_b_norm),\n                    np.dot(target_vec, vec_c_norm)\n                ]\n                \n                # If target is closer to d than to a, b, c, consider it correct\n                if similarity_d &gt; max(similarities):\n                    correct += 1\n                    correct_examples.append((word_a, word_b, word_c, word_d, word_d))\n                else:\n                    incorrect_examples.append((word_a, word_b, word_c, word_d, \"unknown\"))\n                \n                continue\n            except Exception as e:\n                skipped += 1\n                continue\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n        \n        # Skip if any embedding is not available (for Word2Vec and FastText)\n        if vec_a is None or vec_b is None or vec_c is None:\n            skipped += 1\n            continue\n        \n        # Calculate the target vector using the analogy formula: b - a + c \u2248 d\n        target_vec = vec_b - vec_a + vec_c\n        \n        # Normalize the target vector\n        target_vec = target_vec / np.linalg.norm(target_vec)\n        \n        # Find the nearest word to the target vector (excluding a, b, c)\n        candidates = []\n        candidate_words = set([word_d, word_a, word_b, word_c])\n        \n        for word in candidate_words:\n            if word in [word_a, word_b, word_c]:\n                continue  # Skip input words\n                \n            if model_type == \"word2vec\":\n                vec = get_word2vec_embedding(word)\n            elif model_type == \"fasttext\":\n                vec = get_fasttext_embedding(word)\n            \n            if vec is not None:\n                # Normalize the candidate vector\n                vec = vec / np.linalg.norm(vec)\n                similarity = np.dot(target_vec, vec)\n                candidates.append((word, similarity))\n        \n        if not candidates:\n            skipped += 1\n            continue\n            \n        # Sort by similarity (highest first)\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        predicted_word = candidates[0][0]\n        \n        # Check if the prediction is correct\n        is_correct = predicted_word.lower() == word_d.lower()\n        if is_correct:\n            correct += 1\n            correct_examples.append((word_a, word_b, word_c, word_d, predicted_word))\n        else:\n            incorrect_examples.append((word_a, word_b, word_c, word_d, predicted_word))\n    \n    # Calculate accuracy\n    evaluated = total - skipped\n    accuracy = correct / evaluated if evaluated &gt; 0 else 0\n    \n    return {\n        \"model\": model_type,\n        \"total\": total,\n        \"evaluated\": evaluated,\n        \"correct\": correct,\n        \"skipped\": skipped,\n        \"accuracy\": accuracy,\n        \"correct_examples\": correct_examples[:10],  # Save first 10 correct examples\n        \"incorrect_examples\": incorrect_examples[:10]  # Save first 10 incorrect examples\n    }\n</pre> from tqdm import tqdm  def evaluate_analogy(model_type, dataset, sample_size=1000, show_progress=True):     \"\"\"     Evaluate embedding models on word analogy task.          Args:         model_type: \"word2vec\", \"fasttext\", or \"elmo\"         dataset: The analogy dataset         sample_size: Number of examples to sample for evaluation         show_progress: Whether to show a progress bar              Returns:         dict: Evaluation results     \"\"\"     # Sample from the dataset to keep evaluation time reasonable     if sample_size and sample_size &lt; len(dataset):         # Use dataset.select() with Python integers         indices = [int(i) for i in np.random.choice(len(dataset), sample_size, replace=False)]         evaluation_set = dataset.select(indices)     else:         evaluation_set = dataset          total = len(evaluation_set)     correct = 0     skipped = 0          # Keep track of examples where model succeeded or failed     correct_examples = []     incorrect_examples = []          # Process each analogy     iterator = tqdm(range(len(evaluation_set))) if show_progress else range(len(evaluation_set))     for i in iterator:         example = evaluation_set[i]         word_a, word_b, word_c, word_d = example['word_a'], example['word_b'], example['word_c'], example['word_d']                  # Get embeddings based on model type         if model_type == \"word2vec\":             vec_a = get_word2vec_embedding(word_a)             vec_b = get_word2vec_embedding(word_b)              vec_c = get_word2vec_embedding(word_c)                      elif model_type == \"fasttext\":             vec_a = get_fasttext_embedding(word_a)             vec_b = get_fasttext_embedding(word_b)             vec_c = get_fasttext_embedding(word_c)                      elif model_type == \"elmo\":             # For ELMo, create minimal contexts for each word             template = \"The word {} is important.\"             try:                 vec_a = get_elmo_embedding(template.format(word_a), word_a)                 vec_b = get_elmo_embedding(template.format(word_b), word_b)                 vec_c = get_elmo_embedding(template.format(word_c), word_c)                 vec_d = get_elmo_embedding(template.format(word_d), word_d)                                  # For ELMo, check if d is closer to (b-a+c) than a, b, c are                 if vec_a is None or vec_b is None or vec_c is None or vec_d is None:                     skipped += 1                     continue                                      target_vec = vec_b - vec_a + vec_c                 target_vec = target_vec / np.linalg.norm(target_vec)                                  # Normalize vectors                 vec_a_norm = vec_a / np.linalg.norm(vec_a)                 vec_b_norm = vec_b / np.linalg.norm(vec_b)                 vec_c_norm = vec_c / np.linalg.norm(vec_c)                 vec_d_norm = vec_d / np.linalg.norm(vec_d)                                  # Calculate similarities                 similarity_d = np.dot(target_vec, vec_d_norm)                 similarities = [                     np.dot(target_vec, vec_a_norm),                     np.dot(target_vec, vec_b_norm),                     np.dot(target_vec, vec_c_norm)                 ]                                  # If target is closer to d than to a, b, c, consider it correct                 if similarity_d &gt; max(similarities):                     correct += 1                     correct_examples.append((word_a, word_b, word_c, word_d, word_d))                 else:                     incorrect_examples.append((word_a, word_b, word_c, word_d, \"unknown\"))                                  continue             except Exception as e:                 skipped += 1                 continue         else:             raise ValueError(f\"Unknown model type: {model_type}\")                  # Skip if any embedding is not available (for Word2Vec and FastText)         if vec_a is None or vec_b is None or vec_c is None:             skipped += 1             continue                  # Calculate the target vector using the analogy formula: b - a + c \u2248 d         target_vec = vec_b - vec_a + vec_c                  # Normalize the target vector         target_vec = target_vec / np.linalg.norm(target_vec)                  # Find the nearest word to the target vector (excluding a, b, c)         candidates = []         candidate_words = set([word_d, word_a, word_b, word_c])                  for word in candidate_words:             if word in [word_a, word_b, word_c]:                 continue  # Skip input words                              if model_type == \"word2vec\":                 vec = get_word2vec_embedding(word)             elif model_type == \"fasttext\":                 vec = get_fasttext_embedding(word)                          if vec is not None:                 # Normalize the candidate vector                 vec = vec / np.linalg.norm(vec)                 similarity = np.dot(target_vec, vec)                 candidates.append((word, similarity))                  if not candidates:             skipped += 1             continue                      # Sort by similarity (highest first)         candidates.sort(key=lambda x: x[1], reverse=True)         predicted_word = candidates[0][0]                  # Check if the prediction is correct         is_correct = predicted_word.lower() == word_d.lower()         if is_correct:             correct += 1             correct_examples.append((word_a, word_b, word_c, word_d, predicted_word))         else:             incorrect_examples.append((word_a, word_b, word_c, word_d, predicted_word))          # Calculate accuracy     evaluated = total - skipped     accuracy = correct / evaluated if evaluated &gt; 0 else 0          return {         \"model\": model_type,         \"total\": total,         \"evaluated\": evaluated,         \"correct\": correct,         \"skipped\": skipped,         \"accuracy\": accuracy,         \"correct_examples\": correct_examples[:10],  # Save first 10 correct examples         \"incorrect_examples\": incorrect_examples[:10]  # Save first 10 incorrect examples     } In\u00a0[15]: Copied! <pre># Evaluate Word2Vec\nprint(\"\\n\ud83d\udd0d Evaluating Word2Vec on analogy task...\")\nw2v_results = evaluate_analogy(\"word2vec\", analogy_dataset['train'], sample_size=1000)\n\n# Evaluate FastText\nprint(\"\\n\ud83d\udd0d Evaluating FastText on analogy task...\")\nft_results = evaluate_analogy(\"fasttext\", analogy_dataset['train'], sample_size=1000)\n\n# Evaluate ELMo\nprint(\"\\n\ud83d\udd0d Evaluating ELMo on analogy task...\")\nelmo_results = evaluate_analogy(\"elmo\", analogy_dataset['train'], sample_size=1000)\n</pre> # Evaluate Word2Vec print(\"\\n\ud83d\udd0d Evaluating Word2Vec on analogy task...\") w2v_results = evaluate_analogy(\"word2vec\", analogy_dataset['train'], sample_size=1000)  # Evaluate FastText print(\"\\n\ud83d\udd0d Evaluating FastText on analogy task...\") ft_results = evaluate_analogy(\"fasttext\", analogy_dataset['train'], sample_size=1000)  # Evaluate ELMo print(\"\\n\ud83d\udd0d Evaluating ELMo on analogy task...\") elmo_results = evaluate_analogy(\"elmo\", analogy_dataset['train'], sample_size=1000) <pre>\n\ud83d\udd0d Evaluating Word2Vec on analogy task...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 5542.44it/s]\n</pre> <pre>\n\ud83d\udd0d Evaluating FastText on analogy task...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 14123.52it/s]\n</pre> <pre>\n\ud83d\udd0d Evaluating ELMo on analogy task...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [03:51&lt;00:00,  4.31it/s]\n</pre> In\u00a0[16]: Copied! <pre># Accuracy summary\nprint(\"\\n\ud83d\udcca Word Analogy Results:\")\nprint(f\"Word2Vec Accuracy: {w2v_results['accuracy']:.4f} ({w2v_results['correct']}/{w2v_results['evaluated']} correct, {w2v_results['skipped']} skipped)\")\nprint(f\"FastText Accuracy: {ft_results['accuracy']:.4f} ({ft_results['correct']}/{ft_results['evaluated']} correct, {ft_results['skipped']} skipped)\")\nprint(f\"ELMo Accuracy: {elmo_results['accuracy']:.4f} ({elmo_results['correct']}/{elmo_results['evaluated']} correct, {elmo_results['skipped']} skipped)\")\n\n# Sample predictions\nprint(\"\\n\u2705 Example correct predictions (Word2Vec):\")\nfor ex in w2v_results['correct_examples'][:5]:\n    print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} \u2713\")\n\nprint(\"\\n\u274c Example incorrect predictions (Word2Vec):\")\nfor ex in w2v_results['incorrect_examples'][:5]:\n    print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} (predicted: {ex[4]}) \u2717\")\n\nprint(\"\\n\u2705 Example correct predictions (FastText):\")\nfor ex in ft_results['correct_examples'][:5]:\n    print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} \u2713\")\n\nprint(\"\\n\u274c Example incorrect predictions (FastText):\")\nfor ex in ft_results['incorrect_examples'][:5]:\n    print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} (predicted: {ex[4]}) \u2717\")\n\nprint(\"\\n\u2705 Example correct predictions (ELMo):\")\nfor ex in elmo_results['correct_examples'][:5]:\n    print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} \u2713\")\n\nprint(\"\\n\u274c Example incorrect predictions (ELMo):\")\nfor ex in elmo_results['incorrect_examples'][:5]:\n    print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} (predicted: {ex[4]}) \u2717\")\n</pre> # Accuracy summary print(\"\\n\ud83d\udcca Word Analogy Results:\") print(f\"Word2Vec Accuracy: {w2v_results['accuracy']:.4f} ({w2v_results['correct']}/{w2v_results['evaluated']} correct, {w2v_results['skipped']} skipped)\") print(f\"FastText Accuracy: {ft_results['accuracy']:.4f} ({ft_results['correct']}/{ft_results['evaluated']} correct, {ft_results['skipped']} skipped)\") print(f\"ELMo Accuracy: {elmo_results['accuracy']:.4f} ({elmo_results['correct']}/{elmo_results['evaluated']} correct, {elmo_results['skipped']} skipped)\")  # Sample predictions print(\"\\n\u2705 Example correct predictions (Word2Vec):\") for ex in w2v_results['correct_examples'][:5]:     print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} \u2713\")  print(\"\\n\u274c Example incorrect predictions (Word2Vec):\") for ex in w2v_results['incorrect_examples'][:5]:     print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} (predicted: {ex[4]}) \u2717\")  print(\"\\n\u2705 Example correct predictions (FastText):\") for ex in ft_results['correct_examples'][:5]:     print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} \u2713\")  print(\"\\n\u274c Example incorrect predictions (FastText):\") for ex in ft_results['incorrect_examples'][:5]:     print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} (predicted: {ex[4]}) \u2717\")  print(\"\\n\u2705 Example correct predictions (ELMo):\") for ex in elmo_results['correct_examples'][:5]:     print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} \u2713\")  print(\"\\n\u274c Example incorrect predictions (ELMo):\") for ex in elmo_results['incorrect_examples'][:5]:     print(f\"{ex[0]} : {ex[1]} :: {ex[2]} : {ex[3]} (predicted: {ex[4]}) \u2717\") <pre>\n\ud83d\udcca Word Analogy Results:\nWord2Vec Accuracy: 1.0000 (1000/1000 correct, 0 skipped)\nFastText Accuracy: 1.0000 (1000/1000 correct, 0 skipped)\nELMo Accuracy: 0.0420 (42/1000 correct, 0 skipped)\n\n\u2705 Example correct predictions (Word2Vec):\nUkraine : hryvnia :: Denmark : krone \u2713\nPeru : Peruvian :: Austria : Austrian \u2713\nHonolulu : Hawaii :: Anchorage : Alaska \u2713\nmango : mangoes :: rat : rats \u2713\nIreland : Irish :: China : Chinese \u2713\n\n\u274c Example incorrect predictions (Word2Vec):\n\n\u2705 Example correct predictions (FastText):\nBelmopan : Belize :: Helsinki : Finland \u2713\nPortugal : Portuguese :: Denmark : Danish \u2713\nLondon : England :: Tallinn : Estonia \u2713\nLusaka : Zambia :: Nouakchott : Mauritania \u2713\nBelarus : Belorussian :: Brazil : Brazilian \u2713\n\n\u274c Example incorrect predictions (FastText):\n\n\u2705 Example correct predictions (ELMo):\ndescribing : described :: taking : took \u2713\nlow : lowest :: weak : weakest \u2713\nhot : hottest :: strong : strongest \u2713\nSweden : Swedish :: Australia : Australian \u2713\nstrange : strangest :: strong : strongest \u2713\n\n\u274c Example incorrect predictions (ELMo):\nNicosia : Cyprus :: Paramaribo : Suriname (predicted: unknown) \u2717\nNassau : Bahamas :: Nuuk : Greenland (predicted: unknown) \u2717\nWorcester : Massachusetts :: Houston : Texas (predicted: unknown) \u2717\nbig : biggest :: bright : brightest (predicted: unknown) \u2717\nGaborone : Botswana :: Mogadishu : Somalia (predicted: unknown) \u2717\n</pre> In\u00a0[17]: Copied! <pre>plt.figure(figsize=(10, 6))\nmodels = [\"Word2Vec\", \"FastText\", \"ELMo\"]\naccuracies = [w2v_results['accuracy'], ft_results['accuracy'], elmo_results['accuracy']]\nplt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])\nplt.title(\"Word Analogy Task Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0, 1.1)\nfor i, v in enumerate(accuracies):\n    plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(10, 6)) models = [\"Word2Vec\", \"FastText\", \"ELMo\"] accuracies = [w2v_results['accuracy'], ft_results['accuracy'], elmo_results['accuracy']] plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'lightcoral']) plt.title(\"Word Analogy Task Accuracy\") plt.ylabel(\"Accuracy\") plt.ylim(0, 1.1) for i, v in enumerate(accuracies):     plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center') plt.tight_layout() plt.show()  In\u00a0[18]: Copied! <pre>from sklearn.manifold import TSNE\n\nsemantic_categories = {\n    \"Colors\": [\n        \"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"brown\", \n        \"black\", \"white\", \"gray\", \"violet\", \"indigo\", \"teal\", \"maroon\", \"cyan\", \n        \"magenta\", \"turquoise\", \"lavender\", \"crimson\"\n    ],\n    \"Cities\": [\n        \"paris\", \"london\", \"berlin\", \"tokyo\", \"rome\", \"madrid\", \"moscow\", \"beijing\",\n        \"delhi\", \"cairo\", \"sydney\", \"istanbul\", \"dubai\", \"bangkok\", \"singapore\",\n        \"chicago\", \"toronto\", \"barcelona\", \"seoul\", \"stockholm\"\n    ],\n    \"Animals\": [\n        \"dog\", \"cat\", \"lion\", \"tiger\", \"elephant\", \"giraffe\", \"zebra\", \"monkey\",\n        \"bear\", \"wolf\", \"fox\", \"deer\", \"rabbit\", \"snake\", \"eagle\", \"dolphin\",\n        \"shark\", \"penguin\", \"koala\", \"kangaroo\"\n    ],\n    \"Family\": [\n        \"mother\", \"father\", \"sister\", \"brother\", \"son\", \"daughter\", \"uncle\",\n        \"aunt\", \"cousin\", \"grandfather\", \"grandmother\", \"husband\", \"wife\",\n        \"parent\", \"child\", \"niece\", \"nephew\", \"sibling\", \"family\", \"relative\"\n    ],\n    \"Emotions\": [\n        \"happy\", \"sad\", \"angry\", \"excited\", \"afraid\", \"joyful\", \"depressed\",\n        \"anxious\", \"content\", \"curious\", \"surprised\", \"disgusted\", \"bored\",\n        \"calm\", \"proud\", \"ashamed\", \"jealous\", \"hopeful\", \"love\", \"hate\"\n    ],\n    \"Food\": [\n        \"bread\", \"rice\", \"pasta\", \"cheese\", \"meat\", \"chicken\", \"beef\", \"fish\",\n        \"apple\", \"banana\", \"orange\", \"pizza\", \"chocolate\", \"soup\", \"salad\",\n        \"burger\", \"sandwich\", \"vegetable\", \"fruit\", \"dessert\"\n    ],\n    \"Technology\": [\n        \"computer\", \"phone\", \"internet\", \"software\", \"website\", \"app\", \"data\",\n        \"algorithm\", \"network\", \"digital\", \"robot\", \"camera\", \"screen\", \"device\",\n        \"program\", \"wireless\", \"battery\", \"processor\", \"storage\", \"keyboard\"\n    ]\n}\n\ndef extract_embeddings_for_visualization(categories, models=[\"word2vec\", \"fasttext\", \"elmo\"]):\n    \"\"\"\n    Extracts normalized embeddings for all words in all categories using each model.\n    \"\"\"\n    result = {}\n\n    for model_name in models:\n        print(f\"\ud83d\udd04 Extracting embeddings for {model_name}...\")\n        all_embeddings, all_words, all_categories = [], [], []\n\n        for category, words in categories.items():\n            for word in words:\n                if model_name == \"word2vec\":\n                    embedding = get_word2vec_embedding(word)\n                elif model_name == \"fasttext\":\n                    embedding = get_fasttext_embedding(word)\n                elif model_name == \"elmo\":\n                    try:\n                        embedding = get_elmo_embedding(f\"The word {word} is commonly used.\", word)\n                    except:\n                        embedding = None\n\n                if embedding is not None:\n                    all_embeddings.append(embedding)\n                    all_words.append(word)\n                    all_categories.append(category)\n\n        if all_embeddings:\n            norm = np.linalg.norm(all_embeddings, axis=1, keepdims=True)\n            normalized_embeddings = np.array(all_embeddings) / norm\n            result[model_name] = (normalized_embeddings, all_words, all_categories)\n\n    return result\n</pre> from sklearn.manifold import TSNE  semantic_categories = {     \"Colors\": [         \"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"brown\",          \"black\", \"white\", \"gray\", \"violet\", \"indigo\", \"teal\", \"maroon\", \"cyan\",          \"magenta\", \"turquoise\", \"lavender\", \"crimson\"     ],     \"Cities\": [         \"paris\", \"london\", \"berlin\", \"tokyo\", \"rome\", \"madrid\", \"moscow\", \"beijing\",         \"delhi\", \"cairo\", \"sydney\", \"istanbul\", \"dubai\", \"bangkok\", \"singapore\",         \"chicago\", \"toronto\", \"barcelona\", \"seoul\", \"stockholm\"     ],     \"Animals\": [         \"dog\", \"cat\", \"lion\", \"tiger\", \"elephant\", \"giraffe\", \"zebra\", \"monkey\",         \"bear\", \"wolf\", \"fox\", \"deer\", \"rabbit\", \"snake\", \"eagle\", \"dolphin\",         \"shark\", \"penguin\", \"koala\", \"kangaroo\"     ],     \"Family\": [         \"mother\", \"father\", \"sister\", \"brother\", \"son\", \"daughter\", \"uncle\",         \"aunt\", \"cousin\", \"grandfather\", \"grandmother\", \"husband\", \"wife\",         \"parent\", \"child\", \"niece\", \"nephew\", \"sibling\", \"family\", \"relative\"     ],     \"Emotions\": [         \"happy\", \"sad\", \"angry\", \"excited\", \"afraid\", \"joyful\", \"depressed\",         \"anxious\", \"content\", \"curious\", \"surprised\", \"disgusted\", \"bored\",         \"calm\", \"proud\", \"ashamed\", \"jealous\", \"hopeful\", \"love\", \"hate\"     ],     \"Food\": [         \"bread\", \"rice\", \"pasta\", \"cheese\", \"meat\", \"chicken\", \"beef\", \"fish\",         \"apple\", \"banana\", \"orange\", \"pizza\", \"chocolate\", \"soup\", \"salad\",         \"burger\", \"sandwich\", \"vegetable\", \"fruit\", \"dessert\"     ],     \"Technology\": [         \"computer\", \"phone\", \"internet\", \"software\", \"website\", \"app\", \"data\",         \"algorithm\", \"network\", \"digital\", \"robot\", \"camera\", \"screen\", \"device\",         \"program\", \"wireless\", \"battery\", \"processor\", \"storage\", \"keyboard\"     ] }  def extract_embeddings_for_visualization(categories, models=[\"word2vec\", \"fasttext\", \"elmo\"]):     \"\"\"     Extracts normalized embeddings for all words in all categories using each model.     \"\"\"     result = {}      for model_name in models:         print(f\"\ud83d\udd04 Extracting embeddings for {model_name}...\")         all_embeddings, all_words, all_categories = [], [], []          for category, words in categories.items():             for word in words:                 if model_name == \"word2vec\":                     embedding = get_word2vec_embedding(word)                 elif model_name == \"fasttext\":                     embedding = get_fasttext_embedding(word)                 elif model_name == \"elmo\":                     try:                         embedding = get_elmo_embedding(f\"The word {word} is commonly used.\", word)                     except:                         embedding = None                  if embedding is not None:                     all_embeddings.append(embedding)                     all_words.append(word)                     all_categories.append(category)          if all_embeddings:             norm = np.linalg.norm(all_embeddings, axis=1, keepdims=True)             normalized_embeddings = np.array(all_embeddings) / norm             result[model_name] = (normalized_embeddings, all_words, all_categories)      return result In\u00a0[19]: Copied! <pre>def visualize_embeddings(embeddings_dict, perplexity=30, n_iter=1000):\n    \"\"\"\n    Run t-SNE and plot embeddings colored by category.\n    \"\"\"\n    unique_categories = list(set(cat for _, _, cats in embeddings_dict.values() for cat in cats))\n    color_map = {cat: sns.color_palette(\"husl\", len(unique_categories))[i] for i, cat in enumerate(unique_categories)}\n\n    fig, axes = plt.subplots(1, len(embeddings_dict), figsize=(6 * len(embeddings_dict), 6))\n    if len(embeddings_dict) == 1:\n        axes = [axes]\n\n    for i, (model, (embeds, words, categories)) in enumerate(embeddings_dict.items()):\n        print(f\"\ud83e\uddec Running t-SNE for {model}...\")\n        reduced = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter).fit_transform(embeds)\n\n        for category in set(categories):\n            idxs = [j for j, c in enumerate(categories) if c == category]\n            axes[i].scatter(reduced[idxs, 0], reduced[idxs, 1], label=category, color=color_map[category], alpha=0.7)\n\n        # Label a subset of words\n        sample = range(0, len(words), len(words) // 30 + 1)\n        for idx in sample:\n            axes[i].annotate(words[idx], (reduced[idx, 0], reduced[idx, 1]), fontsize=7, alpha=0.6)\n\n        axes[i].set_title(f\"{model.capitalize()} Embeddings\")\n        axes[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        axes[i].set_xticks([]), axes[i].set_yticks([])\n\n    plt.tight_layout()\n    plt.savefig(\"embedding_clusters.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n</pre> def visualize_embeddings(embeddings_dict, perplexity=30, n_iter=1000):     \"\"\"     Run t-SNE and plot embeddings colored by category.     \"\"\"     unique_categories = list(set(cat for _, _, cats in embeddings_dict.values() for cat in cats))     color_map = {cat: sns.color_palette(\"husl\", len(unique_categories))[i] for i, cat in enumerate(unique_categories)}      fig, axes = plt.subplots(1, len(embeddings_dict), figsize=(6 * len(embeddings_dict), 6))     if len(embeddings_dict) == 1:         axes = [axes]      for i, (model, (embeds, words, categories)) in enumerate(embeddings_dict.items()):         print(f\"\ud83e\uddec Running t-SNE for {model}...\")         reduced = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter).fit_transform(embeds)          for category in set(categories):             idxs = [j for j, c in enumerate(categories) if c == category]             axes[i].scatter(reduced[idxs, 0], reduced[idxs, 1], label=category, color=color_map[category], alpha=0.7)          # Label a subset of words         sample = range(0, len(words), len(words) // 30 + 1)         for idx in sample:             axes[i].annotate(words[idx], (reduced[idx, 0], reduced[idx, 1]), fontsize=7, alpha=0.6)          axes[i].set_title(f\"{model.capitalize()} Embeddings\")         axes[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')         axes[i].set_xticks([]), axes[i].set_yticks([])      plt.tight_layout()     plt.savefig(\"embedding_clusters.png\", dpi=300, bbox_inches=\"tight\")     plt.show()  In\u00a0[20]: Copied! <pre>embeddings_data = extract_embeddings_for_visualization(semantic_categories)\nvisualize_embeddings(embeddings_data)\n</pre> embeddings_data = extract_embeddings_for_visualization(semantic_categories) visualize_embeddings(embeddings_data) <pre>\ud83d\udd04 Extracting embeddings for word2vec...\n\ud83d\udd04 Extracting embeddings for fasttext...\n\ud83d\udd04 Extracting embeddings for elmo...\n\ud83e\uddec Running t-SNE for word2vec...\n</pre> <pre>/Users/agomberto/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n  warnings.warn(\npython(74732) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n</pre> <pre>\ud83e\uddec Running t-SNE for fasttext...\n</pre> <pre>/Users/agomberto/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n  warnings.warn(\n</pre> <pre>\ud83e\uddec Running t-SNE for elmo...\n</pre> <pre>/Users/agomberto/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n  warnings.warn(\n</pre> <ol> <li><p>All three models \u2014 Word2Vec, FastText, and ELMo \u2014 were able to form meaningful semantic clusters.</p> <ul> <li>For example, animals grouped with animals, cities with cities, emotions with emotions.</li> <li>Even ELMo, despite being contextual, produced strong clusters using a fixed sentence template.</li> </ul> </li> <li><p>Word2Vec tends to create more separated clusters.</p> <ul> <li>One potential reason is that its embeddings are trained on global co-occurrence statistics, optimizing for broader distinctions between word meanings across the corpus.</li> <li>In contrast, ELMo is designed for context flexibility, which might compress semantic space slightly when averaged over generic templates.</li> </ul> </li> </ol> In\u00a0[21]: Copied! <pre>def compute_cluster_metrics(embeddings_dict):\n    from sklearn.metrics import silhouette_score\n    from scipy.spatial.distance import pdist, squareform\n\n    metrics = []\n\n    for model, (embeds, _, categories) in embeddings_dict.items():\n        labels = pd.factorize(categories)[0]\n        dists = squareform(pdist(embeds, 'cosine'))\n\n        # Intra-cluster\n        intra = []\n        for cat in set(categories):\n            idx = [i for i, c in enumerate(categories) if c == cat]\n            if len(idx) &gt;= 2:\n                intra.append(np.mean(dists[np.ix_(idx, idx)][np.triu_indices(len(idx), 1)]))\n        avg_intra = np.mean(intra)\n\n        # Inter-cluster\n        inter = []\n        for i, c1 in enumerate(set(categories)):\n            for j, c2 in enumerate(set(categories)):\n                if j &lt;= i: continue\n                idx1 = [ix for ix, c in enumerate(categories) if c == c1]\n                idx2 = [ix for ix, c in enumerate(categories) if c == c2]\n                inter.append(np.mean(dists[np.ix_(idx1, idx2)]))\n        avg_inter = np.mean(inter)\n\n        # Silhouette score\n        try:\n            sil = silhouette_score(embeds, labels, metric='cosine')\n        except:\n            sil = np.nan\n\n        metrics.append({\n            \"Model\": model,\n            \"Intra-cluster Distance\": avg_intra,\n            \"Inter-cluster Distance\": avg_inter,\n            \"Cluster Separation\": avg_inter - avg_intra,\n            \"Silhouette Score\": sil\n        })\n\n    return pd.DataFrame(metrics)\n\nclustering_metrics = compute_cluster_metrics(embeddings_data)\nprint(clustering_metrics)\n</pre> def compute_cluster_metrics(embeddings_dict):     from sklearn.metrics import silhouette_score     from scipy.spatial.distance import pdist, squareform      metrics = []      for model, (embeds, _, categories) in embeddings_dict.items():         labels = pd.factorize(categories)[0]         dists = squareform(pdist(embeds, 'cosine'))          # Intra-cluster         intra = []         for cat in set(categories):             idx = [i for i, c in enumerate(categories) if c == cat]             if len(idx) &gt;= 2:                 intra.append(np.mean(dists[np.ix_(idx, idx)][np.triu_indices(len(idx), 1)]))         avg_intra = np.mean(intra)          # Inter-cluster         inter = []         for i, c1 in enumerate(set(categories)):             for j, c2 in enumerate(set(categories)):                 if j &lt;= i: continue                 idx1 = [ix for ix, c in enumerate(categories) if c == c1]                 idx2 = [ix for ix, c in enumerate(categories) if c == c2]                 inter.append(np.mean(dists[np.ix_(idx1, idx2)]))         avg_inter = np.mean(inter)          # Silhouette score         try:             sil = silhouette_score(embeds, labels, metric='cosine')         except:             sil = np.nan          metrics.append({             \"Model\": model,             \"Intra-cluster Distance\": avg_intra,             \"Inter-cluster Distance\": avg_inter,             \"Cluster Separation\": avg_inter - avg_intra,             \"Silhouette Score\": sil         })      return pd.DataFrame(metrics)  clustering_metrics = compute_cluster_metrics(embeddings_data) print(clustering_metrics)  <pre>      Model  Intra-cluster Distance  Inter-cluster Distance  \\\n0  word2vec                0.596058                0.914031   \n1  fasttext                0.564327                0.885210   \n2      elmo                0.725275                0.923759   \n\n   Cluster Separation  Silhouette Score  \n0            0.317973          0.293279  \n1            0.320883          0.298813  \n2            0.198485          0.148961  \n</pre> <pre>/var/folders/2z/g737jg9d2jj206wkf56g7gyc0000gn/T/ipykernel_71449/793317931.py:8: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  labels = pd.factorize(categories)[0]\n/var/folders/2z/g737jg9d2jj206wkf56g7gyc0000gn/T/ipykernel_71449/793317931.py:8: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  labels = pd.factorize(categories)[0]\n/var/folders/2z/g737jg9d2jj206wkf56g7gyc0000gn/T/ipykernel_71449/793317931.py:8: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  labels = pd.factorize(categories)[0]\n</pre> In\u00a0[22]: Copied! <pre>plt.figure(figsize=(10, 6))\nx = np.arange(len(clustering_metrics))\nwidth = 0.2\ncolors = ['#ff9999', '#66b3ff', '#99ff99']\n\nfor i, metric in enumerate([\"Intra-cluster Distance\", \"Inter-cluster Distance\", \"Silhouette Score\"]):\n    plt.bar(x + i*width - width, clustering_metrics[metric], width, label=metric, color=colors[i])\n\nplt.xticks(x, clustering_metrics[\"Model\"])\nplt.ylabel(\"Score\")\nplt.title(\"Clustering Quality by Model\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"clustering_metrics.png\", dpi=300)\nplt.show()\n</pre> plt.figure(figsize=(10, 6)) x = np.arange(len(clustering_metrics)) width = 0.2 colors = ['#ff9999', '#66b3ff', '#99ff99']  for i, metric in enumerate([\"Intra-cluster Distance\", \"Inter-cluster Distance\", \"Silhouette Score\"]):     plt.bar(x + i*width - width, clustering_metrics[metric], width, label=metric, color=colors[i])  plt.xticks(x, clustering_metrics[\"Model\"]) plt.ylabel(\"Score\") plt.title(\"Clustering Quality by Model\") plt.legend() plt.tight_layout() plt.savefig(\"clustering_metrics.png\", dpi=300) plt.show()  In\u00a0[23]: Copied! <pre>import random\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)\n\n# Load dataset\nprint(\"Loading CoNLL-2003 dataset...\")\nconll_dataset = load_dataset(\"eriktks/conll2003\")\n\n# Subset\ntrain_dataset = conll_dataset[\"train\"].select(range(5000))\ntest_dataset = conll_dataset[\"test\"].select(range(1000))\nval_dataset = conll_dataset[\"validation\"].select(range(500))\n\nprint(f\"Train set: {len(train_dataset)} examples\")\nprint(f\"Validation set: {len(val_dataset)} examples\")\nprint(f\"Test set: {len(test_dataset)} examples\")\n\n# Inspect a sample\nsample = train_dataset[0]\nprint(\"\\nSample:\")\nprint(\"Tokens:\", sample['tokens'])\nprint(\"NER Tags:\", sample['ner_tags'])\n</pre> import random  # Reproducibility np.random.seed(42) tf.random.set_seed(42) random.seed(42)  # Load dataset print(\"Loading CoNLL-2003 dataset...\") conll_dataset = load_dataset(\"eriktks/conll2003\")  # Subset train_dataset = conll_dataset[\"train\"].select(range(5000)) test_dataset = conll_dataset[\"test\"].select(range(1000)) val_dataset = conll_dataset[\"validation\"].select(range(500))  print(f\"Train set: {len(train_dataset)} examples\") print(f\"Validation set: {len(val_dataset)} examples\") print(f\"Test set: {len(test_dataset)} examples\")  # Inspect a sample sample = train_dataset[0] print(\"\\nSample:\") print(\"Tokens:\", sample['tokens']) print(\"NER Tags:\", sample['ner_tags']) <pre>Loading CoNLL-2003 dataset...\nTrain set: 5000 examples\nValidation set: 500 examples\nTest set: 1000 examples\n\nSample:\nTokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\nNER Tags: [3, 0, 7, 0, 0, 0, 7, 0, 0]\n</pre> In\u00a0[24]: Copied! <pre>from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\n# Tag mapping\ntag_names = [\n    'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n    'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'\n]\nnum_tags = len(tag_names)\nprint(\"\\nNER Tags:\")\nfor i, tag in enumerate(tag_names):\n    print(f\"{i}: {tag}\")\n\n# Build vocabulary\nword_to_idx = {'PAD': 0, 'UNK': 1}\nfor example in train_dataset:\n    for token in example['tokens']:\n        if token not in word_to_idx:\n            word_to_idx[token] = len(word_to_idx)\n\nprint(f\"\\nVocabulary size: {len(word_to_idx)}\")\n\n# Max sequence length\nmax_len = min(max(len(example['tokens']) for example in train_dataset), 128)\nprint(f\"Max sequence length: {max_len}\")\n</pre> from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.utils import to_categorical  # Tag mapping tag_names = [     'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',     'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC' ] num_tags = len(tag_names) print(\"\\nNER Tags:\") for i, tag in enumerate(tag_names):     print(f\"{i}: {tag}\")  # Build vocabulary word_to_idx = {'PAD': 0, 'UNK': 1} for example in train_dataset:     for token in example['tokens']:         if token not in word_to_idx:             word_to_idx[token] = len(word_to_idx)  print(f\"\\nVocabulary size: {len(word_to_idx)}\")  # Max sequence length max_len = min(max(len(example['tokens']) for example in train_dataset), 128) print(f\"Max sequence length: {max_len}\") <pre>\nNER Tags:\n0: O\n1: B-PER\n2: I-PER\n3: B-ORG\n4: I-ORG\n5: B-LOC\n6: I-LOC\n7: B-MISC\n8: I-MISC\n\nVocabulary size: 12244\nMax sequence length: 60\n</pre> In\u00a0[25]: Copied! <pre>def prepare_baseline_data(dataset):\n    X = []\n    y = []\n    sequence_lengths = []\n    \n    for example in dataset:\n        # Get sequence length\n        seq_len = min(len(example['tokens']), max_len)\n        sequence_lengths.append(seq_len)\n        \n        # Convert tokens to indices\n        token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in example['tokens']]\n        # Pad or truncate to max_len\n        padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]\n        \n        # One-hot encode labels\n        labels = to_categorical([tag for tag in example['ner_tags']], num_classes=num_tags)\n        # Pad labels\n        padded_labels = np.zeros((max_len, num_tags))\n        padded_labels[:min(len(labels), max_len)] = labels[:min(len(labels), max_len)]\n        \n        X.append(padded_indices)\n        y.append(padded_labels)\n    \n    return np.array(X), np.array(y), sequence_lengths\n</pre> def prepare_baseline_data(dataset):     X = []     y = []     sequence_lengths = []          for example in dataset:         # Get sequence length         seq_len = min(len(example['tokens']), max_len)         sequence_lengths.append(seq_len)                  # Convert tokens to indices         token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in example['tokens']]         # Pad or truncate to max_len         padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]                  # One-hot encode labels         labels = to_categorical([tag for tag in example['ner_tags']], num_classes=num_tags)         # Pad labels         padded_labels = np.zeros((max_len, num_tags))         padded_labels[:min(len(labels), max_len)] = labels[:min(len(labels), max_len)]                  X.append(padded_indices)         y.append(padded_labels)          return np.array(X), np.array(y), sequence_lengths  In\u00a0[38]: Copied! <pre>def get_embedding_dim(embedding_type):\n    \"\"\"Return the dimensionality of each embedding type\"\"\"\n    if embedding_type == \"word2vec\":\n        return 300  # Word2Vec dimension\n    elif embedding_type == \"fasttext\":\n        return 300  # FastText dimension\n    elif embedding_type == \"elmo\":\n        return 512  # ELMo dimension\n    else:\n        raise ValueError(f\"Unknown embedding type: {embedding_type}\")\n\ndef prepare_pretrained_data(dataset, embedding_type):\n    \"\"\"Prepare data with pre-trained embeddings - optimized version\"\"\"\n    X_indices = []\n    y = []\n    sequence_lengths = []\n    \n    # First pass: collect tokens and prepare indices and labels\n    for example in dataset:\n        tokens = example['tokens']\n        seq_len = min(len(tokens), max_len)\n        sequence_lengths.append(seq_len)\n        \n        # Convert tokens to indices\n        token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in tokens]\n        # Pad to max_len\n        padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]\n        X_indices.append(padded_indices)\n        \n        # Process labels\n        labels = to_categorical([tag for tag in example['ner_tags']], num_classes=num_tags)\n        padded_labels = np.zeros((max_len, num_tags))\n        padded_labels[:min(len(labels), max_len)] = labels[:min(len(labels), max_len)]\n        y.append(padded_labels)\n    \n    X_indices = np.array(X_indices)\n    y = np.array(y)\n    \n    # Determine embedding dimension\n    embedding_dim = 300  # Default for Word2Vec and FastText\n    if embedding_type == \"elmo\":\n        embedding_dim = 512\n    \n    # Create empty embedding matrix\n    X_embeddings = np.zeros((len(dataset), max_len, embedding_dim))\n    \n    # Extract embeddings based on embedding type\n    print(f\"Extracting {embedding_type} embeddings...\")\n    \n    if embedding_type == \"elmo\":\n        # Optimized batch processing for ELMo\n        batch_size = 32  # Process this many examples at once\n        \n        for batch_start in tqdm(range(0, len(dataset), batch_size)):\n            batch_end = min(batch_start + batch_size, len(dataset))\n            \n            # Process each example in the batch\n            for i in range(batch_end - batch_start):\n                # Get actual index in the full dataset\n                example_idx = batch_start + i\n                \n                # Get current example from the dataset\n                if example_idx &lt; len(dataset):\n                    example = dataset[example_idx]\n                    \n                    # Get tokens for this example\n                    tokens = example['tokens'][:max_len]\n                    \n                    # Create a single sentence from all tokens to get embeddings for the whole sequence at once\n                    full_sentence = \" \".join(tokens)\n                    \n                    try:\n                        # Get embeddings for the entire sentence at once\n                        embeddings = elmo_model.signatures[\"default\"](\n                            tf.constant([full_sentence])\n                        )\n                        \n                        # Extract embeddings for each token position\n                        word_embeddings = embeddings[\"word_emb\"][0, :len(tokens), :].numpy()\n                        \n                        # Store in our embedding matrix\n                        X_embeddings[example_idx, :len(tokens), :] = word_embeddings\n                        \n                    except Exception as e:\n                        print(f\"Error getting ELMo embeddings for example {example_idx}: {e}\")\n                        # Leave as zeros for this example\n    else:\n        # Word2Vec and FastText extraction - much faster with vectorized operations\n        all_tokens = []\n        for example in dataset:\n            tokens = example['tokens'][:max_len]\n            # Pad with empty strings for consistent shape\n            padded_tokens = tokens + [''] * (max_len - len(tokens))\n            all_tokens.append(padded_tokens)\n        \n        # Convert to numpy array for easier indexing\n        all_tokens = np.array(all_tokens)\n        \n        # Process in batches for memory efficiency\n        batch_size = 1000\n        for batch_start in tqdm(range(0, len(dataset), batch_size)):\n            batch_end = min(batch_start + batch_size, len(dataset))\n            \n            for i in range(batch_start, batch_end):\n                for j, token in enumerate(all_tokens[i]):\n                    if token == '':  # Skip padding tokens\n                        continue\n                    \n                    if embedding_type == \"word2vec\":\n                        embedding = get_word2vec_embedding(token.lower())\n                    elif embedding_type == \"fasttext\":\n                        embedding = get_fasttext_embedding(token.lower())\n                    \n                    # Use zeros for missing embeddings\n                    if embedding is not None:\n                        X_embeddings[i, j] = embedding\n    \n    return X_embeddings, X_indices, y, sequence_lengths\n</pre> def get_embedding_dim(embedding_type):     \"\"\"Return the dimensionality of each embedding type\"\"\"     if embedding_type == \"word2vec\":         return 300  # Word2Vec dimension     elif embedding_type == \"fasttext\":         return 300  # FastText dimension     elif embedding_type == \"elmo\":         return 512  # ELMo dimension     else:         raise ValueError(f\"Unknown embedding type: {embedding_type}\")  def prepare_pretrained_data(dataset, embedding_type):     \"\"\"Prepare data with pre-trained embeddings - optimized version\"\"\"     X_indices = []     y = []     sequence_lengths = []          # First pass: collect tokens and prepare indices and labels     for example in dataset:         tokens = example['tokens']         seq_len = min(len(tokens), max_len)         sequence_lengths.append(seq_len)                  # Convert tokens to indices         token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in tokens]         # Pad to max_len         padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]         X_indices.append(padded_indices)                  # Process labels         labels = to_categorical([tag for tag in example['ner_tags']], num_classes=num_tags)         padded_labels = np.zeros((max_len, num_tags))         padded_labels[:min(len(labels), max_len)] = labels[:min(len(labels), max_len)]         y.append(padded_labels)          X_indices = np.array(X_indices)     y = np.array(y)          # Determine embedding dimension     embedding_dim = 300  # Default for Word2Vec and FastText     if embedding_type == \"elmo\":         embedding_dim = 512          # Create empty embedding matrix     X_embeddings = np.zeros((len(dataset), max_len, embedding_dim))          # Extract embeddings based on embedding type     print(f\"Extracting {embedding_type} embeddings...\")          if embedding_type == \"elmo\":         # Optimized batch processing for ELMo         batch_size = 32  # Process this many examples at once                  for batch_start in tqdm(range(0, len(dataset), batch_size)):             batch_end = min(batch_start + batch_size, len(dataset))                          # Process each example in the batch             for i in range(batch_end - batch_start):                 # Get actual index in the full dataset                 example_idx = batch_start + i                                  # Get current example from the dataset                 if example_idx &lt; len(dataset):                     example = dataset[example_idx]                                          # Get tokens for this example                     tokens = example['tokens'][:max_len]                                          # Create a single sentence from all tokens to get embeddings for the whole sequence at once                     full_sentence = \" \".join(tokens)                                          try:                         # Get embeddings for the entire sentence at once                         embeddings = elmo_model.signatures[\"default\"](                             tf.constant([full_sentence])                         )                                                  # Extract embeddings for each token position                         word_embeddings = embeddings[\"word_emb\"][0, :len(tokens), :].numpy()                                                  # Store in our embedding matrix                         X_embeddings[example_idx, :len(tokens), :] = word_embeddings                                              except Exception as e:                         print(f\"Error getting ELMo embeddings for example {example_idx}: {e}\")                         # Leave as zeros for this example     else:         # Word2Vec and FastText extraction - much faster with vectorized operations         all_tokens = []         for example in dataset:             tokens = example['tokens'][:max_len]             # Pad with empty strings for consistent shape             padded_tokens = tokens + [''] * (max_len - len(tokens))             all_tokens.append(padded_tokens)                  # Convert to numpy array for easier indexing         all_tokens = np.array(all_tokens)                  # Process in batches for memory efficiency         batch_size = 1000         for batch_start in tqdm(range(0, len(dataset), batch_size)):             batch_end = min(batch_start + batch_size, len(dataset))                          for i in range(batch_start, batch_end):                 for j, token in enumerate(all_tokens[i]):                     if token == '':  # Skip padding tokens                         continue                                          if embedding_type == \"word2vec\":                         embedding = get_word2vec_embedding(token.lower())                     elif embedding_type == \"fasttext\":                         embedding = get_fasttext_embedding(token.lower())                                          # Use zeros for missing embeddings                     if embedding is not None:                         X_embeddings[i, j] = embedding          return X_embeddings, X_indices, y, sequence_lengths In\u00a0[39]: Copied! <pre>from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout, Lambda\n\ndef build_baseline_model():\n    \"\"\"Build a baseline BiLSTM model with trainable embeddings\"\"\"\n    input_layer = Input(shape=(max_len,))\n    \n    # Trainable embedding layer\n    embedding_layer = Embedding(\n        input_dim=len(word_to_idx),\n        output_dim=100,\n        input_length=max_len,\n        mask_zero=True\n    )(input_layer)\n    \n    # Bidirectional LSTM\n    bilstm = Bidirectional(LSTM(units=100, return_sequences=True))(embedding_layer)\n    bilstm = Dropout(0.3)(bilstm)\n    \n    # Output layer\n    output = TimeDistributed(Dense(num_tags, activation='softmax'))(bilstm)\n    \n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy', 'recall', 'precision']\n    )\n    \n    return model\n</pre> from tensorflow.keras.models import Model from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout, Lambda  def build_baseline_model():     \"\"\"Build a baseline BiLSTM model with trainable embeddings\"\"\"     input_layer = Input(shape=(max_len,))          # Trainable embedding layer     embedding_layer = Embedding(         input_dim=len(word_to_idx),         output_dim=100,         input_length=max_len,         mask_zero=True     )(input_layer)          # Bidirectional LSTM     bilstm = Bidirectional(LSTM(units=100, return_sequences=True))(embedding_layer)     bilstm = Dropout(0.3)(bilstm)          # Output layer     output = TimeDistributed(Dense(num_tags, activation='softmax'))(bilstm)          model = Model(inputs=input_layer, outputs=output)     model.compile(         optimizer='adam',         loss='categorical_crossentropy',         metrics=['accuracy', 'recall', 'precision']     )          return model In\u00a0[40]: Copied! <pre>def build_pretrained_model(embedding_type):\n    \"\"\"Build a BiLSTM model for pre-trained embeddings\"\"\"\n    embedding_dim = 300  # Default for Word2Vec and FastText\n    if embedding_type == \"elmo\":\n        embedding_dim = 512\n    \n    # Input layers\n    embedding_input = Input(shape=(max_len, embedding_dim))\n    indices_input = Input(shape=(max_len,))\n    \n    # Create a mask from indices input using Lambda layer\n    mask_layer = Lambda(lambda x: tf.cast(tf.not_equal(x, 0), tf.bool))(indices_input)\n    \n    # BiLSTM layers - using masking layer directly\n    bilstm = Bidirectional(LSTM(units=100, return_sequences=True))(embedding_input, mask=mask_layer)\n    bilstm = Dropout(0.3)(bilstm)\n    \n    # Output layer\n    output = TimeDistributed(Dense(num_tags, activation='softmax'))(bilstm)\n    \n    model = Model(inputs=[embedding_input, indices_input], outputs=output)\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy', 'recall', 'precision']\n    )\n    \n    return model\n</pre> def build_pretrained_model(embedding_type):     \"\"\"Build a BiLSTM model for pre-trained embeddings\"\"\"     embedding_dim = 300  # Default for Word2Vec and FastText     if embedding_type == \"elmo\":         embedding_dim = 512          # Input layers     embedding_input = Input(shape=(max_len, embedding_dim))     indices_input = Input(shape=(max_len,))          # Create a mask from indices input using Lambda layer     mask_layer = Lambda(lambda x: tf.cast(tf.not_equal(x, 0), tf.bool))(indices_input)          # BiLSTM layers - using masking layer directly     bilstm = Bidirectional(LSTM(units=100, return_sequences=True))(embedding_input, mask=mask_layer)     bilstm = Dropout(0.3)(bilstm)          # Output layer     output = TimeDistributed(Dense(num_tags, activation='softmax'))(bilstm)          model = Model(inputs=[embedding_input, indices_input], outputs=output)     model.compile(         optimizer='adam',         loss='categorical_crossentropy',         metrics=['accuracy', 'recall', 'precision']     )          return model In\u00a0[41]: Copied! <pre>from sklearn.metrics import classification_report, confusion_matrix\n\ndef evaluate_model(y_true, y_pred, sequence_lengths):\n    \"\"\"Convert predictions to flat format and evaluate\"\"\"\n    # Flatten predictions and true values, respecting sequence lengths\n    y_pred_flat = []\n    y_true_flat = []\n    \n    for i in range(len(y_pred)):\n        for j in range(sequence_lengths[i]):\n            y_pred_flat.append(np.argmax(y_pred[i, j]))\n            y_true_flat.append(np.argmax(y_true[i, j]))\n    \n    # Compute metrics\n    report = classification_report(\n        y_true_flat, y_pred_flat,\n        target_names=tag_names,\n        output_dict=True\n    )\n    \n    cm = confusion_matrix(y_true_flat, y_pred_flat)\n    \n    return report, cm\n</pre> from sklearn.metrics import classification_report, confusion_matrix  def evaluate_model(y_true, y_pred, sequence_lengths):     \"\"\"Convert predictions to flat format and evaluate\"\"\"     # Flatten predictions and true values, respecting sequence lengths     y_pred_flat = []     y_true_flat = []          for i in range(len(y_pred)):         for j in range(sequence_lengths[i]):             y_pred_flat.append(np.argmax(y_pred[i, j]))             y_true_flat.append(np.argmax(y_true[i, j]))          # Compute metrics     report = classification_report(         y_true_flat, y_pred_flat,         target_names=tag_names,         output_dict=True     )          cm = confusion_matrix(y_true_flat, y_pred_flat)          return report, cm In\u00a0[42]: Copied! <pre>def show_examples(dataset, y_true, y_pred, sequence_lengths, model_name, num_examples=3):\n    \"\"\"Show correct and incorrect examples\"\"\"\n    print(f\"\\n{'='*20} {model_name.upper()} EXAMPLES {'='*20}\")\n    \n    # Find examples with errors and correct predictions\n    error_indices = []\n    correct_indices = []\n    \n    for i in range(len(y_pred)):\n        has_error = False\n        for j in range(sequence_lengths[i]):\n            if np.argmax(y_pred[i, j]) != np.argmax(y_true[i, j]):\n                has_error = True\n                break\n        \n        if has_error:\n            error_indices.append(i)\n        else:\n            correct_indices.append(i)\n    \n    # Show correct examples\n    print(f\"\\n\u2705 CORRECT EXAMPLES:\")\n    for idx in correct_indices[:num_examples]:\n        tokens = dataset[idx]['tokens'][:sequence_lengths[idx]]\n        print(\"\\nExample:\")\n        for j, token in enumerate(tokens):\n            tag = tag_names[np.argmax(y_true[idx, j])]\n            print(f\"{token} ({tag})\", end=\" \")\n        print()\n    \n    # Show incorrect examples\n    print(f\"\\n\u274c INCORRECT EXAMPLES:\")\n    for idx in error_indices[:num_examples]:\n        tokens = dataset[idx]['tokens'][:sequence_lengths[idx]]\n        print(\"\\nExample:\")\n        for j, token in enumerate(tokens):\n            true_tag = tag_names[np.argmax(y_true[idx, j])]\n            pred_tag = tag_names[np.argmax(y_pred[idx, j])]\n            \n            if true_tag != pred_tag:\n                print(f\"{token} (True: {true_tag}, Pred: {pred_tag})\", end=\" \")\n            else:\n                print(f\"{token} ({true_tag})\", end=\" \")\n        print()\n</pre> def show_examples(dataset, y_true, y_pred, sequence_lengths, model_name, num_examples=3):     \"\"\"Show correct and incorrect examples\"\"\"     print(f\"\\n{'='*20} {model_name.upper()} EXAMPLES {'='*20}\")          # Find examples with errors and correct predictions     error_indices = []     correct_indices = []          for i in range(len(y_pred)):         has_error = False         for j in range(sequence_lengths[i]):             if np.argmax(y_pred[i, j]) != np.argmax(y_true[i, j]):                 has_error = True                 break                  if has_error:             error_indices.append(i)         else:             correct_indices.append(i)          # Show correct examples     print(f\"\\n\u2705 CORRECT EXAMPLES:\")     for idx in correct_indices[:num_examples]:         tokens = dataset[idx]['tokens'][:sequence_lengths[idx]]         print(\"\\nExample:\")         for j, token in enumerate(tokens):             tag = tag_names[np.argmax(y_true[idx, j])]             print(f\"{token} ({tag})\", end=\" \")         print()          # Show incorrect examples     print(f\"\\n\u274c INCORRECT EXAMPLES:\")     for idx in error_indices[:num_examples]:         tokens = dataset[idx]['tokens'][:sequence_lengths[idx]]         print(\"\\nExample:\")         for j, token in enumerate(tokens):             true_tag = tag_names[np.argmax(y_true[idx, j])]             pred_tag = tag_names[np.argmax(y_pred[idx, j])]                          if true_tag != pred_tag:                 print(f\"{token} (True: {true_tag}, Pred: {pred_tag})\", end=\" \")             else:                 print(f\"{token} ({true_tag})\", end=\" \")         print() In\u00a0[47]: Copied! <pre>def plot_metrics(metrics_dict):\n    \"\"\"Plot metrics comparison across models, handling entities properly\"\"\"\n    models = list(metrics_dict.keys())\n    \n    # 1. First plot: Entity-type performance\n    plt.figure(figsize=(12, 6))\n    \n    # Entity types\n    entity_types = ['PER', 'ORG', 'LOC', 'MISC']\n    x = np.arange(len(entity_types))\n    width = 0.2\n    \n    # Check for entity types and calculate F1 scores\n    entity_scores = {}\n    for model in models:\n        entity_scores[model] = []\n        for entity in entity_types:\n            b_tag = f'B-{entity}'\n            i_tag = f'I-{entity}'\n            \n            # Get scores (safely with defaults)\n            b_score = metrics_dict[model].get(b_tag, {}).get('f1-score', 0)\n            i_score = metrics_dict[model].get(i_tag, {}).get('f1-score', 0)\n            \n            # Average them (or just use one if only one is available)\n            if b_tag in metrics_dict[model] and i_tag in metrics_dict[model]:\n                entity_scores[model].append((b_score + i_score) / 2)\n            elif b_tag in metrics_dict[model]:\n                entity_scores[model].append(b_score)\n            elif i_tag in metrics_dict[model]:\n                entity_scores[model].append(i_score)\n            else:\n                # No score for this entity type\n                entity_scores[model].append(0)\n    \n    # Plot entity scores\n    for i, model in enumerate(models):\n        offset = width * (i - len(models)/2 + 0.5)\n        plt.bar(x + offset, entity_scores[model], width, label=model)\n    \n    plt.xlabel('Entity Type')\n    plt.ylabel('F1 Score')\n    plt.title('Entity Recognition Performance by Type')\n    plt.xticks(x, entity_types)\n    plt.legend()\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # 2. Second plot: Overall macro metrics (precision, recall, f1)\n    plt.figure(figsize=(12, 6))\n    \n    # Metrics to plot\n    metrics_to_plot = ['precision', 'recall', 'f1-score']\n    x = np.arange(len(metrics_to_plot))\n    width = 0.2\n    \n    # Plot overall metrics\n    for i, model in enumerate(models):\n        # Get macro average values\n        values = []\n        for metric in metrics_to_plot:\n            try:\n                values.append(metrics_dict[model]['macro avg'][metric])\n            except KeyError:\n                # Handle missing metrics gracefully\n                values.append(0)\n        \n        offset = width * (i - len(models)/2 + 0.5)\n        plt.bar(x + offset, values, width, label=model)\n    \n    plt.title(\"Overall Macro-Average Performance\")\n    plt.xlabel(\"Metric\")\n    plt.ylabel(\"Score\")\n    plt.xticks(x, metrics_to_plot)\n    plt.legend()\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.ylim(0, 1.0)\n    plt.tight_layout()\n    plt.show()\n    \n    # 3. Third plot: Weighted F1 score comparison\n    plt.figure(figsize=(10, 6))\n    \n    # Extract weighted F1 scores\n    weighted_f1 = []\n    for model in models:\n        try:\n            weighted_f1.append(metrics_dict[model]['weighted avg']['f1-score'])\n        except KeyError:\n            weighted_f1.append(0)\n    \n    # Plot weighted F1\n    plt.bar(models, weighted_f1, color='green')\n    plt.title(\"Weighted F1 Score Comparison\")\n    plt.ylabel(\"Weighted F1 Score\")\n    plt.ylim(0, 1.0)\n    \n    # Add value labels on top of bars\n    for i, v in enumerate(weighted_f1):\n        plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n    \n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_metrics(metrics_dict):     \"\"\"Plot metrics comparison across models, handling entities properly\"\"\"     models = list(metrics_dict.keys())          # 1. First plot: Entity-type performance     plt.figure(figsize=(12, 6))          # Entity types     entity_types = ['PER', 'ORG', 'LOC', 'MISC']     x = np.arange(len(entity_types))     width = 0.2          # Check for entity types and calculate F1 scores     entity_scores = {}     for model in models:         entity_scores[model] = []         for entity in entity_types:             b_tag = f'B-{entity}'             i_tag = f'I-{entity}'                          # Get scores (safely with defaults)             b_score = metrics_dict[model].get(b_tag, {}).get('f1-score', 0)             i_score = metrics_dict[model].get(i_tag, {}).get('f1-score', 0)                          # Average them (or just use one if only one is available)             if b_tag in metrics_dict[model] and i_tag in metrics_dict[model]:                 entity_scores[model].append((b_score + i_score) / 2)             elif b_tag in metrics_dict[model]:                 entity_scores[model].append(b_score)             elif i_tag in metrics_dict[model]:                 entity_scores[model].append(i_score)             else:                 # No score for this entity type                 entity_scores[model].append(0)          # Plot entity scores     for i, model in enumerate(models):         offset = width * (i - len(models)/2 + 0.5)         plt.bar(x + offset, entity_scores[model], width, label=model)          plt.xlabel('Entity Type')     plt.ylabel('F1 Score')     plt.title('Entity Recognition Performance by Type')     plt.xticks(x, entity_types)     plt.legend()     plt.grid(axis='y', linestyle='--', alpha=0.7)     plt.tight_layout()     plt.show()          # 2. Second plot: Overall macro metrics (precision, recall, f1)     plt.figure(figsize=(12, 6))          # Metrics to plot     metrics_to_plot = ['precision', 'recall', 'f1-score']     x = np.arange(len(metrics_to_plot))     width = 0.2          # Plot overall metrics     for i, model in enumerate(models):         # Get macro average values         values = []         for metric in metrics_to_plot:             try:                 values.append(metrics_dict[model]['macro avg'][metric])             except KeyError:                 # Handle missing metrics gracefully                 values.append(0)                  offset = width * (i - len(models)/2 + 0.5)         plt.bar(x + offset, values, width, label=model)          plt.title(\"Overall Macro-Average Performance\")     plt.xlabel(\"Metric\")     plt.ylabel(\"Score\")     plt.xticks(x, metrics_to_plot)     plt.legend()     plt.grid(axis='y', linestyle='--', alpha=0.7)     plt.ylim(0, 1.0)     plt.tight_layout()     plt.show()          # 3. Third plot: Weighted F1 score comparison     plt.figure(figsize=(10, 6))          # Extract weighted F1 scores     weighted_f1 = []     for model in models:         try:             weighted_f1.append(metrics_dict[model]['weighted avg']['f1-score'])         except KeyError:             weighted_f1.append(0)          # Plot weighted F1     plt.bar(models, weighted_f1, color='green')     plt.title(\"Weighted F1 Score Comparison\")     plt.ylabel(\"Weighted F1 Score\")     plt.ylim(0, 1.0)          # Add value labels on top of bars     for i, v in enumerate(weighted_f1):         plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center')          plt.grid(axis='y', linestyle='--', alpha=0.7)     plt.tight_layout()     plt.show() In\u00a0[44]: Copied! <pre>from tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Masking\n\ndef run_baseline_pipeline():\n    \"\"\"Run training and evaluation for baseline model\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"BASELINE MODEL (TRAINABLE EMBEDDINGS)\")\n    print(\"=\"*80)\n    \n    # Prepare data\n    X_train, y_train, train_lengths = prepare_baseline_data(train_dataset)\n    X_val, y_val, val_lengths = prepare_baseline_data(val_dataset)\n    X_test, y_test, test_lengths = prepare_baseline_data(test_dataset)\n    \n    # Build and train model\n    model = build_baseline_model()\n    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n    \n    print(\"Training baseline model...\")\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=5,\n        batch_size=64,\n        callbacks=[early_stopping]\n    )\n    \n    # Evaluate\n    print(\"Evaluating baseline model...\")\n    y_pred = model.predict(X_test)\n    report, cm = evaluate_model(y_test, y_pred, test_lengths)\n    \n    # Display examples\n    show_examples(test_dataset, y_test, y_pred, test_lengths, \"baseline\")\n    \n    return report, history, y_pred, y_test\n\n\ndef run_pretrained_pipeline(embedding_type):\n    \"\"\"Run training and evaluation for pre-trained embedding model\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"{embedding_type.upper()} MODEL\")\n    print(\"=\"*80)\n    \n    # Prepare data\n    print(f\"Preparing {embedding_type} data...\")\n    X_train_emb, X_train_idx, y_train, train_lengths = prepare_pretrained_data(train_dataset, embedding_type)\n    X_val_emb, X_val_idx, y_val, val_lengths = prepare_pretrained_data(val_dataset, embedding_type)\n    X_test_emb, X_test_idx, y_test, test_lengths = prepare_pretrained_data(test_dataset, embedding_type)\n    \n    # Build and train model\n    model = build_pretrained_model(embedding_type)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n    \n    print(f\"Training {embedding_type} model...\")\n    history = model.fit(\n        [X_train_emb, X_train_idx], y_train,  # Pass both inputs as a list\n        validation_data=([X_val_emb, X_val_idx], y_val),\n        epochs=5,\n        batch_size=64,\n        callbacks=[early_stopping]\n    )\n    \n    # Evaluate\n    print(f\"Evaluating {embedding_type} model...\")\n    y_pred = model.predict([X_test_emb, X_test_idx])  # Pass both inputs as a list\n    report, cm = evaluate_model(y_test, y_pred, test_lengths)\n    \n    # Display examples\n    show_examples(test_dataset, y_test, y_pred, test_lengths, embedding_type)\n    \n    return report, history, y_pred, y_test\n</pre> from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.layers import Masking  def run_baseline_pipeline():     \"\"\"Run training and evaluation for baseline model\"\"\"     print(\"\\n\" + \"=\"*80)     print(\"BASELINE MODEL (TRAINABLE EMBEDDINGS)\")     print(\"=\"*80)          # Prepare data     X_train, y_train, train_lengths = prepare_baseline_data(train_dataset)     X_val, y_val, val_lengths = prepare_baseline_data(val_dataset)     X_test, y_test, test_lengths = prepare_baseline_data(test_dataset)          # Build and train model     model = build_baseline_model()     early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)          print(\"Training baseline model...\")     history = model.fit(         X_train, y_train,         validation_data=(X_val, y_val),         epochs=5,         batch_size=64,         callbacks=[early_stopping]     )          # Evaluate     print(\"Evaluating baseline model...\")     y_pred = model.predict(X_test)     report, cm = evaluate_model(y_test, y_pred, test_lengths)          # Display examples     show_examples(test_dataset, y_test, y_pred, test_lengths, \"baseline\")          return report, history, y_pred, y_test   def run_pretrained_pipeline(embedding_type):     \"\"\"Run training and evaluation for pre-trained embedding model\"\"\"     print(\"\\n\" + \"=\"*80)     print(f\"{embedding_type.upper()} MODEL\")     print(\"=\"*80)          # Prepare data     print(f\"Preparing {embedding_type} data...\")     X_train_emb, X_train_idx, y_train, train_lengths = prepare_pretrained_data(train_dataset, embedding_type)     X_val_emb, X_val_idx, y_val, val_lengths = prepare_pretrained_data(val_dataset, embedding_type)     X_test_emb, X_test_idx, y_test, test_lengths = prepare_pretrained_data(test_dataset, embedding_type)          # Build and train model     model = build_pretrained_model(embedding_type)     early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)          print(f\"Training {embedding_type} model...\")     history = model.fit(         [X_train_emb, X_train_idx], y_train,  # Pass both inputs as a list         validation_data=([X_val_emb, X_val_idx], y_val),         epochs=5,         batch_size=64,         callbacks=[early_stopping]     )          # Evaluate     print(f\"Evaluating {embedding_type} model...\")     y_pred = model.predict([X_test_emb, X_test_idx])  # Pass both inputs as a list     report, cm = evaluate_model(y_test, y_pred, test_lengths)          # Display examples     show_examples(test_dataset, y_test, y_pred, test_lengths, embedding_type)          return report, history, y_pred, y_test In\u00a0[45]: Copied! <pre>\"\"\"\nmetrics = {}\n\nprint(\"Running baseline pipeline...\")\nbaseline_report, _, _, _ = run_baseline_pipeline()\nmetrics[\"baseline\"] = baseline_report\n\nprint(\"Running Word2Vec pipeline...\")\nw2v_report, _, _, _ = run_pretrained_pipeline(\"word2vec\")\nmetrics[\"word2vec\"] = w2v_report\n\nprint(\"Running FastText pipeline...\")\nft_report, _, _, _ = run_pretrained_pipeline(\"fasttext\")\nmetrics[\"fasttext\"] = ft_report\n\"\"\"\nprint(\"Running ELMo pipeline...\")\nelmo_report, _, _, _ = run_pretrained_pipeline(\"elmo\")\nmetrics[\"elmo\"] = elmo_report\n</pre> \"\"\" metrics = {}  print(\"Running baseline pipeline...\") baseline_report, _, _, _ = run_baseline_pipeline() metrics[\"baseline\"] = baseline_report  print(\"Running Word2Vec pipeline...\") w2v_report, _, _, _ = run_pretrained_pipeline(\"word2vec\") metrics[\"word2vec\"] = w2v_report  print(\"Running FastText pipeline...\") ft_report, _, _, _ = run_pretrained_pipeline(\"fasttext\") metrics[\"fasttext\"] = ft_report \"\"\" print(\"Running ELMo pipeline...\") elmo_report, _, _, _ = run_pretrained_pipeline(\"elmo\") metrics[\"elmo\"] = elmo_report <pre>Running ELMo pipeline...\n\n================================================================================\nELMO MODEL\n================================================================================\nPreparing elmo data...\nExtracting elmo embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 157/157 [09:00&lt;00:00,  3.44s/it]\n</pre> <pre>Extracting elmo embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:46&lt;00:00,  2.89s/it]\n</pre> <pre>Extracting elmo embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [01:38&lt;00:00,  3.06s/it]\n</pre> <pre>Training elmo model...\nEpoch 1/5\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12s 114ms/step - accuracy: 0.9454 - loss: 0.2046 - precision: 0.8788 - recall: 0.5675 - val_accuracy: 0.9902 - val_loss: 0.0371 - val_precision: 0.9783 - val_recall: 0.9175\nEpoch 2/5\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 123ms/step - accuracy: 0.9893 - loss: 0.0386 - precision: 0.9785 - recall: 0.9274 - val_accuracy: 0.9944 - val_loss: 0.0215 - val_precision: 0.9824 - val_recall: 0.9570\nEpoch 3/5\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 129ms/step - accuracy: 0.9936 - loss: 0.0229 - precision: 0.9839 - recall: 0.9594 - val_accuracy: 0.9958 - val_loss: 0.0158 - val_precision: 0.9837 - val_recall: 0.9718\nEpoch 4/5\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 130ms/step - accuracy: 0.9959 - loss: 0.0152 - precision: 0.9876 - recall: 0.9747 - val_accuracy: 0.9962 - val_loss: 0.0133 - val_precision: 0.9848 - val_recall: 0.9763\nEpoch 5/5\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 10s 132ms/step - accuracy: 0.9973 - loss: 0.0108 - precision: 0.9914 - recall: 0.9839 - val_accuracy: 0.9965 - val_loss: 0.0119 - val_precision: 0.9860 - val_recall: 0.9784\nEvaluating elmo model...\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 31ms/step\n\n==================== ELMO EXAMPLES ====================\n\n\u2705 CORRECT EXAMPLES:\n\nExample:\nNadim (B-PER) Ladki (I-PER) \n\nExample:\nAL-AIN (B-LOC) , (O) United (B-LOC) Arab (I-LOC) Emirates (I-LOC) 1996-12-06 (O) \n\nExample:\nBut (O) China (B-LOC) saw (O) their (O) luck (O) desert (O) them (O) in (O) the (O) second (O) match (O) of (O) the (O) group (O) , (O) crashing (O) to (O) a (O) surprise (O) 2-0 (O) defeat (O) to (O) newcomers (O) Uzbekistan (B-LOC) . (O) \n\n\u274c INCORRECT EXAMPLES:\n\nExample:\nSOCCER (O) - (O) JAPAN (B-LOC) GET (O) LUCKY (O) WIN (O) , (O) CHINA (True: B-PER, Pred: B-LOC) IN (O) SURPRISE (O) DEFEAT (O) . (O) \n\nExample:\nJapan (B-LOC) began (O) the (O) defence (O) of (O) their (O) Asian (B-MISC) Cup (I-MISC) title (O) with (O) a (O) lucky (O) 2-1 (O) win (O) against (O) Syria (B-LOC) in (O) a (O) Group (O) C (True: O, Pred: I-MISC) championship (O) match (O) on (O) Friday (O) . (O) \n\nExample:\nRUGBY (B-ORG) UNION (I-ORG) - (O) CUTTITTA (True: B-PER, Pred: B-ORG) BACK (O) FOR (O) ITALY (B-LOC) AFTER (O) A (O) YEAR (O) . (O) \n</pre> In\u00a0[50]: Copied! <pre>print(\"\\n\" + \"=\"*80)\nprint(\"\ud83d\udcca FINAL COMPARISON\")\nprint(\"=\"*80)\n\nfor model in metrics:\n    print(f\"\\n{model.upper()}:\")\n    print(f\"F1 Score (macro): {metrics[model]['macro avg']['f1-score']:.4f}\")\n    print(f\"F1 Score (weighted): {metrics[model]['weighted avg']['f1-score']:.4f}\")\n    print(f\"Precision (macro): {metrics[model]['macro avg']['precision']:.4f}\")\n    print(f\"Recall (macro): {metrics[model]['macro avg']['recall']:.4f}\")\n\nplot_metrics(metrics)\n</pre> print(\"\\n\" + \"=\"*80) print(\"\ud83d\udcca FINAL COMPARISON\") print(\"=\"*80)  for model in metrics:     print(f\"\\n{model.upper()}:\")     print(f\"F1 Score (macro): {metrics[model]['macro avg']['f1-score']:.4f}\")     print(f\"F1 Score (weighted): {metrics[model]['weighted avg']['f1-score']:.4f}\")     print(f\"Precision (macro): {metrics[model]['macro avg']['precision']:.4f}\")     print(f\"Recall (macro): {metrics[model]['macro avg']['recall']:.4f}\")  plot_metrics(metrics) <pre>\n================================================================================\n\ud83d\udcca FINAL COMPARISON\n================================================================================\n\nBASELINE:\nF1 Score (macro): 0.5967\nF1 Score (weighted): 0.8763\nPrecision (macro): 0.6659\nRecall (macro): 0.5619\n\nWORD2VEC:\nF1 Score (macro): 0.4983\nF1 Score (weighted): 0.8835\nPrecision (macro): 0.6128\nRecall (macro): 0.4797\n\nFASTTEXT:\nF1 Score (macro): 0.6885\nF1 Score (weighted): 0.9238\nPrecision (macro): 0.8085\nRecall (macro): 0.6392\n\nELMO:\nF1 Score (macro): 0.8768\nF1 Score (weighted): 0.9713\nPrecision (macro): 0.8868\nRecall (macro): 0.8691\n</pre> In\u00a0[68]: Copied! <pre># Load the SST-2 dataset\n# Load the SST-2 dataset\nprint(\"Loading SST-2 dataset...\")\nsst2_dataset = load_dataset(\"stanfordnlp/sst2\")\n\n# Check the label distribution to understand the dataset\nprint(\"\\nExamining label distribution...\")\ntrain_labels = sst2_dataset[\"train\"][\"label\"]\nlabel_counts = {}\nfor label in train_labels:\n    if label not in label_counts:\n        label_counts[label] = 0\n    label_counts[label] += 1\n\n\nprint(f\"Label distribution in training set: {label_counts}\")\n\n# Get number of classes\nnum_classes = len(label_counts)\nprint(f\"Number of classes: {num_classes}\")\n\n# Subsample for quicker evaluation\ndataset= sst2_dataset[\"train\"].shuffle(seed=42)\ntrain_dataset = dataset.select(range(5000))\ntest_dataset = dataset.select(range(5000, 6000))\nval_dataset = dataset.select(range(6000, 6500))\n\nprint(f\"Train set: {len(train_dataset)} examples\")\nprint(f\"Validation set: {len(val_dataset)} examples\")\nprint(f\"Test set: {len(test_dataset)} examples\")\n\n# View a sample\nsample = train_dataset[0]\nprint(\"\\nSample data:\")\nprint(f\"Sentence: {sample['sentence']}\")\nprint(f\"Label: {sample['label']}\")\n</pre> # Load the SST-2 dataset # Load the SST-2 dataset print(\"Loading SST-2 dataset...\") sst2_dataset = load_dataset(\"stanfordnlp/sst2\")  # Check the label distribution to understand the dataset print(\"\\nExamining label distribution...\") train_labels = sst2_dataset[\"train\"][\"label\"] label_counts = {} for label in train_labels:     if label not in label_counts:         label_counts[label] = 0     label_counts[label] += 1   print(f\"Label distribution in training set: {label_counts}\")  # Get number of classes num_classes = len(label_counts) print(f\"Number of classes: {num_classes}\")  # Subsample for quicker evaluation dataset= sst2_dataset[\"train\"].shuffle(seed=42) train_dataset = dataset.select(range(5000)) test_dataset = dataset.select(range(5000, 6000)) val_dataset = dataset.select(range(6000, 6500))  print(f\"Train set: {len(train_dataset)} examples\") print(f\"Validation set: {len(val_dataset)} examples\") print(f\"Test set: {len(test_dataset)} examples\")  # View a sample sample = train_dataset[0] print(\"\\nSample data:\") print(f\"Sentence: {sample['sentence']}\") print(f\"Label: {sample['label']}\") <pre>Loading SST-2 dataset...\n\nExamining label distribution...\nLabel distribution in training set: {0: 29780, 1: 37569}\nNumber of classes: 2\nTrain set: 5000 examples\nValidation set: 500 examples\nTest set: 1000 examples\n\nSample data:\nSentence: klein , charming in comedies like american pie and dead-on in election , \nLabel: 1\n</pre> In\u00a0[69]: Copied! <pre># Create vocabulary from training data\nprint(\"\\nCreating vocabulary...\")\nword_to_idx = {'PAD': 0, 'UNK': 1}\nfor example in train_dataset:\n    for token in example['sentence'].split():\n        if token not in word_to_idx:\n            word_to_idx[token] = len(word_to_idx)\n\nprint(f\"Vocabulary size: {len(word_to_idx)}\")\n\n# Set maximum sequence length\nmax_len = max(len(example['sentence'].split()) for example in train_dataset)\nprint(f\"Maximum sequence length: {max_len}\")\nmax_len = min(max_len, 50)  # Cap at 50 tokens\nprint(f\"Using sequence length: {max_len}\")\n</pre> # Create vocabulary from training data print(\"\\nCreating vocabulary...\") word_to_idx = {'PAD': 0, 'UNK': 1} for example in train_dataset:     for token in example['sentence'].split():         if token not in word_to_idx:             word_to_idx[token] = len(word_to_idx)  print(f\"Vocabulary size: {len(word_to_idx)}\")  # Set maximum sequence length max_len = max(len(example['sentence'].split()) for example in train_dataset) print(f\"Maximum sequence length: {max_len}\") max_len = min(max_len, 50)  # Cap at 50 tokens print(f\"Using sequence length: {max_len}\") <pre>\nCreating vocabulary...\nVocabulary size: 7652\nMaximum sequence length: 50\nUsing sequence length: 50\n</pre> In\u00a0[71]: Copied! <pre># Function to prepare data for the baseline model\ndef prepare_baseline_data(dataset):\n    X = []\n    y = []\n    sequence_lengths = []\n    \n    for example in dataset:\n        # Tokenize the sentence\n        tokens = example['sentence'].split()\n        seq_len = min(len(tokens), max_len)\n        sequence_lengths.append(seq_len)\n        \n        # Convert tokens to indices\n        token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in tokens]\n        # Pad or truncate to max_len\n        padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]\n        X.append(padded_indices)\n        \n        # Get label\n        y.append(example['label'])\n    \n    return np.array(X), np.array(y), sequence_lengths\n\ndef prepare_pretrained_data(dataset, embedding_type):\n    \"\"\"Prepare data with pre-trained embeddings\"\"\"\n    X_indices = []\n    y = []\n    sequence_lengths = []\n    \n    # First pass: collect tokens and prepare indices and labels\n    for example in dataset:\n        # Tokenize the sentence\n        tokens = example['sentence'].split()\n        seq_len = min(len(tokens), max_len)\n        sequence_lengths.append(seq_len)\n        \n        # Convert tokens to indices\n        token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in tokens]\n        # Pad to max_len\n        padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]\n        X_indices.append(padded_indices)\n        \n        # Get label\n        y.append(example['label'])\n    \n    X_indices = np.array(X_indices)\n    y = np.array(y)\n    \n    # Determine embedding dimension\n    embedding_dim = 300  # Default for Word2Vec and FastText\n    if embedding_type == \"elmo\":\n        embedding_dim = 512\n    \n    # Create empty embedding matrix\n    X_embeddings = np.zeros((len(dataset), max_len, embedding_dim))\n    \n    # Extract embeddings based on embedding type\n    print(f\"Extracting {embedding_type} embeddings...\")\n    \n    if embedding_type == \"elmo\":\n        # Optimized batch processing for ELMo\n        batch_size = 64  # Process this many examples at once\n        \n        for batch_start in tqdm(range(0, len(dataset), batch_size)):\n            batch_end = min(batch_start + batch_size, len(dataset))\n            \n            # Process each example in the batch\n            for i in range(batch_end - batch_start):\n                # Get actual index in the full dataset\n                example_idx = batch_start + i\n                \n                # Get current example from the dataset\n                if example_idx &lt; len(dataset):\n                    example = dataset[example_idx]\n                    \n                    # Get tokens\n                    tokens = example['sentence'].split()[:max_len]\n                    \n                    # Create a sentence\n                    full_sentence = example['sentence']\n                    \n                    try:\n                        # Get embeddings for the entire sentence at once\n                        embeddings = elmo_model.signatures[\"default\"](\n                            tf.constant([full_sentence])\n                        )\n                        \n                        # Extract embeddings for each token position\n                        word_embeddings = embeddings[\"word_emb\"][0, :len(tokens), :].numpy()\n                        \n                        # Store in our embedding matrix\n                        X_embeddings[example_idx, :len(tokens), :] = word_embeddings\n                        \n                    except Exception as e:\n                        print(f\"Error getting ELMo embeddings for example {example_idx}: {e}\")\n                        # Leave as zeros for this example\n    else:\n        # Word2Vec and FastText extraction\n        for i, example in enumerate(tqdm(dataset)):\n            tokens = example['sentence'].split()[:max_len]\n            \n            for j, token in enumerate(tokens):\n                if embedding_type == \"word2vec\":\n                    embedding = get_word2vec_embedding(token.lower())\n                elif embedding_type == \"fasttext\":\n                    embedding = get_fasttext_embedding(token.lower())\n                \n                # Use zeros for missing embeddings\n                if embedding is not None:\n                    X_embeddings[i, j] = embedding\n    \n    return X_embeddings, X_indices, y, sequence_lengths\n</pre> # Function to prepare data for the baseline model def prepare_baseline_data(dataset):     X = []     y = []     sequence_lengths = []          for example in dataset:         # Tokenize the sentence         tokens = example['sentence'].split()         seq_len = min(len(tokens), max_len)         sequence_lengths.append(seq_len)                  # Convert tokens to indices         token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in tokens]         # Pad or truncate to max_len         padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]         X.append(padded_indices)                  # Get label         y.append(example['label'])          return np.array(X), np.array(y), sequence_lengths  def prepare_pretrained_data(dataset, embedding_type):     \"\"\"Prepare data with pre-trained embeddings\"\"\"     X_indices = []     y = []     sequence_lengths = []          # First pass: collect tokens and prepare indices and labels     for example in dataset:         # Tokenize the sentence         tokens = example['sentence'].split()         seq_len = min(len(tokens), max_len)         sequence_lengths.append(seq_len)                  # Convert tokens to indices         token_indices = [word_to_idx.get(token, word_to_idx['UNK']) for token in tokens]         # Pad to max_len         padded_indices = pad_sequences([token_indices], maxlen=max_len, padding='post', truncating='post')[0]         X_indices.append(padded_indices)                  # Get label         y.append(example['label'])          X_indices = np.array(X_indices)     y = np.array(y)          # Determine embedding dimension     embedding_dim = 300  # Default for Word2Vec and FastText     if embedding_type == \"elmo\":         embedding_dim = 512          # Create empty embedding matrix     X_embeddings = np.zeros((len(dataset), max_len, embedding_dim))          # Extract embeddings based on embedding type     print(f\"Extracting {embedding_type} embeddings...\")          if embedding_type == \"elmo\":         # Optimized batch processing for ELMo         batch_size = 64  # Process this many examples at once                  for batch_start in tqdm(range(0, len(dataset), batch_size)):             batch_end = min(batch_start + batch_size, len(dataset))                          # Process each example in the batch             for i in range(batch_end - batch_start):                 # Get actual index in the full dataset                 example_idx = batch_start + i                                  # Get current example from the dataset                 if example_idx &lt; len(dataset):                     example = dataset[example_idx]                                          # Get tokens                     tokens = example['sentence'].split()[:max_len]                                          # Create a sentence                     full_sentence = example['sentence']                                          try:                         # Get embeddings for the entire sentence at once                         embeddings = elmo_model.signatures[\"default\"](                             tf.constant([full_sentence])                         )                                                  # Extract embeddings for each token position                         word_embeddings = embeddings[\"word_emb\"][0, :len(tokens), :].numpy()                                                  # Store in our embedding matrix                         X_embeddings[example_idx, :len(tokens), :] = word_embeddings                                              except Exception as e:                         print(f\"Error getting ELMo embeddings for example {example_idx}: {e}\")                         # Leave as zeros for this example     else:         # Word2Vec and FastText extraction         for i, example in enumerate(tqdm(dataset)):             tokens = example['sentence'].split()[:max_len]                          for j, token in enumerate(tokens):                 if embedding_type == \"word2vec\":                     embedding = get_word2vec_embedding(token.lower())                 elif embedding_type == \"fasttext\":                     embedding = get_fasttext_embedding(token.lower())                                  # Use zeros for missing embeddings                 if embedding is not None:                     X_embeddings[i, j] = embedding          return X_embeddings, X_indices, y, sequence_lengths  In\u00a0[72]: Copied! <pre>def build_baseline_model():\n    \"\"\"Build a baseline BiLSTM model with trainable embeddings\"\"\"\n    input_layer = Input(shape=(max_len,))\n    \n    # Trainable embedding layer\n    embedding_layer = Embedding(\n        input_dim=len(word_to_idx),\n        output_dim=100,\n        input_length=max_len,\n        mask_zero=True\n    )(input_layer)\n    \n    # Bidirectional LSTM\n    bilstm = Bidirectional(LSTM(units=100))(embedding_layer)\n    bilstm = Dropout(0.3)(bilstm)\n    \n    # Output layer\n    output = Dense(1, activation='sigmoid')(bilstm)\n    \n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy', 'recall', 'precision']\n    )\n    \n    return model\n\ndef build_pretrained_model(embedding_type):\n    \"\"\"Build a model for pre-trained embeddings\"\"\"\n    embedding_dim = 300  # Default for Word2Vec and FastText\n    if embedding_type == \"elmo\":\n        embedding_dim = 512\n    \n    # Input layer\n    embedding_input = Input(shape=(max_len, embedding_dim))\n    \n    # Add masking layer\n    masked_embeddings = Masking(mask_value=0.0)(embedding_input)\n    \n    # BiLSTM layer\n    bilstm = Bidirectional(LSTM(units=100))(masked_embeddings)\n    bilstm = Dropout(0.3)(bilstm)\n    \n    # Output layer\n    output = Dense(1, activation='sigmoid')(bilstm)\n    \n    model = Model(inputs=embedding_input, outputs=output)\n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy', 'recall', 'precision']\n    )\n    \n    return model\n</pre> def build_baseline_model():     \"\"\"Build a baseline BiLSTM model with trainable embeddings\"\"\"     input_layer = Input(shape=(max_len,))          # Trainable embedding layer     embedding_layer = Embedding(         input_dim=len(word_to_idx),         output_dim=100,         input_length=max_len,         mask_zero=True     )(input_layer)          # Bidirectional LSTM     bilstm = Bidirectional(LSTM(units=100))(embedding_layer)     bilstm = Dropout(0.3)(bilstm)          # Output layer     output = Dense(1, activation='sigmoid')(bilstm)          model = Model(inputs=input_layer, outputs=output)     model.compile(         optimizer='adam',         loss='binary_crossentropy',         metrics=['accuracy', 'recall', 'precision']     )          return model  def build_pretrained_model(embedding_type):     \"\"\"Build a model for pre-trained embeddings\"\"\"     embedding_dim = 300  # Default for Word2Vec and FastText     if embedding_type == \"elmo\":         embedding_dim = 512          # Input layer     embedding_input = Input(shape=(max_len, embedding_dim))          # Add masking layer     masked_embeddings = Masking(mask_value=0.0)(embedding_input)          # BiLSTM layer     bilstm = Bidirectional(LSTM(units=100))(masked_embeddings)     bilstm = Dropout(0.3)(bilstm)          # Output layer     output = Dense(1, activation='sigmoid')(bilstm)          model = Model(inputs=embedding_input, outputs=output)     model.compile(         optimizer='adam',         loss='binary_crossentropy',         metrics=['accuracy', 'recall', 'precision']     )          return model  In\u00a0[73]: Copied! <pre>from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n\n# Evaluation helpers\ndef evaluate_model(y_true, y_pred):\n    \"\"\"Evaluate classification performance\"\"\"\n    # Convert probabilities to binary predictions\n    y_pred_binary = (y_pred &gt; 0.5).astype(int)\n    \n    # Compute metrics\n    accuracy = accuracy_score(y_true, y_pred_binary)\n    f1 = f1_score(y_true, y_pred_binary, average='macro')\n    report = classification_report(y_true, y_pred_binary, output_dict=True)\n    cm = confusion_matrix(y_true, y_pred_binary)\n    \n    return accuracy, f1, report, cm\n\ndef plot_results(metrics):\n    \"\"\"Plot comparison between models\"\"\"\n    models = list(metrics.keys())\n    \n    # Accuracy and F1 score\n    plt.figure(figsize=(10, 6))\n    \n    accuracy_scores = [metrics[model]['accuracy'] for model in models]\n    f1_scores = [metrics[model]['f1'] for model in models]\n    \n    x = np.arange(len(models))\n    width = 0.35\n    \n    plt.bar(x - width/2, accuracy_scores, width, label='Accuracy', color='skyblue')\n    plt.bar(x + width/2, f1_scores, width, label='F1 Score', color='lightgreen')\n    \n    plt.xlabel('Model')\n    plt.ylabel('Score')\n    plt.title('Model Performance Comparison')\n    plt.xticks(x, models)\n    plt.ylim(0, 1)\n    plt.legend()\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # Confusion matrices\n    plt.figure(figsize=(15, 5))\n    \n    for i, model in enumerate(models):\n        plt.subplot(1, len(models), i+1)\n        cm = metrics[model]['confusion_matrix']\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n        plt.title(f'{model.capitalize()} Confusion Matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.tight_layout()\n    \n    plt.show()\n\ndef show_examples(dataset, y_true, y_pred, model_name, num_examples=5):\n    \"\"\"Show correct and incorrect examples\"\"\"\n    print(f\"\\n{'='*20} {model_name.upper()} EXAMPLES {'='*20}\")\n    \n    # Convert probabilities to binary predictions\n    y_pred_binary = (y_pred &gt; 0.5).astype(int)\n    \n    # Find examples with errors and correct predictions\n    correct_indices = []\n    error_indices = []\n    \n    for i in range(len(y_pred)):\n        if y_pred_binary[i] == y_true[i]:\n            correct_indices.append(i)\n        else:\n            error_indices.append(i)\n    \n    # Show correct examples\n    print(f\"\\n\u2705 CORRECT EXAMPLES:\")\n    for idx in correct_indices[:num_examples]:\n        text = dataset[idx]['sentence']\n        true_label = \"Positive\" if y_true[idx] == 1 else \"Negative\"\n        confidence = max(y_pred[idx], 1 - y_pred[idx])\n        print(f\"Text: {text}\")\n        print(f\"True label: {true_label}, Model confidence: {confidence:.4f}\")\n        print()\n    \n    # Show incorrect examples\n    print(f\"\\n\u274c INCORRECT EXAMPLES:\")\n    for idx in error_indices[:num_examples]:\n        text = dataset[idx]['sentence']\n        true_label = \"Positive\" if y_true[idx] == 1 else \"Negative\"\n        pred_label = \"Positive\" if y_pred_binary[idx] == 1 else \"Negative\"\n        confidence = max(y_pred[idx], 1 - y_pred[idx])\n        print(f\"Text: {text}\")\n        print(f\"True label: {true_label}, Predicted: {pred_label}, Confidence: {confidence:.4f}\")\n        print()\n</pre> from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix  # Evaluation helpers def evaluate_model(y_true, y_pred):     \"\"\"Evaluate classification performance\"\"\"     # Convert probabilities to binary predictions     y_pred_binary = (y_pred &gt; 0.5).astype(int)          # Compute metrics     accuracy = accuracy_score(y_true, y_pred_binary)     f1 = f1_score(y_true, y_pred_binary, average='macro')     report = classification_report(y_true, y_pred_binary, output_dict=True)     cm = confusion_matrix(y_true, y_pred_binary)          return accuracy, f1, report, cm  def plot_results(metrics):     \"\"\"Plot comparison between models\"\"\"     models = list(metrics.keys())          # Accuracy and F1 score     plt.figure(figsize=(10, 6))          accuracy_scores = [metrics[model]['accuracy'] for model in models]     f1_scores = [metrics[model]['f1'] for model in models]          x = np.arange(len(models))     width = 0.35          plt.bar(x - width/2, accuracy_scores, width, label='Accuracy', color='skyblue')     plt.bar(x + width/2, f1_scores, width, label='F1 Score', color='lightgreen')          plt.xlabel('Model')     plt.ylabel('Score')     plt.title('Model Performance Comparison')     plt.xticks(x, models)     plt.ylim(0, 1)     plt.legend()     plt.grid(axis='y', linestyle='--', alpha=0.7)     plt.tight_layout()     plt.show()          # Confusion matrices     plt.figure(figsize=(15, 5))          for i, model in enumerate(models):         plt.subplot(1, len(models), i+1)         cm = metrics[model]['confusion_matrix']         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)         plt.title(f'{model.capitalize()} Confusion Matrix')         plt.xlabel('Predicted')         plt.ylabel('True')         plt.tight_layout()          plt.show()  def show_examples(dataset, y_true, y_pred, model_name, num_examples=5):     \"\"\"Show correct and incorrect examples\"\"\"     print(f\"\\n{'='*20} {model_name.upper()} EXAMPLES {'='*20}\")          # Convert probabilities to binary predictions     y_pred_binary = (y_pred &gt; 0.5).astype(int)          # Find examples with errors and correct predictions     correct_indices = []     error_indices = []          for i in range(len(y_pred)):         if y_pred_binary[i] == y_true[i]:             correct_indices.append(i)         else:             error_indices.append(i)          # Show correct examples     print(f\"\\n\u2705 CORRECT EXAMPLES:\")     for idx in correct_indices[:num_examples]:         text = dataset[idx]['sentence']         true_label = \"Positive\" if y_true[idx] == 1 else \"Negative\"         confidence = max(y_pred[idx], 1 - y_pred[idx])         print(f\"Text: {text}\")         print(f\"True label: {true_label}, Model confidence: {confidence:.4f}\")         print()          # Show incorrect examples     print(f\"\\n\u274c INCORRECT EXAMPLES:\")     for idx in error_indices[:num_examples]:         text = dataset[idx]['sentence']         true_label = \"Positive\" if y_true[idx] == 1 else \"Negative\"         pred_label = \"Positive\" if y_pred_binary[idx] == 1 else \"Negative\"         confidence = max(y_pred[idx], 1 - y_pred[idx])         print(f\"Text: {text}\")         print(f\"True label: {true_label}, Predicted: {pred_label}, Confidence: {confidence:.4f}\")         print() In\u00a0[74]: Copied! <pre>def run_baseline_pipeline():\n    \"\"\"Run training and evaluation for baseline model\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"BASELINE MODEL (TRAINABLE EMBEDDINGS)\")\n    print(\"=\"*80)\n    \n    # Prepare data\n    X_train, y_train, train_lengths = prepare_baseline_data(train_dataset)\n    X_val, y_val, val_lengths = prepare_baseline_data(val_dataset)\n    X_test, y_test, test_lengths = prepare_baseline_data(test_dataset)\n    \n    # Build and train model\n    model = build_baseline_model()\n    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n    \n    print(\"Training baseline model...\")\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=10,\n        batch_size=64,\n        callbacks=[early_stopping]\n    )\n    \n    # Evaluate\n    print(\"Evaluating baseline model...\")\n    y_pred = model.predict(X_test).flatten()\n    accuracy, f1, report, cm = evaluate_model(y_test, y_pred)\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Display examples\n    show_examples(test_dataset, y_test, y_pred, \"baseline\")\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'report': report,\n        'confusion_matrix': cm,\n        'predictions': y_pred,\n        'true_labels': y_test\n    }\n\ndef run_pretrained_pipeline(embedding_type):\n    \"\"\"Run training and evaluation for pre-trained embedding model\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"{embedding_type.upper()} MODEL\")\n    print(\"=\"*80)\n    \n    # Prepare data\n    print(f\"Preparing {embedding_type} data...\")\n    X_train_emb, X_train_idx, y_train, train_lengths = prepare_pretrained_data(train_dataset, embedding_type)\n    X_val_emb, X_val_idx, y_val, val_lengths = prepare_pretrained_data(val_dataset, embedding_type)\n    X_test_emb, X_test_idx, y_test, test_lengths = prepare_pretrained_data(test_dataset, embedding_type)\n    \n    # Build and train model\n    model = build_pretrained_model(embedding_type)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n    \n    print(f\"Training {embedding_type} model...\")\n    history = model.fit(\n        X_train_emb, y_train,\n        validation_data=(X_val_emb, y_val),\n        epochs=10,\n        batch_size=64,\n        callbacks=[early_stopping]\n    )\n    \n    # Evaluate\n    print(f\"Evaluating {embedding_type} model...\")\n    y_pred = model.predict(X_test_emb).flatten()\n    accuracy, f1, report, cm = evaluate_model(y_test, y_pred)\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Display examples\n    show_examples(test_dataset, y_test, y_pred, embedding_type)\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'report': report,\n        'confusion_matrix': cm,\n        'predictions': y_pred,\n        'true_labels': y_test\n    }\n</pre> def run_baseline_pipeline():     \"\"\"Run training and evaluation for baseline model\"\"\"     print(\"\\n\" + \"=\"*80)     print(\"BASELINE MODEL (TRAINABLE EMBEDDINGS)\")     print(\"=\"*80)          # Prepare data     X_train, y_train, train_lengths = prepare_baseline_data(train_dataset)     X_val, y_val, val_lengths = prepare_baseline_data(val_dataset)     X_test, y_test, test_lengths = prepare_baseline_data(test_dataset)          # Build and train model     model = build_baseline_model()     early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)          print(\"Training baseline model...\")     history = model.fit(         X_train, y_train,         validation_data=(X_val, y_val),         epochs=10,         batch_size=64,         callbacks=[early_stopping]     )          # Evaluate     print(\"Evaluating baseline model...\")     y_pred = model.predict(X_test).flatten()     accuracy, f1, report, cm = evaluate_model(y_test, y_pred)          print(f\"Accuracy: {accuracy:.4f}\")     print(f\"F1 Score: {f1:.4f}\")          # Display examples     show_examples(test_dataset, y_test, y_pred, \"baseline\")          return {         'accuracy': accuracy,         'f1': f1,         'report': report,         'confusion_matrix': cm,         'predictions': y_pred,         'true_labels': y_test     }  def run_pretrained_pipeline(embedding_type):     \"\"\"Run training and evaluation for pre-trained embedding model\"\"\"     print(\"\\n\" + \"=\"*80)     print(f\"{embedding_type.upper()} MODEL\")     print(\"=\"*80)          # Prepare data     print(f\"Preparing {embedding_type} data...\")     X_train_emb, X_train_idx, y_train, train_lengths = prepare_pretrained_data(train_dataset, embedding_type)     X_val_emb, X_val_idx, y_val, val_lengths = prepare_pretrained_data(val_dataset, embedding_type)     X_test_emb, X_test_idx, y_test, test_lengths = prepare_pretrained_data(test_dataset, embedding_type)          # Build and train model     model = build_pretrained_model(embedding_type)     early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)          print(f\"Training {embedding_type} model...\")     history = model.fit(         X_train_emb, y_train,         validation_data=(X_val_emb, y_val),         epochs=10,         batch_size=64,         callbacks=[early_stopping]     )          # Evaluate     print(f\"Evaluating {embedding_type} model...\")     y_pred = model.predict(X_test_emb).flatten()     accuracy, f1, report, cm = evaluate_model(y_test, y_pred)          print(f\"Accuracy: {accuracy:.4f}\")     print(f\"F1 Score: {f1:.4f}\")          # Display examples     show_examples(test_dataset, y_test, y_pred, embedding_type)          return {         'accuracy': accuracy,         'f1': f1,         'report': report,         'confusion_matrix': cm,         'predictions': y_pred,         'true_labels': y_test     } In\u00a0[75]: Copied! <pre># Run all pipelines\nmetrics = {}\n\nprint(\"Running baseline pipeline...\")\nbaseline_results = run_baseline_pipeline()\nmetrics[\"baseline\"] = baseline_results\n\nprint(\"Running Word2Vec pipeline...\")\nw2v_results = run_pretrained_pipeline(\"word2vec\")\nmetrics[\"word2vec\"] = w2v_results\n\nprint(\"Running FastText pipeline...\")\nft_results = run_pretrained_pipeline(\"fasttext\")\nmetrics[\"fasttext\"] = ft_results\n\nprint(\"Running ELMo pipeline...\")\nelmo_results = run_pretrained_pipeline(\"elmo\")\nmetrics[\"elmo\"] = elmo_results\n</pre> # Run all pipelines metrics = {}  print(\"Running baseline pipeline...\") baseline_results = run_baseline_pipeline() metrics[\"baseline\"] = baseline_results  print(\"Running Word2Vec pipeline...\") w2v_results = run_pretrained_pipeline(\"word2vec\") metrics[\"word2vec\"] = w2v_results  print(\"Running FastText pipeline...\") ft_results = run_pretrained_pipeline(\"fasttext\") metrics[\"fasttext\"] = ft_results  print(\"Running ELMo pipeline...\") elmo_results = run_pretrained_pipeline(\"elmo\") metrics[\"elmo\"] = elmo_results <pre>Running baseline pipeline...\n\n================================================================================\nBASELINE MODEL (TRAINABLE EMBEDDINGS)\n================================================================================\nTraining baseline model...\nEpoch 1/10\n</pre> <pre>/Users/agomberto/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n</pre> <pre>79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 43ms/step - accuracy: 0.5420 - loss: 0.6800 - precision: 0.5437 - recall: 0.7726 - val_accuracy: 0.7220 - val_loss: 0.5864 - val_precision: 0.7305 - val_recall: 0.7658\nEpoch 2/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 81ms/step - accuracy: 0.8438 - loss: 0.4340 - precision: 0.8409 - recall: 0.8732 - val_accuracy: 0.7640 - val_loss: 0.5127 - val_precision: 0.7745 - val_recall: 0.7918\nEpoch 3/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 85ms/step - accuracy: 0.9352 - loss: 0.2012 - precision: 0.9431 - recall: 0.9353 - val_accuracy: 0.7880 - val_loss: 0.5525 - val_precision: 0.8099 - val_recall: 0.7918\nEpoch 4/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 93ms/step - accuracy: 0.9691 - loss: 0.1035 - precision: 0.9716 - recall: 0.9703 - val_accuracy: 0.7700 - val_loss: 0.7560 - val_precision: 0.7984 - val_recall: 0.7658\nEvaluating baseline model...\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 30ms/step\nAccuracy: 0.8070\nF1 Score: 0.8036\n\n==================== BASELINE EXAMPLES ====================\n\n\u2705 CORRECT EXAMPLES:\nText: retelling a historically significant , and personal , episode \nTrue label: Positive, Model confidence: 0.9959\n\nText: by turns fanciful , grisly and engagingly quixotic . \nTrue label: Positive, Model confidence: 0.6074\n\nText: the greatest date movies in years \nTrue label: Positive, Model confidence: 0.9636\n\nText: a sluggish pace and lack of genuine narrative \nTrue label: Negative, Model confidence: 0.9441\n\nText: you go into the theater expecting a scary , action-packed chiller \nTrue label: Positive, Model confidence: 0.9537\n\n\n\u274c INCORRECT EXAMPLES:\nText: putrid writing , direction and timing with a smile that says , ` if i stay positive , maybe i can channel one of my greatest pictures , drunken master . \nTrue label: Negative, Predicted: Positive, Confidence: 0.9523\n\nText: there is something that is so meditative and lyrical about babak payami 's boldly quirky iranian drama secret ballot ... a charming and evoking little ditty that manages to show the gentle and humane side of middle eastern world politics \nTrue label: Positive, Predicted: Negative, Confidence: 0.8747\n\nText: to find a place among the studio 's animated classics \nTrue label: Positive, Predicted: Negative, Confidence: 0.6753\n\nText: uptight , middle class bores like antonia \nTrue label: Negative, Predicted: Positive, Confidence: 0.7888\n\nText: an adult who 's apparently been forced by his kids to watch too many barney videos \nTrue label: Positive, Predicted: Negative, Confidence: 0.9987\n\nRunning Word2Vec pipeline...\n\n================================================================================\nWORD2VEC MODEL\n================================================================================\nPreparing word2vec data...\nExtracting word2vec embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:02&lt;00:00, 1739.93it/s]\n</pre> <pre>Extracting word2vec embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:00&lt;00:00, 3375.89it/s]\n</pre> <pre>Extracting word2vec embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 4091.53it/s]\n</pre> <pre>Training word2vec model...\nEpoch 1/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 75ms/step - accuracy: 0.6466 - loss: 0.6438 - precision: 0.6498 - recall: 0.7087 - val_accuracy: 0.8000 - val_loss: 0.4955 - val_precision: 0.8789 - val_recall: 0.7286\nEpoch 2/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 71ms/step - accuracy: 0.8039 - loss: 0.4868 - precision: 0.8343 - recall: 0.7909 - val_accuracy: 0.8180 - val_loss: 0.4304 - val_precision: 0.8803 - val_recall: 0.7658\nEpoch 3/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 88ms/step - accuracy: 0.8289 - loss: 0.4255 - precision: 0.8472 - recall: 0.8305 - val_accuracy: 0.8440 - val_loss: 0.3665 - val_precision: 0.8898 - val_recall: 0.8104\nEpoch 4/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 83ms/step - accuracy: 0.8459 - loss: 0.3828 - precision: 0.8630 - recall: 0.8465 - val_accuracy: 0.8440 - val_loss: 0.3591 - val_precision: 0.8898 - val_recall: 0.8104\nEpoch 5/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 80ms/step - accuracy: 0.8497 - loss: 0.3578 - precision: 0.8645 - recall: 0.8528 - val_accuracy: 0.8360 - val_loss: 0.3912 - val_precision: 0.8945 - val_recall: 0.7881\nEpoch 6/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 79ms/step - accuracy: 0.8563 - loss: 0.3447 - precision: 0.8820 - recall: 0.8446 - val_accuracy: 0.8540 - val_loss: 0.3388 - val_precision: 0.8952 - val_recall: 0.8253\nEpoch 7/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 72ms/step - accuracy: 0.8743 - loss: 0.3128 - precision: 0.8926 - recall: 0.8700 - val_accuracy: 0.8500 - val_loss: 0.3515 - val_precision: 0.9076 - val_recall: 0.8030\nEpoch 8/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 71ms/step - accuracy: 0.8829 - loss: 0.2919 - precision: 0.9017 - recall: 0.8770 - val_accuracy: 0.8200 - val_loss: 0.4402 - val_precision: 0.9050 - val_recall: 0.7435\nEvaluating word2vec model...\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 19ms/step\nAccuracy: 0.8380\nF1 Score: 0.8367\n\n==================== WORD2VEC EXAMPLES ====================\n\n\u2705 CORRECT EXAMPLES:\nText: retelling a historically significant , and personal , episode \nTrue label: Positive, Model confidence: 0.9800\n\nText: by turns fanciful , grisly and engagingly quixotic . \nTrue label: Positive, Model confidence: 0.9729\n\nText: the greatest date movies in years \nTrue label: Positive, Model confidence: 0.9632\n\nText: a sluggish pace and lack of genuine narrative \nTrue label: Negative, Model confidence: 0.9876\n\nText: imaginatively mixed \nTrue label: Positive, Model confidence: 0.9177\n\n\n\u274c INCORRECT EXAMPLES:\nText: you go into the theater expecting a scary , action-packed chiller \nTrue label: Positive, Predicted: Negative, Confidence: 0.8745\n\nText: an adult who 's apparently been forced by his kids to watch too many barney videos \nTrue label: Positive, Predicted: Negative, Confidence: 0.9902\n\nText: a so-called ` comedy ' and not laugh \nTrue label: Negative, Predicted: Positive, Confidence: 0.5616\n\nText: sinister happy ending \nTrue label: Positive, Predicted: Negative, Confidence: 0.5407\n\nText: , swimming gets the details right , from its promenade of barely clad bodies in myrtle beach , s.c. , to the adrenaline jolt of a sudden lunch rush at the diner . \nTrue label: Positive, Predicted: Negative, Confidence: 0.7784\n\nRunning FastText pipeline...\n\n================================================================================\nFASTTEXT MODEL\n================================================================================\nPreparing fasttext data...\nExtracting fasttext embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:01&lt;00:00, 2959.40it/s]\n</pre> <pre>Extracting fasttext embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:00&lt;00:00, 5373.69it/s]\n</pre> <pre>Extracting fasttext embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 5233.77it/s]\n</pre> <pre>Training fasttext model...\nEpoch 1/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 50ms/step - accuracy: 0.6283 - loss: 0.6571 - precision: 0.6269 - recall: 0.7540 - val_accuracy: 0.7940 - val_loss: 0.5059 - val_precision: 0.7730 - val_recall: 0.8736\nEpoch 2/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 64ms/step - accuracy: 0.7938 - loss: 0.5205 - precision: 0.7955 - recall: 0.8280 - val_accuracy: 0.8160 - val_loss: 0.4841 - val_precision: 0.8865 - val_recall: 0.7546\nEpoch 3/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 67ms/step - accuracy: 0.8106 - loss: 0.4586 - precision: 0.8252 - recall: 0.8207 - val_accuracy: 0.8300 - val_loss: 0.4142 - val_precision: 0.8898 - val_recall: 0.7807\nEpoch 4/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 75ms/step - accuracy: 0.8348 - loss: 0.4101 - precision: 0.8505 - recall: 0.8391 - val_accuracy: 0.8220 - val_loss: 0.4095 - val_precision: 0.8879 - val_recall: 0.7658\nEpoch 5/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5s 67ms/step - accuracy: 0.8383 - loss: 0.3821 - precision: 0.8579 - recall: 0.8370 - val_accuracy: 0.8320 - val_loss: 0.4208 - val_precision: 0.8970 - val_recall: 0.7770\nEpoch 6/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 80ms/step - accuracy: 0.8530 - loss: 0.3656 - precision: 0.8706 - recall: 0.8521 - val_accuracy: 0.8360 - val_loss: 0.4371 - val_precision: 0.9083 - val_recall: 0.7732\nEvaluating fasttext model...\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 26ms/step\nAccuracy: 0.8220\nF1 Score: 0.8214\n\n==================== FASTTEXT EXAMPLES ====================\n\n\u2705 CORRECT EXAMPLES:\nText: retelling a historically significant , and personal , episode \nTrue label: Positive, Model confidence: 0.9251\n\nText: by turns fanciful , grisly and engagingly quixotic . \nTrue label: Positive, Model confidence: 0.8855\n\nText: the greatest date movies in years \nTrue label: Positive, Model confidence: 0.8628\n\nText: a sluggish pace and lack of genuine narrative \nTrue label: Negative, Model confidence: 0.9833\n\nText: imaginatively mixed \nTrue label: Positive, Model confidence: 0.7151\n\n\n\u274c INCORRECT EXAMPLES:\nText: you go into the theater expecting a scary , action-packed chiller \nTrue label: Positive, Predicted: Negative, Confidence: 0.5326\n\nText: 's easy to like \nTrue label: Positive, Predicted: Negative, Confidence: 0.5219\n\nText: an adult who 's apparently been forced by his kids to watch too many barney videos \nTrue label: Positive, Predicted: Negative, Confidence: 0.9921\n\nText: , swimming gets the details right , from its promenade of barely clad bodies in myrtle beach , s.c. , to the adrenaline jolt of a sudden lunch rush at the diner . \nTrue label: Positive, Predicted: Negative, Confidence: 0.8425\n\nText: attributable to a movie like this \nTrue label: Positive, Predicted: Negative, Confidence: 0.7568\n\nRunning ELMo pipeline...\n\n================================================================================\nELMO MODEL\n================================================================================\nPreparing elmo data...\nExtracting elmo embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79/79 [07:14&lt;00:00,  5.50s/it]\n</pre> <pre>Extracting elmo embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:46&lt;00:00,  5.78s/it]\n</pre> <pre>Extracting elmo embeddings...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [01:28&lt;00:00,  5.52s/it]\n</pre> <pre>Training elmo model...\nEpoch 1/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 79ms/step - accuracy: 0.7104 - loss: 0.5580 - precision: 0.7176 - recall: 0.7464 - val_accuracy: 0.8500 - val_loss: 0.3529 - val_precision: 0.8975 - val_recall: 0.8141\nEpoch 2/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 98ms/step - accuracy: 0.8682 - loss: 0.3109 - precision: 0.8925 - recall: 0.8569 - val_accuracy: 0.8660 - val_loss: 0.3209 - val_precision: 0.9040 - val_recall: 0.8401\nEpoch 3/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 90ms/step - accuracy: 0.9243 - loss: 0.2148 - precision: 0.9382 - recall: 0.9192 - val_accuracy: 0.8560 - val_loss: 0.3389 - val_precision: 0.8924 - val_recall: 0.8327\nEpoch 4/10\n79/79 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 114ms/step - accuracy: 0.9537 - loss: 0.1401 - precision: 0.9662 - recall: 0.9468 - val_accuracy: 0.8560 - val_loss: 0.3925 - val_precision: 0.8893 - val_recall: 0.8364\nEvaluating elmo model...\n32/32 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 27ms/step\nAccuracy: 0.8480\nF1 Score: 0.8467\n\n==================== ELMO EXAMPLES ====================\n\n\u2705 CORRECT EXAMPLES:\nText: retelling a historically significant , and personal , episode \nTrue label: Positive, Model confidence: 0.9807\n\nText: by turns fanciful , grisly and engagingly quixotic . \nTrue label: Positive, Model confidence: 0.9468\n\nText: the greatest date movies in years \nTrue label: Positive, Model confidence: 0.9395\n\nText: a sluggish pace and lack of genuine narrative \nTrue label: Negative, Model confidence: 0.9918\n\nText: you go into the theater expecting a scary , action-packed chiller \nTrue label: Positive, Model confidence: 0.9047\n\n\n\u274c INCORRECT EXAMPLES:\nText: putrid writing , direction and timing with a smile that says , ` if i stay positive , maybe i can channel one of my greatest pictures , drunken master . \nTrue label: Negative, Predicted: Positive, Confidence: 0.9003\n\nText: if the title is a jeopardy question , then the answer might be `` how does steven seagal come across these days ? '' \nTrue label: Negative, Predicted: Positive, Confidence: 0.5212\n\nText: there is something that is so meditative and lyrical about babak payami 's boldly quirky iranian drama secret ballot ... a charming and evoking little ditty that manages to show the gentle and humane side of middle eastern world politics \nTrue label: Positive, Predicted: Negative, Confidence: 0.7730\n\nText: an adult who 's apparently been forced by his kids to watch too many barney videos \nTrue label: Positive, Predicted: Negative, Confidence: 0.9953\n\nText: even accepting this in the right frame of mind can only provide it with so much leniency . \nTrue label: Negative, Predicted: Positive, Confidence: 0.5123\n\n</pre> In\u00a0[76]: Copied! <pre># Compare all models\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL COMPARISON\")\nprint(\"=\"*80)\n\nfor model in metrics:\n    print(f\"\\n{model.upper()}:\")\n    print(f\"Accuracy: {metrics[model]['accuracy']:.4f}\")\n    print(f\"F1 Score: {metrics[model]['f1']:.4f}\")\n\n# Plot comparison\nplot_results(metrics)\n</pre> # Compare all models print(\"\\n\" + \"=\"*80) print(\"FINAL COMPARISON\") print(\"=\"*80)  for model in metrics:     print(f\"\\n{model.upper()}:\")     print(f\"Accuracy: {metrics[model]['accuracy']:.4f}\")     print(f\"F1 Score: {metrics[model]['f1']:.4f}\")  # Plot comparison plot_results(metrics)  <pre>\n================================================================================\nFINAL COMPARISON\n================================================================================\n\nBASELINE:\nAccuracy: 0.8070\nF1 Score: 0.8036\n\nWORD2VEC:\nAccuracy: 0.8380\nF1 Score: 0.8367\n\nFASTTEXT:\nAccuracy: 0.8220\nF1 Score: 0.8214\n\nELMO:\nAccuracy: 0.8480\nF1 Score: 0.8467\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#embedding-evaluation-intrinsic-and-extrinsic","title":"\ud83d\udcda Embedding Evaluation: Intrinsic and Extrinsic\u00b6","text":"<p>In this notebook, we explore how different word embedding models capture meaning \u2014 both through direct inspection (intrinsic evaluation) and by testing their performance in downstream tasks (extrinsic evaluation).</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#goal","title":"\ud83e\udded Goal\u00b6","text":"<p>We'll compare Word2Vec, FastText, and ELMo embeddings on how well they represent word semantics, structure, and context \u2014 especially for polysemous words (words with multiple meanings, like <code>\"bank\"</code> or <code>\"light\"</code>).</p> <p>By the end, you\u2019ll understand:</p> <ul> <li>Which models best capture word similarity, relationships, and contextual meaning</li> <li>How embeddings differ when used in real-world NLP tasks</li> <li>Why newer contextual models like ELMo offer major advantages for dynamic, context-aware representations</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#table-of-contents","title":"\ud83d\udccc Table of Contents\u00b6","text":"<ol> <li>Introduction to Embedding Models<ul> <li>Static vs Contextual Embeddings</li> </ul> </li> <li>Loading and Preprocessing Embeddings<ul> <li>Word2Vec</li> <li>FastText</li> <li>ELMo</li> </ul> </li> <li>Intrinsic Evaluation<ul> <li>Word Similarity</li> <li>Word Analogies</li> <li>Semantic Clustering</li> <li>Dimensionality Reduction (PCA / t-SNE)</li> </ul> </li> <li>Extrinsic Evaluation<ul> <li>Named Entity Recognition</li> <li>Text classification</li> </ul> </li> <li>Conclusion<ul> <li>Summary and model comparison</li> </ul> </li> </ol>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#highlight-the-elmo-cone-of-context","title":"\ud83d\udd0d Highlight: The ELMo Cone of Context\u00b6","text":"<p>Toward the end of this notebook, we\u2019ll visualize the same word in different sentences using ELMo embeddings \u2014 revealing how its position in embedding space shifts depending on its usage.</p> <p>Think of this as a \"cone of meaning\", where the same word \u2014 like <code>\"pitch\"</code> or <code>\"cell\"</code> \u2014 points in different semantic directions depending on context.</p> <p>This helps illustrate why contextual embeddings matter, and what they offer that static models like Word2Vec or FastText cannot.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#1-introduction-to-embedding-models","title":"1. \ud83e\udde0 Introduction to Embedding Models\u00b6","text":"<p>In Natural Language Processing, word embeddings are dense vector representations of words that aim to capture their semantic meaning.</p> <p>There are two main types of word embeddings:</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#static-embeddings","title":"\ud83d\udd37 Static Embeddings\u00b6","text":"<p>These models assign a single vector per word, regardless of context.</p> <ul> <li>Word2Vec: Predicts context words (Skip-gram) or the center word (CBOW) based on co-occurrence.</li> <li>FastText: Extension of Word2Vec that incorporates subword information, improving out-of-vocabulary handling and morphological understanding.</li> </ul> <p>\u2757Limitation: They cannot disambiguate polysemous words (e.g., \"bank\" in <code>river bank</code> vs <code>investment bank</code>).</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#contextual-embeddings","title":"\ud83c\udf00 Contextual Embeddings\u00b6","text":"<p>These models generate different vectors for the same word depending on its sentence context.</p> <ul> <li>ELMo (Embeddings from Language Models): Uses deep bidirectional LSTMs to compute contextual word representations.</li> <li>Trained on entire sentences \u2014 a word\u2019s embedding changes depending on surrounding words.</li> </ul> <p>\u2705 Contextual embeddings like ELMo are powerful for disambiguating word meaning dynamically.</p> <p>In the next section, we\u2019ll load and prepare pre-trained embeddings for Word2Vec, FastText, and ELMo to evaluate how each performs across different tasks.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#2-loading-and-preprocessing-embeddings","title":"2. \ud83d\udce6 Loading and Preprocessing Embeddings\u00b6","text":"<p>We will now load pre-trained embeddings from three different sources:</p> <ul> <li>\u2705 Word2Vec (via Hugging Face): Static embeddings trained on Google News<ul> <li>Model: <code>NeuML/word2vec</code></li> </ul> </li> <li>\u2705 FastText (via Hugging Face): Subword-aware static embeddings<ul> <li>Model: <code>facebook/fasttext-en-vectors</code></li> </ul> </li> <li>\u2705 ELMo (via Flair): Contextual embeddings that dynamically adapt to sentence context<ul> <li>Setup: See Flair ELMo Docs</li> </ul> </li> </ul> <p>It will take a bit of time to load the models, so go take a break and grab a cup of tea. And be sure to have at least 15Gb of SSD space on your machine or remote instance.</p> <p>We'll define utilities to:</p> <ul> <li>Access embeddings for any word</li> <li>Handle out-of-vocabulary words (especially important for Word2Vec)</li> <li>Extract contextual embeddings from full sentences using ELMo</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#21-install-required-libraries","title":"2.1 Install Required Libraries\u00b6","text":""},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#22-define-the-utility-functions","title":"2.2 Define the utility functions\u00b6","text":"<p>Let's define the utility functions to get the embeddings for the three models. We use a dummy tokenizer for the Elmo model.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#23-quick-test-embedding-lookup-example","title":"2.3 Quick Test: Embedding Lookup Example\u00b6","text":"<p>Before diving into evaluation, let\u2019s test our setup by querying the embeddings for the word \"bank\" using all three models:</p> <ul> <li>Word2Vec and FastText should return a static vector regardless of context.</li> <li>ELMo should return different vectors depending on the sentence (e.g., <code>\"river bank\"</code> vs <code>\"investment bank\"</code>).</li> </ul> <p>We\u2019ll also check:</p> <ul> <li>Embedding shapes (should be 300 for Word2Vec and FastText, 1024 for ELMo)</li> <li>Cosine similarity between ELMo vectors in different contexts</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#sanity-check-embedding-consistency-across-contexts","title":"\ud83e\uddea Sanity Check: Embedding Consistency Across Contexts\u00b6","text":"<p>We tested how the word \"point\" is represented across models and contexts.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#results-summary","title":"\ud83d\udcca Results Summary:\u00b6","text":"Model Embedding Type Dimensionality Context Sensitivity Word2Vec Static 300 \u274c Same vector always FastText Static + subwords 300 \u274c Context-agnostic ELMo Contextual 512 \u2705 Should vary with context"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#actual-output","title":"Actual Output:\u00b6","text":"<ul> <li>\u2705 Word2Vec and FastText returned expected 300d vectors.</li> <li>\u26a0\ufe0f ELMo returned 512d vectors but both were nearly identical, with cosine similarity = 1.0000.</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#why-didnt-elmo-differentiate-the-contexts","title":"\ud83e\udd14 Why Didn\u2019t ELMo Differentiate the Contexts?\u00b6","text":"<p>This can happen due to:</p> <ul> <li>Tokenization mismatch: Our tokenizer may not segment <code>\"point\"</code> correctly if it's repeated or tokenized inconsistently.</li> <li>Low-sensitivity contexts: If both contexts use <code>\"point\"</code> similarly or appear too short, it might produce nearly identical embeddings.</li> <li>Backend artifact: ELMo via TF-hub may use averaged layers or frozen weights, which dampen context variation.</li> </ul> <p>\u2705 We\u2019ll run more varied and longer examples in later sections to highlight ELMo's dynamic behavior more clearly \u2014 including a \"cone of meaning\" visualization.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#3-intrinsic-evaluation-benchmarks","title":"\ud83e\uddea 3. Intrinsic Evaluation Benchmarks\u00b6","text":"<p>Now that our embeddings are loaded and verified, we\u2019ll begin evaluating their quality using standard intrinsic NLP benchmarks.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#what-well-evaluate","title":"\ud83d\udccf What We'll Evaluate\u00b6","text":"<ol> <li><p>Word Similarity Compare model similarity scores to human judgments (e.g., WordSim353)</p> </li> <li><p>Word Analogy Tasks Can the model solve <code>\"king - man + woman = queen\"</code>?</p> </li> <li><p>Semantic Clustering Visual inspection using PCA / t-SNE to see if similar concepts are grouped</p> </li> <li><p>Dimensionality Reduction &amp; Probing Check how separable semantic categories are in low dimensions</p> </li> </ol>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#datasets-benchmarks-well-use","title":"\ud83d\udcda Datasets &amp; Benchmarks We'll Use\u00b6","text":"Task Benchmark Dataset Format Word Similarity WordSim-353, SimLex-999 Word pairs + scores Analogies Google Analogies (Word2Vec-style) 4-word tuples Clustering Custom categories (e.g., colors, cities, family) Word lists <p>These allow us to compare models quantitatively and visually, building an intuition for what kind of meaning each model captures.</p> <p>Next up: let\u2019s implement the Word Similarity evaluation and compare our three models!</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#31-word-similarity-evaluation","title":"3.1 \ud83e\udde9 Word Similarity Evaluation\u00b6","text":"<p>In this task, we assess how well each embedding model captures human-perceived semantic similarity between word pairs.</p> <p>We\u2019ll use the WordSim-353 benchmark, which contains 353 word pairs annotated with human similarity scores (from 0 to 10).</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#evaluation-method","title":"\ud83e\uddea Evaluation Method\u00b6","text":"<ol> <li>For each word pair, compute the cosine similarity between their embeddings.</li> <li>Compare model similarity scores to human scores using Spearman's rank correlation.</li> <li>Higher correlation = better alignment with human judgments.</li> </ol>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#note-on-elmo","title":"\ud83d\udccc Note on ELMo\u00b6","text":"<p>Since ELMo is contextual, we\u2019ll provide each word as a standalone sentence (e.g., <code>\"apple\"</code> \u2192 <code>\"I like apple.\"</code>) While not ideal, this allows for fair comparison to static embeddings in this evaluation.</p> <p>Later, we\u2019ll evaluate ELMo\u2019s strength in context-aware settings.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-1-computing-cosine-similarity-between-word-pairs","title":"\ud83d\udd27 Step 1: Computing Cosine Similarity Between Word Pairs\u00b6","text":"<p>We define a helper function <code>compute_similarity()</code> to:</p> <ul> <li>Retrieve embeddings for two words</li> <li>Compute cosine similarity between them</li> <li>Handle special formatting for ELMo (requires fake context sentences)</li> </ul> <p>This function supports all three models: Word2Vec, FastText, and ELMo.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-2-compute-similarity-scores-for-wordsim-353","title":"\ud83e\uddea Step 2: Compute Similarity Scores for WordSim-353\u00b6","text":"<p>We now apply the similarity function to all 353 word pairs using:</p> <ul> <li>Word2Vec</li> <li>FastText</li> <li>ELMo</li> </ul> <p>We store the scores in new columns and prepare the data for analysis.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-3-compare-with-human-judgments-spearman-correlation","title":"\ud83d\udcca Step 3: Compare with Human Judgments (Spearman Correlation)\u00b6","text":"<p>We evaluate how well each model\u2019s similarity scores align with human judgments using Spearman's rank correlation.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-4-normalize-model-scores","title":"\u2696\ufe0f Step 4: Normalize Model Scores\u00b6","text":"<p>To visually compare model predictions with human ratings (0\u201310), we scale all similarity scores to the same range using min-max normalization.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-5-showcase-example-word-pairs","title":"\ud83d\udd0d Step 5: Showcase Example Word Pairs\u00b6","text":"<p>We highlight:</p> <ul> <li>Top 5 most similar pairs (according to humans)</li> <li>5 least similar pairs</li> <li>Polysemous word examples (e.g., \"bank\", \"cell\")</li> <li>Biggest disagreements between model predictions and human scores</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-6-global-evaluation-summary","title":"\ud83d\udcc8 Step 6: Global Evaluation Summary\u00b6","text":"<p>Here are the overall Spearman correlations between each model and human similarity judgments.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-7-visualizations","title":"\ud83d\udcca Step 7: Visualizations\u00b6","text":"<p>We plot:</p> <ol> <li>Bar chart of Spearman correlations for each model</li> <li>Scatter plot comparing human vs model scores to show linearity or bias</li> </ol>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#step-8-error-analysis-by-word-type","title":"\ud83e\udde0 Step 8: Error Analysis by Word Type\u00b6","text":"<p>We categorize word pairs as:</p> <ul> <li>Concrete (objects, physical items)</li> <li>Abstract (concepts, emotions)</li> <li>Other</li> </ul> <p>Then, we compute average error (|prediction \u2212 human|) for each model by category.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#conclusion-word-similarity-evaluation","title":"\u2705 Conclusion: Word Similarity Evaluation\u00b6","text":"<p>Our comparison across Word2Vec, FastText, and ELMo on the WordSim-353 benchmark reveals several key insights:</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#quantitative-results-spearman-correlation","title":"\ud83d\udcca Quantitative Results (Spearman Correlation)\u00b6","text":"Model Spearman Correlation Word2Vec 0.6876 FastText 0.7506 \u2705 ELMo 0.4521 \u274c <ul> <li>FastText leads in performance, likely due to its ability to handle rare words and morphology via subword units.</li> <li>Word2Vec performs well, but lacks FastText's flexibility for OOV words.</li> <li>ELMo, surprisingly, underperforms in this benchmark.</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#why-elmo-underperforms-here","title":"\ud83e\udd16 Why ELMo Underperforms Here\u00b6","text":"<p>Although ELMo is a contextual model, we\u2019re using it in an artificially decontextualized setting:</p> <ul> <li>We embed words in generic sentences like <code>\"[word]\"</code></li> <li>This does not reflect ELMo's true strength, which lies in adapting meaning based on actual sentence context.</li> </ul> <p>\ud83d\udccc This is like comparing apples and pears \u2014 contextual models aren't designed to output a fixed vector per word like static embeddings.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#error-patterns-by-word-type","title":"\ud83e\udde0 Error Patterns by Word Type\u00b6","text":"Word Type Avg Error (ELMo) Insight Abstract 1.33 ELMo struggled with vague, non-physical concepts Concrete 1.62 Context-free usage hurt ELMo's grounding in meaning Other 1.49 Mixed category, similar limitations"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#takeaway","title":"\ud83d\udd0d Takeaway\u00b6","text":"<p>ELMo was never meant to be used out-of-context \u2014 its strength is in modeling word meaning based on how it's used.</p> <p>In the next section, we\u2019ll move beyond static pairwise similarity and evaluate embeddings on word analogy tasks, where geometry matters more and context matters less.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#32-word-analogy-evaluation","title":"3.2 \ud83d\udd01 Word Analogy Evaluation\u00b6","text":"<p>The Google Word Analogy dataset is a classic benchmark to evaluate the relational structure of word embeddings.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#what-is-a-word-analogy","title":"\ud83e\udde0 What Is a Word Analogy?\u00b6","text":"<p>We test whether embeddings encode consistent vector relationships:</p> <p><code>\"man\" - \"woman\" \u2248 \"king\" - \"queen\"</code> Vector arithmetic: <code>king - man + woman \u2248 queen</code></p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#google-analogy-dataset","title":"\ud83d\udce6 Google Analogy Dataset\u00b6","text":"<ul> <li>Consists of over 19,000 analogies</li> <li>Divided into semantic (e.g. countries, capitals) and syntactic (e.g. verb tenses, pluralization) categories</li> <li>Format: <code>word1 word2 word3 word4</code> (predict word4)</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#evaluation-metric","title":"\ud83d\udcd0 Evaluation Metric\u00b6","text":"<p>We use top-1 accuracy:</p> <p>Is the predicted word (most similar to <code>vec2 - vec1 + vec3</code>) equal to the actual <code>word4</code>?</p> <p>Only Word2Vec and FastText will be evaluated here \u2014 ELMo is not well-suited because:</p> <ul> <li>It requires full-sentence context</li> <li>It does not produce a consistent vector per word, making arithmetic undefined</li> </ul> <p>Let\u2019s load the benchmark and implement the evaluation!</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#defining-the-evaluation-function","title":"\ud83e\udde0 Defining the Evaluation Function\u00b6","text":"<p>The core logic follows the classic Word2Vec analogy vector rule:</p> <p><code>vec_d \u2248 vec_b - vec_a + vec_c</code></p> <p>We then:</p> <ul> <li>Normalize the vector</li> <li>Search for the most similar word in the reduced vocabulary</li> <li>Count how many times the prediction matches the expected <code>word_d</code></li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#evaluating","title":"\ud83d\ude80 Evaluating\u00b6","text":"<p>We now run our evaluation function using:</p> <ul> <li>\u2705 Word2Vec</li> <li>\u2705 FastText</li> <li>\u2705 ELMo</li> </ul> <p>We'll evaluate on a sample of 1000 analogies for speed.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#results-summary-and-sample-predictions","title":"\ud83d\udcca Results Summary and Sample Predictions\u00b6","text":"<p>We display:</p> <ul> <li>Total accuracy for both models</li> <li>A few correct and incorrect examples</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#accuracy-bar-plot","title":"\ud83d\udcc8 Accuracy Bar Plot\u00b6","text":"<p>Here\u2019s a visual comparison of the overall performance of both models on the analogy task.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#conclusion-word-analogy-evaluation","title":"\u2705 Conclusion: Word Analogy Evaluation\u00b6","text":"<p>We evaluated Word2Vec, FastText, and ELMo on 1,000 analogy questions using the vector offset formula:</p> <p><code>vec(B) - vec(A) + vec(C) \u2248 vec(D)</code></p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#results-summary","title":"\ud83d\udcca Results Summary\u00b6","text":"Model Accuracy Word2Vec \u2705 100.0% FastText \u2705 100.0% ELMo \u274c 4.2%"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#what-do-these-results-tell-us","title":"\ud83d\udd0d What Do These Results Tell Us?\u00b6","text":"<ul> <li>Word2Vec and FastText absolutely nailed this benchmark. That\u2019s not surprising \u2014 the analogy format was designed with static embeddings in mind.</li> <li>FastText handles morphological patterns (e.g., <code>feed \u2192 fed</code>) especially well due to its subword modeling.</li> <li>ELMo, however, performs poorly here \u2014 not because it's bad, but because it's the wrong tool for the task:<ul> <li>It produces contextual (not fixed) embeddings.</li> <li>Vector arithmetic like <code>king - man + woman</code> makes little sense without consistent embeddings.</li> </ul> </li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#key-takeaway","title":"\ud83d\udccc Key Takeaway\u00b6","text":"<p>Static models shine in analogy-based evaluations. But contextual models like ELMo should not be judged with analogy vectors \u2014 they shine when the meaning of a word depends on its context.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#33-semantic-clustering-how-well-do-embeddings-group-similar-words","title":"\ud83e\udde9 3.3 Semantic Clustering \u2013 How Well Do Embeddings Group Similar Words?\u00b6","text":"<p>In this section, we'll analyze whether embeddings naturally group semantically related words together.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#what-were-testing","title":"\ud83d\udd0d What We\u2019re Testing\u00b6","text":"<p>We want to check if words from the same semantic category (e.g., \"dog\", \"cat\", \"lion\") are closer together in the embedding space than words from different categories (e.g., \"dog\" vs \"red\").</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#word-categories","title":"\ud83d\udcda Word Categories\u00b6","text":"<p>We manually define 7 semantic categories with 20 words each:</p> <ul> <li>Colors</li> <li>Cities</li> <li>Animals</li> <li>Family</li> <li>Emotions</li> <li>Food</li> <li>Technology</li> </ul> <p>We'll evaluate clustering behavior using:</p> <ul> <li>Word2Vec</li> <li>FastText</li> <li>ELMo (contextualized but with a neutral sentence)</li> </ul> <p>Let\u2019s first extract the embeddings for each category and model.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#t-sne-visualization-of-semantic-clusters","title":"\ud83c\udfa8 t-SNE Visualization of Semantic Clusters\u00b6","text":"<p>To visualize embeddings, we reduce their dimensionality to 2D using t-SNE and color-code each point by its category.</p> <p>We expect:</p> <ul> <li>Clear clusters for each category</li> <li>Some overlap for fuzzy categories (e.g., \"Emotions\", \"Family\")</li> </ul> <p>Let\u2019s compare clustering quality across models!</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#visualizing-clusters-with-t-sne","title":"\ud83d\uddbc\ufe0f Visualizing Clusters with t-SNE\u00b6","text":"<p>We now project the high-dimensional embeddings into 2D using t-SNE and color the words by their semantic category.</p> <p>We expect:</p> <ul> <li>Words from the same category to form tight clusters</li> <li>Different models to vary in how clearly they separate categories</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#quantitative-clustering-evaluation","title":"\ud83d\udcd0 Quantitative Clustering Evaluation\u00b6","text":"<p>We now compute metrics that measure how well clusters are separated:</p> <ul> <li>Intra-cluster distance: Average similarity within categories (lower is better)</li> <li>Inter-cluster distance: Average distance between categories (higher is better)</li> <li>Silhouette score: Combines both into a single metric (-1 to 1)</li> </ul> <p>Let\u2019s see which model organizes semantic categories most effectively.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#clustering-evaluation-key-takeaways","title":"\ud83e\udde0 Clustering Evaluation: Key Takeaways\u00b6","text":"<p>Based on our semantic clustering analysis, here\u2019s how the models performed:</p> Model Intra-cluster \u2193 Inter-cluster \u2191 Separation \u2191 Silhouette \u2191 Word2Vec 0.596 0.914 0.318 0.293 FastText \u2705 0.564 0.885 \u2705 0.321 \u2705 0.299 ELMo \u274c 0.725 \u2705 0.924 \u274c 0.198 \u274c 0.149"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#interpretation","title":"\ud83d\udd0d Interpretation\u00b6","text":"<ul> <li>FastText shows the best overall clustering behavior:<ul> <li>Lowest intra-cluster distances \u2192 Tighter semantic groups</li> <li>Best silhouette score \u2192 Strong global cohesion and separation</li> </ul> </li> <li>Word2Vec is close behind, and still forms well-separated clusters.</li> <li>ELMo, despite its contextual strengths, underperforms in static clustering tasks:<ul> <li>Its vectors vary with sentence context, so neutral context like \"The word X is commonly used.\" flattens its expressiveness</li> <li>This makes it less suitable for category-based static clustering</li> </ul> </li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#final-thoughts-on-intrinsic-evaluation","title":"\ud83d\udccc Final Thoughts on Intrinsic Evaluation\u00b6","text":"<p>Over our evaluations, we\u2019ve observed:</p> <ul> <li>Word Similarity: FastText aligns best with human judgments overall</li> <li>Analogy Tasks: Both Word2Vec and FastText excel, but ELMo struggles without contextualized pairs</li> <li>Clustering: FastText provides the most semantically organized space</li> </ul> <p>Overall, FastText consistently leads across multiple intrinsic metrics, thanks to its subword modeling and ability to generalize across morphology.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#next-extrinsic-evaluation","title":"\ud83d\ude80 Next: Extrinsic Evaluation\u00b6","text":"<p>Intrinsic methods evaluate embeddings in isolation, but ultimately:</p> <p>\"How useful are they in real NLP tasks?\"</p> <p>To answer that, we now turn to extrinsic evaluation:</p> <ul> <li>Named Entity Recognition (NER)</li> <li>Sentiment Classification</li> <li>Downstream performance using frozen embeddings</li> </ul> <p>Let\u2019s explore how each embedding performs when put into real-world tasks.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#extrinsic-evaluation-putting-embeddings-to-the-test","title":"\ud83d\udce6 Extrinsic Evaluation: Putting Embeddings to the Test\u00b6","text":"<p>So far, we\u2019ve evaluated word embeddings using intrinsic methods\u2014assessing their ability to capture meaning in isolation.</p> <p>But ultimately, we care about how well these embeddings help real NLP tasks like:</p> <ul> <li>Named Entity Recognition (NER)</li> <li>Sentiment Analysis</li> <li>Text Classification</li> <li>Question Answering, etc.</li> </ul> <p>This is where extrinsic evaluation comes in:</p> <p>How useful are the embeddings when integrated into an actual model?</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#task-1-named-entity-recognition-ner","title":"\ud83c\udfaf Task 1: Named Entity Recognition (NER)\u00b6","text":"<p>NER is a core NLP task that involves identifying and classifying named entities in text, such as:</p> <ul> <li><code>PER</code> \u2192 Person</li> <li><code>LOC</code> \u2192 Location</li> <li><code>ORG</code> \u2192 Organization</li> <li><code>MISC</code> \u2192 Miscellaneous</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#dataset-conll-2003","title":"\ud83e\uddea Dataset: CoNLL-2003\u00b6","text":"<p>We'll use the CoNLL-2003 dataset, a standard benchmark for NER in English.</p> <p>It includes annotations for:</p> <ul> <li>Named entities</li> <li>Part-of-speech tags</li> <li>Phrase chunking</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#evaluation-focus","title":"\ud83e\udde0 Evaluation Focus\u00b6","text":"<p>We'll test how different embeddings\u2014Word2Vec, FastText, and ELMo\u2014affect downstream NER performance when used as input features to a simple neural model.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#goal","title":"\ud83d\udee0\ufe0f Goal\u00b6","text":"<p>We'll:</p> <ol> <li>Load the CoNLL-2003 dataset</li> <li>Preprocess the data for token-level classification</li> <li>Integrate static and contextual embeddings</li> <li>Train and evaluate models on NER</li> </ol> <p>Let\u2019s begin by loading and inspecting the dataset.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#2-define-labels-and-vocabulary","title":"2. \ud83c\udff7\ufe0f Define Labels and Vocabulary\u00b6","text":"<p>We'll define the tag names used in the dataset and create a vocabulary of all tokens from the training set. Also, we'll calculate the max sequence length (capped at 128 tokens).</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#3-data-preparation","title":"3. \ud83e\uddf9 Data Preparation\u00b6","text":"<p>We'll create two types of preprocessing:</p> <ul> <li><code>prepare_baseline_data</code>: for baseline models using trainable embeddings</li> <li><code>prepare_data_with_pretrained_embeddings</code>: for models using Word2Vec, FastText, or ELMo embeddings</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#31-token-level-embedding-extraction-word2vec-fasttext-elmo","title":"3.1 \ud83d\udd21 Token-Level Embedding Extraction (Word2Vec, FastText, ELMo)\u00b6","text":"<p>For pre-trained embeddings, we extract representations for each token using the appropriate method.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#4-model-architectures","title":"4. \ud83e\udde0 Model Architectures\u00b6","text":"<p>We\u2019ll define two models:</p> <ul> <li>A baseline model using a trainable embedding layer</li> <li>A pre-trained embedding model using BiLSTMs over frozen token embeddings</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#5-model-evaluation-and-metrics","title":"5. \ud83d\udcca Model Evaluation and Metrics\u00b6","text":"<p>This function computes:</p> <ul> <li>Classification report</li> <li>Confusion matrix</li> <li>Predictions vs. true labels</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#6-displaying-predictions-correct-vs-incorrect-examples","title":"6. \ud83d\udd0e Displaying Predictions: Correct vs Incorrect Examples\u00b6","text":"<p>These functions display real examples from the test set, highlighting where the model predicted correctly and where it made mistakes.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#7-visualizing-model-performance-by-entity-type","title":"7. \ud83d\udcc8 Visualizing Model Performance by Entity Type\u00b6","text":"<p>We\u2019ll compare F1 scores across different entity types (<code>PER</code>, <code>LOC</code>, <code>ORG</code>, <code>MISC</code>) and display overall metrics (precision, recall, F1) for each model.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#8-model-training-and-evaluation-pipeline","title":"8. \u2699\ufe0f Model Training and Evaluation Pipeline\u00b6","text":"<p>This function:</p> <ul> <li>Prepares data</li> <li>Builds and trains the model</li> <li>Evaluates on test data</li> <li>Displays predictions and returns metrics</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#9-run-all-embedding-models-and-compare","title":"9. \ud83e\uddea Run All Embedding Models and Compare\u00b6","text":"<p>We'll now run:</p> <ul> <li>A baseline model with trainable embeddings</li> <li>Word2Vec-based NER model</li> <li>FastText-based NER model</li> <li>ELMo-based NER model</li> </ul> <p>We'll collect their metrics for comparison.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#10-final-summary-and-comparison","title":"10. \ud83d\udccb Final Summary and Comparison\u00b6","text":"<p>Here\u2019s the overall comparison of F1 scores and per-entity performance across all models.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#final-analysis-named-entity-recognition-with-different-embeddings","title":"\ud83d\udcc8 Final Analysis: Named Entity Recognition with Different Embeddings\u00b6","text":"<p>After training and evaluating four models on the CoNLL-2003 dataset, we can clearly observe how the choice of word embedding impacts downstream performance in a Named Entity Recognition (NER) task.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#macro-f1-score-comparison","title":"\u2696\ufe0f Macro F1-Score Comparison:\u00b6","text":"Model Macro F1 Weighted F1 Precision Recall Baseline (trainable embedding) 0.5967 0.8763 0.6659 0.5619 Word2Vec 0.4983 0.8835 0.6128 0.4797 FastText 0.6885 0.9238 0.8085 0.6392 ELMo 0.8768 0.9713 0.8868 0.8691"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#key-takeaways","title":"\ud83d\udd0d Key Takeaways:\u00b6","text":"<ol> <li><p>Word Embeddings Improve Performance Even the simplest static embeddings like Word2Vec or FastText improve over the baseline model (weighted F1), proving the value of transfer learning through pre-trained vectors.</p> </li> <li><p>FastText Outperforms Word2Vec FastText captures subword information and is especially useful for rare or out-of-vocabulary words \u2014 resulting in better generalization and stronger metrics than Word2Vec.</p> </li> <li><p>ELMo Dominates the Task With its deep, contextualized representations, ELMo delivers the highest performance across the board. It excels at capturing the meaning of words in their specific context, making it ideal for NER, where ambiguity and polysemy are common (e.g., \u201cApple\u201d as a fruit vs. a company).</p> </li> <li><p>Macro vs. Weighted F1 While weighted F1 scores are high across models due to class imbalance, the macro F1 highlights how well the model handles all entity types equally. ELMo again shines in this setting, offering balanced performance across all classes.</p> </li> </ol>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#final-thoughts","title":"\ud83d\udccc Final Thoughts\u00b6","text":"<p>This extrinsic evaluation validates the findings from our intrinsic experiments:</p> <p>Contextual embeddings like ELMo significantly outperform static embeddings in real NLP tasks.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#extrinsic-evaluation-sentiment-classification-on-sst-2","title":"\ud83d\udccc Extrinsic Evaluation \u2014 Sentiment Classification on SST-2\u00b6","text":"<p>In this final extrinsic evaluation, we assess the utility of word embeddings for sentence-level sentiment classification using the SST-2 (Stanford Sentiment Treebank) dataset.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#why-this-task","title":"\ud83d\udca1 Why This Task?\u00b6","text":"<p>While NER evaluated how well embeddings captured sequence labeling capabilities, sentiment classification examines how well they support global sentence understanding \u2014 especially important for capturing compositional meaning and nuanced expressions.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#embedding-models-compared","title":"\ud83e\uddea Embedding Models Compared:\u00b6","text":"<ul> <li>Baseline: Trainable embedding from scratch</li> <li>Word2Vec: Static embedding trained on a large corpus</li> <li>FastText: Subword-aware static embedding</li> <li>ELMo: Deep, contextual embedding from biLMs</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#loading-the-sst-2-dataset","title":"\ud83d\udce5 Loading the SST-2 Dataset\u00b6","text":"<p>We load a subset of the SST-2 dataset from Hugging Face, limiting each split for fast experimentation.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#vocabulary-construction-and-sequence-padding","title":"\ud83e\uddfe Vocabulary Construction and Sequence Padding\u00b6","text":"<p>We tokenize each sentence and build a word-to-index mapping to prepare sequences for the models. We also fix the maximum sequence length.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#data-preparation","title":"\u2699\ufe0f Data Preparation\u00b6","text":"<p>We define separate functions to prepare input data for the baseline (trainable embeddings) and pretrained models (Word2Vec, FastText, ELMo).</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#model-definitions","title":"\ud83e\udde0 Model Definitions\u00b6","text":"<p>Two types of models are used:</p> <ul> <li>Baseline Model with a trainable embedding layer</li> <li>Pretrained Models using fixed embeddings (passed as input)</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#evaluation-utilities","title":"\ud83e\uddea Evaluation Utilities\u00b6","text":"<p>We define utilities for:</p> <ul> <li>Computing metrics like Accuracy and F1 Score</li> <li>Plotting bar charts and confusion matrices</li> <li>Printing correct/incorrect predictions</li> </ul>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#training-pipelines","title":"\ud83d\ude80 Training Pipelines\u00b6","text":"<p>We define one function for each model type that handles training and evaluation:</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#final-results-comparison","title":"\ud83d\udcca Final Results &amp; Comparison\u00b6","text":"<p>Once all models have run, we display their performance metrics and plots.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#conclusion-extrinsic-evaluation","title":"\u2705 Conclusion: Extrinsic Evaluation\u00b6","text":"<p>Across two downstream NLP tasks \u2014 Named Entity Recognition (NER) and Sentiment Classification (SST-2) \u2014 we assessed the real-world effectiveness of different types of word embeddings.</p>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#summary-of-results","title":"\ud83d\udd2c Summary of Results\u00b6","text":"Task Model F1 Score NER (CoNLL-2003) Baseline 0.8763 Word2Vec 0.8835 FastText 0.9238 ELMo 0.9713 SST-2 Sentiment Baseline 0.8036 Word2Vec 0.8367 FastText 0.8214 ELMo 0.8467"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#key-takeaways","title":"\ud83e\udde0 Key Takeaways\u00b6","text":"<ol> <li><p>Pretrained Embeddings Improve Generalization: Both Word2Vec and FastText improve upon the baseline, confirming that pretrained embeddings offer strong inductive bias, even with small training sets.</p> </li> <li><p>FastText Outperforms Word2Vec: FastText's subword modeling gives it an edge, especially for rare or OOV words \u2014 visible in both NER and SST-2 tasks.</p> </li> <li><p>ELMo Leads the Pack: ELMo consistently achieves the best performance. Its contextual nature allows it to dynamically adapt word representations based on sentence-level context \u2014 especially helpful in cases where static embeddings fail to disambiguate meaning.</p> </li> <li><p>NER vs. Classification:</p> <ul> <li>On NER, the performance gap between models is more pronounced due to the complexity of multi-class sequence tagging and entity boundaries.</li> <li>On SST-2, the difference is smaller but still significant, suggesting that even sentence-level tasks benefit from contextual encoding.</li> </ul> </li> </ol>"},{"location":"chapter3/Session_3_2_Embedding_Evaluation/#final-thoughts","title":"\ud83e\udde9 Final Thoughts\u00b6","text":"<p>This extrinsic evaluation confirms that:</p> <ul> <li>Static embeddings (Word2Vec, FastText) are fast and effective.</li> <li>Contextual embeddings (ELMo) are more powerful, especially when polysemy and context sensitivity matter.</li> <li>For modern NLP pipelines, contextual models \u2014 even if more expensive \u2014 offer the best trade-off between performance and robustness.</li> </ul>"},{"location":"chapter3/Session_3_3_Embedding_Classification/","title":"\ud83d\udcda Embedding Aggregation for Text Classification: TF-IDF vs FastText vs ELMo","text":"In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# Hugging Face datasets\nfrom datasets import load_dataset\n\n# TF-IDF vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# FastText downloader\nfrom huggingface_hub import hf_hub_download\n\n# TensorFlow &amp; ELMo\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n# Model and evaluation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Ensure reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n</pre> import os import numpy as np import pandas as pd from tqdm.auto import tqdm  # Hugging Face datasets from datasets import load_dataset  # TF-IDF vectorizer from sklearn.feature_extraction.text import TfidfVectorizer  # FastText downloader from huggingface_hub import hf_hub_download  # TensorFlow &amp; ELMo import tensorflow as tf import tensorflow_hub as hub  # Model and evaluation from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report, accuracy_score  # Ensure reproducibility SEED = 42 np.random.seed(SEED) tf.random.set_seed(SEED) In\u00a0[2]: Copied! <pre># Load IMDB dataset\nimdb = load_dataset(\"imdb\")\n\n# Extract train and test splits\ntrain_texts = imdb[\"train\"][\"text\"]\ntrain_labels = np.array(imdb[\"train\"][\"label\"])\ntest_texts  = imdb[\"test\"][\"text\"]\ntest_labels  = np.array(imdb[\"test\"][\"label\"])\n\n# Quick sanity check\nprint(f\"Train examples: {len(train_texts)}\")\nprint(f\"Test  examples: {len(test_texts)}\\n\")\n\nprint(\"Example review (train):\")\nprint(train_texts[0][:500], \"...\\n\")\nprint(\"Label:\", train_labels[0])\n</pre> # Load IMDB dataset imdb = load_dataset(\"imdb\")  # Extract train and test splits train_texts = imdb[\"train\"][\"text\"] train_labels = np.array(imdb[\"train\"][\"label\"]) test_texts  = imdb[\"test\"][\"text\"] test_labels  = np.array(imdb[\"test\"][\"label\"])  # Quick sanity check print(f\"Train examples: {len(train_texts)}\") print(f\"Test  examples: {len(test_texts)}\\n\")  print(\"Example review (train):\") print(train_texts[0][:500], \"...\\n\") print(\"Label:\", train_labels[0]) <pre>Train examples: 25000\nTest  examples: 25000\n\nExample review (train):\nI rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attent ...\n\nLabel: 0\n</pre> In\u00a0[12]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nclass Metrics:\n    def __init__(self):\n        self.results = {}\n\n    def run(self, y_true, y_pred, method_name):\n        # Calculate metrics\n        accuracy = accuracy_score(y_true, y_pred)\n        precision = precision_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred)\n\n        # Store results\n        self.results[method_name] = {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n        }\n\n    def plot(self):\n        # Create subplots\n        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Plot each metric\n        for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n            ax = axs[i//2, i%2]\n            values = [res[metric] * 100 for res in self.results.values()]\n            ax.bar(self.results.keys(), values)\n            ax.set_title(metric.capitalize())\n            ax.set_ylim(0, 100)\n\n            # Add values on top of bars\n            for j, v in enumerate(values):\n                ax.text(j, v + 0.5, f\"{v:.2f}%\", ha='center', va='bottom')\n\n        plt.tight_layout()\n        plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  class Metrics:     def __init__(self):         self.results = {}      def run(self, y_true, y_pred, method_name):         # Calculate metrics         accuracy = accuracy_score(y_true, y_pred)         precision = precision_score(y_true, y_pred)         recall = recall_score(y_true, y_pred)         f1 = f1_score(y_true, y_pred)          # Store results         self.results[method_name] = {             'accuracy': accuracy,             'precision': precision,             'recall': recall,             'f1': f1,         }      def plot(self):         # Create subplots         fig, axs = plt.subplots(2, 2, figsize=(15, 10))          # Plot each metric         for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):             ax = axs[i//2, i%2]             values = [res[metric] * 100 for res in self.results.values()]             ax.bar(self.results.keys(), values)             ax.set_title(metric.capitalize())             ax.set_ylim(0, 100)              # Add values on top of bars             for j, v in enumerate(values):                 ax.text(j, v + 0.5, f\"{v:.2f}%\", ha='center', va='bottom')          plt.tight_layout()         plt.show()  In\u00a0[13]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# Vectorize with TF-IDF\ntfidf = TfidfVectorizer(max_features=50000, ngram_range=(1,2), min_df=5, stop_words='english')\nX_train_tfidf = tfidf.fit_transform(train_texts)\nX_test_tfidf  = tfidf.transform(test_texts)\n\n# Train Logistic Regression\nclf_tfidf = LogisticRegression(max_iter=1000)\nclf_tfidf.fit(X_train_tfidf, train_labels)\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression  # Vectorize with TF-IDF tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1,2), min_df=5, stop_words='english') X_train_tfidf = tfidf.fit_transform(train_texts) X_test_tfidf  = tfidf.transform(test_texts)  # Train Logistic Regression clf_tfidf = LogisticRegression(max_iter=1000) clf_tfidf.fit(X_train_tfidf, train_labels)  Out[13]: <pre>LogisticRegression(max_iter=1000)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression(max_iter=1000)</pre> In\u00a0[14]: Copied! <pre># Predict &amp; evaluate\ny_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n\nmetrics = Metrics()\nmetrics.run(test_labels, y_pred_tfidf, \"TF-IDF Baseline\")\nmetrics.plot()\n</pre> # Predict &amp; evaluate y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)  metrics = Metrics() metrics.run(test_labels, y_pred_tfidf, \"TF-IDF Baseline\") metrics.plot() In\u00a0[6]: Copied! <pre>from huggingface_hub import hf_hub_download\nimport fasttext\n\n# Download &amp; load FastText vectors\nfasttext_model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\nft = fasttext.load_model(fasttext_model_path)\n\n# Example: get vector for a word\nprint(\"Vector dim:\", ft.get_word_vector(\"movie\").shape)\n</pre> from huggingface_hub import hf_hub_download import fasttext  # Download &amp; load FastText vectors fasttext_model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\") ft = fasttext.load_model(fasttext_model_path)  # Example: get vector for a word print(\"Vector dim:\", ft.get_word_vector(\"movie\").shape) <pre>Vector dim: (300,)\n</pre> In\u00a0[7]: Copied! <pre>import scipy.sparse as sp\n\ndef ft_concat_embedding(texts, model):\n    \"\"\"Concatenate all word vectors for each text (pads/truncates to max_len).\"\"\"\n    max_len = 100  # for example\n    dims = model.get_word_vector(\"test\").shape[0]\n    features = []\n    for doc in texts:\n        vecs = [model.get_word_vector(w) for w in doc.split()][:max_len]\n        # pad with zeros if shorter\n        if len(vecs) &lt; max_len:\n            vecs.extend([np.zeros(dims)] * (max_len - len(vecs)))\n        features.append(np.concatenate(vecs))\n    return np.vstack(features)\n\ndef ft_mean_embedding(texts, model):\n    \"\"\"Mean of word vectors for each text.\"\"\"\n    features = []\n    for doc in texts:\n        vecs = np.array([model.get_word_vector(w) for w in doc.split()])\n        features.append(vecs.mean(axis=0) if len(vecs) else np.zeros(model.get_word_vector(\"test\").shape))\n    return np.vstack(features)\n\ndef ft_tfidf_weighted_embedding(texts, model, tfidf_vec, tfidf_matrix):\n    \"\"\"TF-IDF weighted average of word vectors.\"\"\"\n    features = []\n    tfidf_vocab = tfidf_vec.vocabulary_\n    idf = tfidf_vec.idf_\n    for i, doc in enumerate(texts):\n        words = doc.split()\n        weights = []\n        vecs = []\n        for w in words:\n            if w in tfidf_vocab:\n                w_idx = tfidf_vocab[w]\n                weights.append(idf[w_idx])\n                vecs.append(model.get_word_vector(w))\n        if vecs:\n            wgt = np.average(np.array(vecs), axis=0, weights=weights)\n        else:\n            wgt = np.zeros(model.get_word_vector(\"test\").shape)\n        features.append(wgt)\n    return np.vstack(features)\n</pre> import scipy.sparse as sp  def ft_concat_embedding(texts, model):     \"\"\"Concatenate all word vectors for each text (pads/truncates to max_len).\"\"\"     max_len = 100  # for example     dims = model.get_word_vector(\"test\").shape[0]     features = []     for doc in texts:         vecs = [model.get_word_vector(w) for w in doc.split()][:max_len]         # pad with zeros if shorter         if len(vecs) &lt; max_len:             vecs.extend([np.zeros(dims)] * (max_len - len(vecs)))         features.append(np.concatenate(vecs))     return np.vstack(features)  def ft_mean_embedding(texts, model):     \"\"\"Mean of word vectors for each text.\"\"\"     features = []     for doc in texts:         vecs = np.array([model.get_word_vector(w) for w in doc.split()])         features.append(vecs.mean(axis=0) if len(vecs) else np.zeros(model.get_word_vector(\"test\").shape))     return np.vstack(features)  def ft_tfidf_weighted_embedding(texts, model, tfidf_vec, tfidf_matrix):     \"\"\"TF-IDF weighted average of word vectors.\"\"\"     features = []     tfidf_vocab = tfidf_vec.vocabulary_     idf = tfidf_vec.idf_     for i, doc in enumerate(texts):         words = doc.split()         weights = []         vecs = []         for w in words:             if w in tfidf_vocab:                 w_idx = tfidf_vocab[w]                 weights.append(idf[w_idx])                 vecs.append(model.get_word_vector(w))         if vecs:             wgt = np.average(np.array(vecs), axis=0, weights=weights)         else:             wgt = np.zeros(model.get_word_vector(\"test\").shape)         features.append(wgt)     return np.vstack(features) In\u00a0[8]: Copied! <pre># Example texts\nsample_texts = [\n    \"This movie was outstanding and full of suspense\",\n    \"I did not enjoy the plot, it was too predictable\",\n    \"An absolute masterpiece with brilliant acting\"\n]\n\ntfidf_matrix = tfidf.transform(sample_texts) #we already fit the vectorizer on the corpus previously\n\n# 3. Generate embeddings\nemb_concat = ft_concat_embedding(sample_texts, ft)\nemb_mean   = ft_mean_embedding(sample_texts, ft)\nemb_tf     = ft_tfidf_weighted_embedding(sample_texts, ft, tfidf, tfidf_matrix)\n\n# 4. Inspect shapes and a snippet of the first vector\nprint(\"Concatenation:\", emb_concat.shape, \"\u2014 first 5 values:\", emb_concat[0][:5])\nprint(\"Mean pooling:\", emb_mean.shape, \"\u2014 first 5 values:\", emb_mean[0][:5])\nprint(\"TF-IDF wgt:\", emb_tf.shape,   \"\u2014 first 5 values:\", emb_tf[0][:5])\n</pre> # Example texts sample_texts = [     \"This movie was outstanding and full of suspense\",     \"I did not enjoy the plot, it was too predictable\",     \"An absolute masterpiece with brilliant acting\" ]  tfidf_matrix = tfidf.transform(sample_texts) #we already fit the vectorizer on the corpus previously  # 3. Generate embeddings emb_concat = ft_concat_embedding(sample_texts, ft) emb_mean   = ft_mean_embedding(sample_texts, ft) emb_tf     = ft_tfidf_weighted_embedding(sample_texts, ft, tfidf, tfidf_matrix)  # 4. Inspect shapes and a snippet of the first vector print(\"Concatenation:\", emb_concat.shape, \"\u2014 first 5 values:\", emb_concat[0][:5]) print(\"Mean pooling:\", emb_mean.shape, \"\u2014 first 5 values:\", emb_mean[0][:5]) print(\"TF-IDF wgt:\", emb_tf.shape,   \"\u2014 first 5 values:\", emb_tf[0][:5]) <pre>Concatenation: (3, 30000) \u2014 first 5 values: [-0.03508458  0.10469877  0.00859013  0.10003099  0.02248639]\nMean pooling: (3, 300) \u2014 first 5 values: [-0.0043145  -0.00577394 -0.0089867   0.00266586 -0.00065803]\nTF-IDF wgt: (3, 300) \u2014 first 5 values: [-0.01418748 -0.00709886  0.03651825  0.03428475  0.01234921]\n</pre> In\u00a0[10]: Copied! <pre>from tqdm import tqdm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n# 3. Build three pipelines using our embedding functions + logistic regression\npipelines = {\n    \"FT_Concat\": Pipeline([\n        (\"embed\", FunctionTransformer(lambda X: ft_concat_embedding(X, ft), validate=False)),\n        (\"clf\",   LogisticRegression(max_iter=1000))\n    ]),\n    \"FT_Mean\": Pipeline([\n        (\"embed\", FunctionTransformer(lambda X: ft_mean_embedding(X, ft), validate=False)),\n        (\"clf\",   LogisticRegression(max_iter=1000))\n    ]),\n    \"FT_TFIDF\": Pipeline([\n        (\"embed\", FunctionTransformer(\n            lambda X: ft_tfidf_weighted_embedding(X, ft, tfidf, tfidf.transform(X)),\n            validate=False\n        )),\n        (\"clf\",   LogisticRegression(max_iter=1000))\n    ]),\n}\n\nfor name, pipe in tqdm(pipelines.items()):\n    pipe.fit(train_texts, train_labels)\n</pre> from tqdm import tqdm from sklearn.pipeline import Pipeline from sklearn.preprocessing import FunctionTransformer  # 3. Build three pipelines using our embedding functions + logistic regression pipelines = {     \"FT_Concat\": Pipeline([         (\"embed\", FunctionTransformer(lambda X: ft_concat_embedding(X, ft), validate=False)),         (\"clf\",   LogisticRegression(max_iter=1000))     ]),     \"FT_Mean\": Pipeline([         (\"embed\", FunctionTransformer(lambda X: ft_mean_embedding(X, ft), validate=False)),         (\"clf\",   LogisticRegression(max_iter=1000))     ]),     \"FT_TFIDF\": Pipeline([         (\"embed\", FunctionTransformer(             lambda X: ft_tfidf_weighted_embedding(X, ft, tfidf, tfidf.transform(X)),             validate=False         )),         (\"clf\",   LogisticRegression(max_iter=1000))     ]), }  for name, pipe in tqdm(pipelines.items()):     pipe.fit(train_texts, train_labels) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [07:30&lt;00:00, 150.33s/it]\n</pre> In\u00a0[15]: Copied! <pre>for name, pipe in pipelines.items():    \n    preds = pipe.predict(test_texts)\n    metrics.run(test_labels, preds, method_name=name)\n\n# 5. Visualize all results\nmetrics.plot()\n</pre> for name, pipe in pipelines.items():         preds = pipe.predict(test_texts)     metrics.run(test_labels, preds, method_name=name)  # 5. Visualize all results metrics.plot() In\u00a0[35]: Copied! <pre>from tqdm import tqdm\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Load the ELMo model\nelmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")\n\nclass ELMoTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"A scikit-learn transformer for extracting mean-pooled ELMo embeddings.\"\"\"\n    def __init__(self, model):\n        self.model = model\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # Process in batches for better performance\n        batch_size = 32  # You can adjust this based on your available memory\n        num_samples = len(X)\n        num_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division\n        \n        embeddings_list = []\n        \n        for i in tqdm(range(num_batches)):\n            # Get current batch\n            start_idx = i * batch_size\n            end_idx = min((i + 1) * batch_size, num_samples)\n            batch_texts = X[start_idx:end_idx]\n            \n            # Process batch\n            embedding = self.model.signatures[\"default\"](\n                tf.constant(batch_texts)\n            )\n            \n            # Extract embeddings from batch\n            batch_embeddings = embedding[\"default\"].numpy()\n            embeddings_list.append(batch_embeddings)\n        \n        # Concatenate all batches\n        return np.vstack(embeddings_list)\n</pre> from tqdm import tqdm import tensorflow as tf import tensorflow_hub as hub from sklearn.base import BaseEstimator, TransformerMixin  # Load the ELMo model elmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")  class ELMoTransformer(BaseEstimator, TransformerMixin):     \"\"\"A scikit-learn transformer for extracting mean-pooled ELMo embeddings.\"\"\"     def __init__(self, model):         self.model = model      def fit(self, X, y=None):         return self      def transform(self, X):         # Process in batches for better performance         batch_size = 32  # You can adjust this based on your available memory         num_samples = len(X)         num_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division                  embeddings_list = []                  for i in tqdm(range(num_batches)):             # Get current batch             start_idx = i * batch_size             end_idx = min((i + 1) * batch_size, num_samples)             batch_texts = X[start_idx:end_idx]                          # Process batch             embedding = self.model.signatures[\"default\"](                 tf.constant(batch_texts)             )                          # Extract embeddings from batch             batch_embeddings = embedding[\"default\"].numpy()             embeddings_list.append(batch_embeddings)                  # Concatenate all batches         return np.vstack(embeddings_list)  In\u00a0[33]: Copied! <pre># Grab a single test review\nsingle_review = train_texts[0]\nprint(\"Raw review text:\\n\", single_review, \"\\n\")\n\n# Instantiate and transform\nelmo_trans = ELMoTransformer(elmo_model)\nsingle_emb = elmo_trans.transform([single_review])\n\n# Inspect the output\nprint(\"ELMo embedding shape:\", single_emb.shape)\nprint(\"First 5 dimensions of embedding:\\n\", single_emb[0][:5])\n</pre> # Grab a single test review single_review = train_texts[0] print(\"Raw review text:\\n\", single_review, \"\\n\")  # Instantiate and transform elmo_trans = ELMoTransformer(elmo_model) single_emb = elmo_trans.transform([single_review])  # Inspect the output print(\"ELMo embedding shape:\", single_emb.shape) print(\"First 5 dimensions of embedding:\\n\", single_emb[0][:5])  <pre>Raw review text:\n I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.&lt;br /&gt;&lt;br /&gt;The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.&lt;br /&gt;&lt;br /&gt;What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.&lt;br /&gt;&lt;br /&gt;I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot. \n\nELMo embedding shape: (1, 1024)\nFirst 5 dimensions of embedding:\n [-0.05402381 -0.11614838 -0.19337955 -0.11014549  0.09235258]\n</pre> In\u00a0[40]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n\n# Assemble the pipeline\nclf = LogisticRegression(max_iter=1000)\n\n# First, create an instance of the ELMoTransformer class\nelmo_transformer = ELMoTransformer(elmo_model)\n\n# Then call the transform method on the instance\ntrain_elmo_embeddings = elmo_transformer.transform(train_texts)\ntest_elmo_embeddings = elmo_transformer.transform(test_texts)\n\n# Now train your classifier\nclf = LogisticRegression(max_iter=1000)\nclf.fit(train_elmo_embeddings, train_labels)\n</pre> from sklearn.linear_model import LogisticRegression  # Assemble the pipeline clf = LogisticRegression(max_iter=1000)  # First, create an instance of the ELMoTransformer class elmo_transformer = ELMoTransformer(elmo_model)  # Then call the transform method on the instance train_elmo_embeddings = elmo_transformer.transform(train_texts) test_elmo_embeddings = elmo_transformer.transform(test_texts)  # Now train your classifier clf = LogisticRegression(max_iter=1000) clf.fit(train_elmo_embeddings, train_labels) <pre>  4%|\u258d         | 33/782 [2:14:25&lt;50:50:52, 244.40s/it]  \n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[40], line 10\n      7 elmo_transformer = ELMoTransformer(elmo_model)\n      9 # Then call the transform method on the instance\n---&gt; 10 train_elmo_embeddings = elmo_transformer.transform(train_texts)\n     11 test_elmo_embeddings = elmo_transformer.transform(test_texts)\n     13 # Now train your classifier\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319, in _wrap_method_output.&lt;locals&gt;.wrapped(self, X, *args, **kwargs)\n    317 @wraps(f)\n    318 def wrapped(self, X, *args, **kwargs):\n--&gt; 319     data_to_wrap = f(self, X, *args, **kwargs)\n    320     if isinstance(data_to_wrap, tuple):\n    321         # only wrap the first output for cross decomposition\n    322         return_tuple = (\n    323             _wrap_data_with_container(method, data_to_wrap[0], X, self),\n    324             *data_to_wrap[1:],\n    325         )\n\nCell In[35], line 32, in ELMoTransformer.transform(self, X)\n     29 batch_texts = X[start_idx:end_idx]\n     31 # Process batch\n---&gt; 32 embedding = self.model.signatures[\"default\"](\n     33     tf.constant(batch_texts)\n     34 )\n     36 # Extract embeddings from batch\n     37 batch_embeddings = embedding[\"default\"].numpy()\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1170, in ConcreteFunction.__call__(self, *args, **kwargs)\n   1120 def __call__(self, *args, **kwargs):\n   1121   \"\"\"Executes the wrapped function.\n   1122 \n   1123   ConcreteFunctions have two signatures:\n   (...)   1168     TypeError: If the arguments do not match the function's signature.\n   1169   \"\"\"\n-&gt; 1170   return self._call_impl(args, kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/wrap_function.py:261, in WrappedFunction._call_impl(self, args, kwargs)\n    259   return self._call_flat(args, self.captured_inputs)\n    260 else:\n--&gt; 261   return super()._call_impl(args, kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1179, in ConcreteFunction._call_impl(self, args, kwargs)\n   1177 if self.function_type is not None:\n   1178   try:\n-&gt; 1179     return self._call_with_structured_signature(args, kwargs)\n   1180   except TypeError as structured_err:\n   1181     try:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1263, in ConcreteFunction._call_with_structured_signature(self, args, kwargs)\n   1258 bound_args = (\n   1259     function_type_utils.canonicalize_function_inputs(\n   1260         args, kwargs, self.function_type)\n   1261 )\n   1262 filtered_flat_args = self.function_type.unpack_inputs(bound_args)\n-&gt; 1263 return self._call_flat(\n   1264     filtered_flat_args,\n   1265     captured_inputs=self.captured_inputs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322, in ConcreteFunction._call_flat(self, tensor_inputs, captured_inputs)\n   1318 possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n   1319 if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n   1320     and executing_eagerly):\n   1321   # No tape is watching; skip to running the function.\n-&gt; 1322   return self._inference_function.call_preflattened(args)\n   1323 forward_backward = self._select_forward_and_backward_functions(\n   1324     args,\n   1325     possible_gradient_type,\n   1326     executing_eagerly)\n   1327 forward_function, args_with_tangents = forward_backward.forward()\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216, in AtomicFunction.call_preflattened(self, args)\n    214 def call_preflattened(self, args: Sequence[core.Tensor]) -&gt; Any:\n    215   \"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\n--&gt; 216   flat_outputs = self.call_flat(*args)\n    217   return self.function_type.pack_output(flat_outputs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251, in AtomicFunction.call_flat(self, *args)\n    249 with record.stop_recording():\n    250   if self._bound_context.executing_eagerly():\n--&gt; 251     outputs = self._bound_context.call_function(\n    252         self.name,\n    253         list(args),\n    254         len(self.function_type.flat_outputs),\n    255     )\n    256   else:\n    257     outputs = make_call_op_in_graph(\n    258         self,\n    259         list(args),\n    260         self._bound_context.function_call_options.as_attrs(),\n    261     )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688, in Context.call_function(self, name, tensor_inputs, num_outputs)\n   1686 cancellation_context = cancellation.context()\n   1687 if cancellation_context is None:\n-&gt; 1688   outputs = execute.execute(\n   1689       name.decode(\"utf-8\"),\n   1690       num_outputs=num_outputs,\n   1691       inputs=tensor_inputs,\n   1692       attrs=attrs,\n   1693       ctx=self,\n   1694   )\n   1695 else:\n   1696   outputs = execute.execute_with_cancellation(\n   1697       name.decode(\"utf-8\"),\n   1698       num_outputs=num_outputs,\n   (...)   1702       cancellation_manager=cancellation_context,\n   1703   )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/bse-nlp-DetGwK6_-py3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     51 try:\n     52   ctx.ensure_initialized()\n---&gt; 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n     54                                       inputs, attrs, num_outputs)\n     55 except core._NotOkStatusException as e:\n     56   if name is not None:\n\nKeyboardInterrupt: </pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate\ny_elmo = clf.predict(test_elmo_embeddings)\nmetrics.run(test_labels, y_elmo, \"ELMo\")\nmetrics.plot()\n</pre> # Evaluate y_elmo = clf.predict(test_elmo_embeddings) metrics.run(test_labels, y_elmo, \"ELMo\") metrics.plot()"},{"location":"chapter3/Session_3_3_Embedding_Classification/#embedding-aggregation-for-text-classification-tf-idf-vs-fasttext-vs-elmo","title":"\ud83d\udcda Embedding Aggregation for Text Classification: TF-IDF vs FastText vs ELMo\u00b6","text":"<p>In this notebook, we explore how different vectorization strategies impact the performance of a logistic regression classifier on the IMDB sentiment dataset. We will examine:</p> <ul> <li>TF-IDF bag-of-words</li> <li>FastText static word embeddings</li> <li>ELMo contextual embeddings</li> </ul> <p>Our focus is on how to transform word embeddings into sentence embeddings and how these choices affect classification.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#goal","title":"\ud83e\udded Goal\u00b6","text":"<ul> <li><p>Implement logistic regression classifiers using three feature types:</p> <ol> <li>TF-IDF vectors</li> <li>FastText word embeddings aggregated into sentence vectors</li> <li>ELMo embeddings aggregated into sentence vectors</li> </ol> </li> <li><p>Compare three pooling strategies for embedding-based methods:</p> <ul> <li>Concatenation (fixed-length, truncated/padded)</li> <li>Mean pooling</li> <li>TF-IDF-weighted mean pooling</li> </ul> </li> <li><p>Benchmark against native ELMo sentence representations.</p> </li> <li><p>Analyze differences in word importance between TF-IDF and median-based embeddings.</p> </li> </ul>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#table-of-contents","title":"\ud83d\udccc Table of Contents\u00b6","text":"<ol> <li>Environment Setup &amp; Imports</li> <li>Data Loading &amp; Preprocessing</li> <li>TF-IDF Baseline</li> <li>FastText Embeddings<ul> <li>Loading FastText</li> <li>Aggregation Strategies</li> <li>Classification</li> </ul> </li> <li>ELMo Embeddings</li> <li>Model Comparison &amp; Interpretation</li> <li>Conclusion</li> </ol>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#0-introduction","title":"0. \ud83e\udde0 Introduction\u00b6","text":"<p>Turning raw text into numerical features is a key step in NLP pipelines. While TF-IDF treats each word as an independent feature, word embeddings capture semantic relationships but require pooling to create fixed-length sentence vectors.</p> <p>Pooling strategies can dramatically influence downstream performance:</p> <ul> <li>Concatenation preserves positional information up to a fixed length.</li> <li>Mean pooling offers a simple average representation.</li> <li>TF-IDF-weighted mean emphasizes more informative words.</li> </ul> <p>Why this matters: Understanding how pooling choices impact classification helps in selecting appropriate strategies for tasks like sentiment analysis, document retrieval, and beyond.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#1-environment-setup-imports","title":"1. \u2699\ufe0f Environment Setup &amp; Imports\u00b6","text":"<p>First, we install and import all the libraries we\u2019ll need:</p> <ul> <li>Data &amp; Modeling: <code>datasets</code>, <code>scikit-learn</code>, <code>numpy</code>, <code>pandas</code></li> <li>TF-IDF: <code>TfidfVectorizer</code></li> <li>FastText: <code>huggingface_hub</code></li> <li>ELMo: <code>tensorflow</code> and <code>tensorflow_hub</code></li> <li>Classifier: <code>LogisticRegression</code></li> </ul>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#2-loading-preprocessing-the-imdb-dataset","title":"2. \ud83d\uddc4\ufe0f Loading &amp; Preprocessing the IMDB Dataset\u00b6","text":"<p>We'll load the IMDB sentiment dataset from Hugging Face and prepare our train/test splits.</p> <ul> <li>The dataset has 25,000 labeled movie reviews for training and 25,000 for testing.</li> <li>Each example has a <code>text</code> field (the review) and a <code>label</code> (0 = negative, 1 = positive).</li> <li>We'll extract texts and labels into NumPy arrays for downstream feature extraction.</li> </ul>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#2bis-evaluation-utilities","title":"2bis \ud83d\udee0\ufe0f Evaluation Utilities\u00b6","text":"<p>Before we dive into TF-IDF vectorization and logistic regression, let's define a helper class to compute and plot our classification metrics (accuracy, precision, recall, F1) for each method we test.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#3-tf-idf-baseline","title":"3. TF-IDF Baseline\u00b6","text":"<p>In this section, we\u2019ll build our first extrinsic evaluation: train a Logistic Regression classifier on TF-IDF representations of IMDB movie reviews. This will serve as a strong baseline before we move on to embedding-based features.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#tf-idf-vectorization-model-training","title":"\ud83d\udd27 TF-IDF Vectorization &amp; Model Training\u00b6","text":"<p>We\u2019ll convert each review into a TF-IDF vector (unigrams + bigrams, capped at 50k features, min_df = 5, stop_words = 'english'), then train a Logistic Regression model.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#evaluate-on-test-set","title":"\ud83d\udcca Evaluate on Test Set\u00b6","text":"<p>Use our <code>Metrics</code> helper to compute and plot accuracy, precision, recall, and F1 for the TF-IDF baseline.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#quick-comment-on-tf-idf-baseline","title":"\u270f\ufe0f Quick Comment on TF-IDF Baseline\u00b6","text":"<p>Our TF-IDF + Logistic Regression baseline achieves 88.30% across accuracy, precision, recall and F1. This is a strong starting point\u2014showing that simple bag-of-ngrams still captures a lot of sentiment signal in IMDB reviews.</p> <p>Now, let\u2019s see how adding subword information via FastText embeddings changes our performance!</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#4-fasttext-embedding-features","title":"4. FastText Embedding Features\u00b6","text":"<p>In this section, we\u2019ll:</p> <ol> <li>Load the pre-trained FastText model from Hugging Face.</li> <li>Write helper functions to turn each review into:<ul> <li>Concatenation of all word vectors</li> <li>Mean of word vectors</li> <li>TF-IDF-weighted average of word vectors</li> </ul> </li> <li>Train a Logistic Regression classifier on each feature set.</li> <li>Compare results against our TF-IDF baseline.</li> </ol>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#from-word-embeddings-to-sentence-embeddings","title":"\ud83d\udd17 From Word Embeddings to Sentence Embeddings\u00b6","text":"<p>In our sentiment classification task, each review is a whole sentence (or paragraph), but FastText (and other embedding models) give us one vector per word. To feed these into a Logistic Regression classifier, we need a fixed-length representation for each review.</p> <p>We'll explore three common strategies to aggregate word vectors into a single sentence embedding:</p> <ol> <li>Concatenation \u2013 preserve word order (up to a limit)</li> <li>Mean pooling \u2013 simple average of all word vectors</li> <li>TF-IDF weighted average \u2013 emphasize important words by weighting</li> </ol>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#fasttext-sentence-embedding-strategies","title":"\ud83d\udce6 FastText Sentence Embedding Strategies\u00b6","text":"<p>Below are the helper functions we\u2019ll use. They each take a list of raw texts and our FastText model, and output a 2D NumPy array where each row is the sentence embedding for one review.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#41-concatenation-of-word-vectors","title":"4.1 \u2702\ufe0f Concatenation of Word Vectors\u00b6","text":"<ul> <li>What it does:<ol> <li>Split the review into words.</li> <li>Fetch each word's FastText vector, up to a fixed maximum length (<code>max_len</code>).</li> <li>Concatenate them in order, padding with zeros if the review is shorter than <code>max_len</code>.</li> </ol> </li> <li>Why: Keeps some sense of word order and fine-grained structure, at the cost of high dimensionality.</li> </ul>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#412-mean-pooling-of-word-vectors","title":"4.1.2 \u2696\ufe0f Mean Pooling of Word Vectors\u00b6","text":"<ul> <li>What it does:<ol> <li>Split the review into words.</li> <li>Fetch each word's vector.</li> <li>Compute the arithmetic mean across all vectors.</li> </ol> </li> <li>Why: Produces a compact, order-agnostic summary of the sentence. Often surprisingly effective despite its simplicity.</li> </ul>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#413-tf-idf-weighted-average-of-word-vectors","title":"4.1.3 \ud83d\udcdd TF-IDF Weighted Average of Word Vectors\u00b6","text":"<ul> <li>What it does:<ol> <li>Precompute TF-IDF scores for each word in our corpus.</li> <li>For each review, split into words and look up each word\u2019s TF-IDF weight.</li> <li>Compute a weighted average of the FastText vectors, using TF-IDF weights to upweight rare/informative words.</li> </ol> </li> <li>Why: Balances the simplicity of averaging with a way to emphasize words that carry more sentiment information.</li> </ul> <p>Next, we\u2019ll implement these functions in code, generate feature matrices for our train and test sets, and train a Logistic Regression on each to see how they compare!</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#building-fasttext-pipelines-training-evaluation","title":"\ud83c\udfd7\ufe0f Building FastText Pipelines, Training &amp; Evaluation\u00b6","text":"<p>Now that we have our three sentence\u2010embedding strategies for FastText, we\u2019ll:</p> <ol> <li>Generate features for train and test sets via each strategy</li> <li>Train a Logistic Regression on each feature set</li> <li>Evaluate and visualize results with our <code>Metrics</code> helper</li> </ol> <p>This will let us directly compare how concatenation, mean pooling, and TF-IDF\u2010weighted pooling fare on movie\u2010review sentiment.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#interpreting-the-fasttext-results","title":"\ud83e\uddd0 Interpreting the FastText Results\u00b6","text":"<p>Our FastText pipelines yielded the following F1 scores on the IMDB test set:</p> <ul> <li>FT_Concat: 69%</li> <li>FT_Mean: 79%</li> <li>FT_TFIDF: 80%</li> <li>TF-IDF Baseline: 88%</li> </ul> <p>Why do these static\u2010embedding strategies fall short of a pure TF-IDF model?</p> <ol> <li><p>Concatenation (69% F1) Concatenating every word vector up to a fixed length creates a very high\u2010dimensional feature space.</p> <ul> <li>Every review becomes one enormous vector whose layout depends on word order and padding.</li> <li>The model must learn to ignore \u201cempty\u201d (zero) slots and discover complex positional patterns.</li> <li>Signal is diluted by noise\u2014rare words, padding vectors, and varying lengths\u2014making it hard for a simple linear classifier to find a robust decision boundary.</li> </ul> </li> <li><p>Mean Pooling (79% F1) Taking the unweighted average of all word vectors collapses the entire review into one compact summary.</p> <ul> <li>This reduces dimensionality and aggregates information, which helps a lot compared to concatenation.</li> <li>However, every word (including stop-words like \u201cthe\u201d, \u201cand\u201d, \u201cof\u201d) contributes equally, so irrelevant tokens still dilute your signal.</li> <li>Word order and local contextual cues are lost\u2014\u201cnot good\u201d and \u201cgood not\u201d become identical.</li> </ul> </li> <li><p>TF-IDF-Weighted Pooling (80% F1) Weighting each word vector by its TF-IDF score improves slightly over the unweighted mean:</p> <ul> <li>High-informativeness words (e.g. \u201chorrible\u201d, \u201cmasterpiece\u201d) get more emphasis, while common words get down-weighted.</li> <li>Yet this still treats the review as a \u201cbag of weighted vectors\u201d with no sense of word adjacency or syntax.</li> <li>In practice, the marginal gain (&lt;1%) shows that simple linear weighting can only go so far in capturing sentence\u2010level nuance.</li> </ul> </li> </ol> <p>\ud83d\udd11 Key Takeaway: Static word embeddings\u2014whether concatenated, averaged, or TF-IDF-weighted\u2014cannot fully recover the rich structure, negation, and context that a bag-of-words TF-IDF representation already encodes very effectively. In the next section, we\u2019ll see how contextual models like ELMo can dynamically adapt to word order and usage, potentially closing this performance gap.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#5-contextual-embeddings-with-elmo","title":"5. \ud83c\udf00 Contextual Embeddings with ELMo\u00b6","text":"<p>ELMo (Embeddings from Language Models) generates contextualized word vectors: the same word has different representations depending on its sentence. This lets us capture negation, polysemy, and subtle syntactic cues that static methods miss.</p> <p>In this section we will:</p> <ol> <li>Load the pre-trained ELMo model from TensorFlow Hub</li> <li>Wrap it in a scikit-learn transformer that mean-pools over tokens</li> <li>Train &amp; evaluate a logistic regression on these sentence embeddings</li> </ol>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#example-using-elmotransformer-on-a-single-review","title":"\ud83e\uddea Example: Using <code>ELMoTransformer</code> on a Single Review\u00b6","text":"<p>Below we take one IMDb review, pass it through our <code>ELMoTransformer</code>, and inspect the resulting 1024-dimensional sentence embedding.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#building-the-elmo-pipeline-training-evaluation","title":"\ud83c\udfd7\ufe0f Building the ELMo Pipeline, Training &amp; Evaluation\u00b6","text":"<p>We\u2019ll now wire up our <code>ELMoTransformer</code> with a <code>LogisticRegression</code> and fit it on the IMDB training set, and evaluate on the test split using our <code>Metrics</code> helper.</p>"},{"location":"chapter3/Session_3_3_Embedding_Classification/#elmo-training-time-resources-note","title":"\u23f3 ELMo Training: Time &amp; Resources Note\u00b6","text":"<p>Training and fine-tuning models that leverage ELMo embeddings can be very time-consuming on a typical CPU-only setup\u2014often taking several hours (or more) per epoch on the full IMDb dataset. To run this end-to-end in a reasonable time, we highly recommend using a GPU-enabled environment or a cloud instance with a GPU.</p> <p>Tip: If you don\u2019t have access to a GPU right now, you can go on colab or feel free to skip the local training step and instead refer to Notebook #2, where we\u2019ve precomputed the ELMo sentence embeddings and already trained the logistic regression. You can inspect the classification results and interpret the performance without waiting for the full training run.</p>"},{"location":"chapter4/","title":"Session 4: Practical NLP - 1","text":""},{"location":"chapter4/#course-materials-practical-nlp-1","title":"\ud83c\udf93 Course Materials - Practical NLP - 1","text":""},{"location":"chapter4/#session-4-text-classification-pipelines-and-explainability","title":"Session 4: Text Classification Pipelines and Explainability","text":"<p>In this hands-on session, we walk through the evolution of text classification pipelines, from traditional approaches like TF-IDF + linear classifiers to modern deep learning models with LSTM and pretrained word embeddings like Word2Vec. The session closes with an introduction to model explainability using LIME, giving students insight into how models make decisions.</p> <p>This notebook is designed as a modular blueprint that can be reused and extended for many text classification tasks.</p>"},{"location":"chapter4/#notebooks","title":"\ud83d\udcd3 Notebooks","text":"<ul> <li>Session 4: From TF-IDF to LSTMs and Explainability</li> <li>Session 4: correction</li> </ul>"},{"location":"chapter4/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ol> <li>Understand how to turn raw text into machine-readable input (TF-IDF, tokenization, embeddings).</li> <li>Build baseline and deep models (logistic regression, BiLSTM).</li> <li>Integrate pre-trained embeddings (Word2Vec) into custom pipelines.</li> <li>Apply explainability tools like LIME to interpret model behavior.</li> <li>Compare models using quantitative and qualitative evaluation (metrics &amp; examples).</li> </ol>"},{"location":"chapter4/#bibliography-recommended-reading","title":"\ud83d\udcd6 Bibliography &amp; Recommended Reading","text":"<ul> <li>scikit-learn TF-IDF Documentation \u2013 Link</li> <li>spaCy Tokenizer Docs \u2013 Link</li> <li>Gensim Word2Vec Tutorial \u2013 Link</li> <li>LIME GitHub Repo \u2013 Link</li> </ul>"},{"location":"chapter4/#practical-components","title":"\ud83d\udcbb Practical Components","text":"<ul> <li>\ud83e\uddea Build a text classifier from scratch using <code>TF-IDF + LogisticRegression</code>.</li> <li>\ud83e\udde0 Train an LSTM with one-hot or pre-trained embeddings.</li> <li>\ud83d\udce6 Use Word2Vec embeddings from Hugging Face.</li> <li>\ud83d\udd0d Explain and debug predictions with LIME for real-world NLP workflows.</li> <li>\ud83c\udfaf Compare models using both metrics and example-level outputs.</li> </ul>"},{"location":"chapter4/Session_4/","title":"Advanced Methods in Natural Language Processing - Session 4","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# \u2705 CLASS TEMPLATE: Metric Comparison Across Models\n\n# Create a reusable Metrics class to:\n# - Store evaluation results (accuracy, precision, recall, F1)\n# - Compare multiple models side-by-side\n# - Plot the results using bar charts\n\n# \ud83d\udd27 Instructions:\n# 1. Define a class `Metrics` that holds a dictionary to store metrics for each method.\n# 2. Implement a `.run(y_true, y_pred, method_name)` method that:\n#    - Computes accuracy, precision, recall, and F1-score.\n#    - Stores them in the dictionary under the given method name.\n# 3. Implement a `.plot()` method that:\n#    - Creates a 2x2 grid of bar plots (one per metric).\n#    - Displays the comparison of all methods added via `.run()`.\n\n# \ud83d\udd0d Hint:\n# - Use `sklearn.metrics` functions like `accuracy_score`, `precision_score`, etc.\n# - Multiply values by 100 to show percentages.\n# - Use `plt.subplots()` for subplot creation.\n# - Add value annotations above each bar using `ax.text()`.\n\n# \ud83d\udca1 Once implemented, you can use it like this:\n# metrics = Metrics()\n# metrics.run(y_true, y_pred_baseline, \"Baseline\")\n# metrics.run(y_true, y_pred_lstm, \"LSTM\")\n# metrics.plot()\n\n\nclass Metrics:\n    def __init__(self):\n        \n\n    def run(self, y_true, y_pred, method_name, average='macro'):\n        \n\n    def plot(self):\n        \n</pre> import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # \u2705 CLASS TEMPLATE: Metric Comparison Across Models  # Create a reusable Metrics class to: # - Store evaluation results (accuracy, precision, recall, F1) # - Compare multiple models side-by-side # - Plot the results using bar charts  # \ud83d\udd27 Instructions: # 1. Define a class `Metrics` that holds a dictionary to store metrics for each method. # 2. Implement a `.run(y_true, y_pred, method_name)` method that: #    - Computes accuracy, precision, recall, and F1-score. #    - Stores them in the dictionary under the given method name. # 3. Implement a `.plot()` method that: #    - Creates a 2x2 grid of bar plots (one per metric). #    - Displays the comparison of all methods added via `.run()`.  # \ud83d\udd0d Hint: # - Use `sklearn.metrics` functions like `accuracy_score`, `precision_score`, etc. # - Multiply values by 100 to show percentages. # - Use `plt.subplots()` for subplot creation. # - Add value annotations above each bar using `ax.text()`.  # \ud83d\udca1 Once implemented, you can use it like this: # metrics = Metrics() # metrics.run(y_true, y_pred_baseline, \"Baseline\") # metrics.run(y_true, y_pred_lstm, \"LSTM\") # metrics.plot()   class Metrics:     def __init__(self):               def run(self, y_true, y_pred, method_name, average='macro'):               def plot(self):          In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\n# Load the 'ag_news' dataset\ndataset = load_dataset(\"ag_news\")\n\n# Explore the structure of the dataset\nprint(dataset)\n</pre> from datasets import load_dataset  # Load the 'ag_news' dataset dataset = load_dataset(\"ag_news\")  # Explore the structure of the dataset print(dataset) <p>Let's create stratified samples for training and validation sets ensuring that each class is represented in proportion to its frequency. It will go faster with just a sample, and we will be able to make tests on validation test before trying to work on the testing set.</p> In\u00a0[3]: Copied! <pre>from sklearn.model_selection import train_test_split\n\ndata = dataset['train']['text']\nlabels = dataset['train']['label']\n\ntest_data = dataset['test']['text']\ntest_labels = dataset['test']['label']\n\n# Stratified split to create a smaller training and validation set\ntrain_data, valid_data, train_labels, valid_labels = train_test_split(\n    data, labels, stratify=labels, test_size=0.2, random_state=42\n)\n\n# Further split to get 10k and 2k samples respectively\ntrain_data, _, train_labels, _ = train_test_split(\n    train_data, train_labels, stratify=train_labels, train_size=10000, random_state=42\n)\nvalid_data, _, valid_labels, _ = train_test_split(\n    valid_data, valid_labels, stratify=valid_labels, train_size=2000, random_state=42\n)\n</pre> from sklearn.model_selection import train_test_split  data = dataset['train']['text'] labels = dataset['train']['label']  test_data = dataset['test']['text'] test_labels = dataset['test']['label']  # Stratified split to create a smaller training and validation set train_data, valid_data, train_labels, valid_labels = train_test_split(     data, labels, stratify=labels, test_size=0.2, random_state=42 )  # Further split to get 10k and 2k samples respectively train_data, _, train_labels, _ = train_test_split(     train_data, train_labels, stratify=train_labels, train_size=10000, random_state=42 ) valid_data, _, valid_labels, _ = train_test_split(     valid_data, valid_labels, stratify=valid_labels, train_size=2000, random_state=42 ) In\u00a0[\u00a0]: Copied! <pre>from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport nltk\nfrom nltk.corpus import stopwords\n\n# \u2705 TASK: Visualize Word Distributions Per Class with Word Clouds\n\n# Goal:\n# - Create word clouds for each class label to visually inspect frequent terms.\n# - This helps you understand which words dominate each topic (e.g., sports, business, etc.)\n\n# \ud83e\uddf0 Required Libraries:\n# - `wordcloud.WordCloud` for generating word clouds\n# - `nltk.corpus.stopwords` for filtering out common English words\n# - `matplotlib.pyplot` for plotting\n# - `collections.defaultdict` for grouping text by label\n\n# \ud83e\ude9c Instructions:\n\n# 1. Import the required libraries.\n# 2. Download and prepare the list of English stopwords using NLTK.\n# 3. Create a dictionary `label_data` (e.g., with `defaultdict`) to accumulate all text per class label.\n# 4. Set up a 2x2 subplot layout using `plt.subplots` to visualize 4 categories.\n</pre> from wordcloud import WordCloud import matplotlib.pyplot as plt from collections import defaultdict import nltk from nltk.corpus import stopwords  # \u2705 TASK: Visualize Word Distributions Per Class with Word Clouds  # Goal: # - Create word clouds for each class label to visually inspect frequent terms. # - This helps you understand which words dominate each topic (e.g., sports, business, etc.)  # \ud83e\uddf0 Required Libraries: # - `wordcloud.WordCloud` for generating word clouds # - `nltk.corpus.stopwords` for filtering out common English words # - `matplotlib.pyplot` for plotting # - `collections.defaultdict` for grouping text by label  # \ud83e\ude9c Instructions:  # 1. Import the required libraries. # 2. Download and prepare the list of English stopwords using NLTK. # 3. Create a dictionary `label_data` (e.g., with `defaultdict`) to accumulate all text per class label. # 4. Set up a 2x2 subplot layout using `plt.subplots` to visualize 4 categories. In\u00a0[\u00a0]: Copied! <pre>from collections import Counter\nimport matplotlib.pyplot as plt\n\n# \u2705 TASK: Visualize Class Distribution with a Pie Chart\n\n# Goal:\n# - Create a pie chart to show the proportion of each class in the training dataset.\n# - This helps you detect class imbalance, which is important for model training and evaluation.\n\n# \ud83e\uddf0 Required Libraries:\n# - `collections.Counter` to count how many examples exist per label.\n# - `matplotlib.pyplot` to create the pie chart.\n\n# \ud83e\ude9c Instructions:\n\n# 1. Use `Counter` to count how many times each label appears in `train_labels`.\n# 2. Convert label indices to label names (e.g., 'World', 'Business') using the `labels` dictionary.\n# 3. Prepare data for plotting\n# 4. Use `plt.pie()` to draw the pie chart.\n# 5. Call `plt.axis('equal')` to make sure the pie chart is circular.\n# 6. Add a title and display the chart with `plt.show()`.\n\n# \ud83c\udfaf Output: A pie chart showing the proportion of news categories in your dataset.\n</pre> from collections import Counter import matplotlib.pyplot as plt  # \u2705 TASK: Visualize Class Distribution with a Pie Chart  # Goal: # - Create a pie chart to show the proportion of each class in the training dataset. # - This helps you detect class imbalance, which is important for model training and evaluation.  # \ud83e\uddf0 Required Libraries: # - `collections.Counter` to count how many examples exist per label. # - `matplotlib.pyplot` to create the pie chart.  # \ud83e\ude9c Instructions:  # 1. Use `Counter` to count how many times each label appears in `train_labels`. # 2. Convert label indices to label names (e.g., 'World', 'Business') using the `labels` dictionary. # 3. Prepare data for plotting # 4. Use `plt.pie()` to draw the pie chart. # 5. Call `plt.axis('equal')` to make sure the pie chart is circular. # 6. Add a title and display the chart with `plt.show()`.  # \ud83c\udfaf Output: A pie chart showing the proportion of news categories in your dataset.  In\u00a0[6]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n# \u2705 TASK: Build a Baseline Text Classification Pipeline\n\n# Goal:\n# - Create a text classification pipeline that transforms text data into TF-IDF features\n#   and trains a Logistic Regression model on them.\n\n# \ud83e\uddf0 Required Tools:\n# - `TfidfVectorizer` to convert raw text into numerical features.\n# - `LogisticRegression` as a simple yet effective classifier for baseline comparison.\n# - `Pipeline` to combine preprocessing and model training into a single object.\n\n# \ud83e\ude9c Instructions:\n\n# 1. Create a `Pipeline` \n# 2. Fit the pipeline on your training data\n# 3. Make predictions on the validation set using `.predict(valid_data)`.\n\n# \ud83c\udfaf Output:\n# - A trained model that you can use to make predictions and evaluate performance.\n# - Predicted labels for the validation set stored in `valid_preds`.\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split  # \u2705 TASK: Build a Baseline Text Classification Pipeline  # Goal: # - Create a text classification pipeline that transforms text data into TF-IDF features #   and trains a Logistic Regression model on them.  # \ud83e\uddf0 Required Tools: # - `TfidfVectorizer` to convert raw text into numerical features. # - `LogisticRegression` as a simple yet effective classifier for baseline comparison. # - `Pipeline` to combine preprocessing and model training into a single object.  # \ud83e\ude9c Instructions:  # 1. Create a `Pipeline`  # 2. Fit the pipeline on your training data # 3. Make predictions on the validation set using `.predict(valid_data)`.  # \ud83c\udfaf Output: # - A trained model that you can use to make predictions and evaluate performance. # - Predicted labels for the validation set stored in `valid_preds`.  In\u00a0[7]: Copied! <pre>metrics_val= Metrics()\nmetrics_val.run(valid_labels, valid_preds, \"basic TF-IDF\")\n</pre> metrics_val= Metrics() metrics_val.run(valid_labels, valid_preds, \"basic TF-IDF\") In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n\n# \u2705 TASK: Tune Your TF-IDF + Logistic Regression Pipeline using Grid Search\n\n# Goal:\n# - Use `GridSearchCV` to find the best combination of hyperparameters for the pipeline.\n# - Improve performance by systematically testing different settings for `TfidfVectorizer`.\n\n# \ud83e\uddf0 Required Tools:\n# - `GridSearchCV`: for systematic search over hyperparameter space with cross-validation.\n# - `param_grid`: dictionary defining which parameters to tune and their candidate values.\n\n# \ud83e\ude9c Instructions:\n\n# 1. Define the parameter grid\n# 2. Create a `GridSearchCV` object\n# 3. Fit the grid search on the training set\n</pre> from sklearn.model_selection import GridSearchCV  # \u2705 TASK: Tune Your TF-IDF + Logistic Regression Pipeline using Grid Search  # Goal: # - Use `GridSearchCV` to find the best combination of hyperparameters for the pipeline. # - Improve performance by systematically testing different settings for `TfidfVectorizer`.  # \ud83e\uddf0 Required Tools: # - `GridSearchCV`: for systematic search over hyperparameter space with cross-validation. # - `param_grid`: dictionary defining which parameters to tune and their candidate values.  # \ud83e\ude9c Instructions:  # 1. Define the parameter grid # 2. Create a `GridSearchCV` object # 3. Fit the grid search on the training set In\u00a0[\u00a0]: Copied! <pre>metrics_val.plot()\n</pre> metrics_val.plot() In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.preprocessing.text import Tokenizer\n\n# Define vocabulary size (maximum number of words to keep)\nvocab_size = 5000  # \ud83d\udd27 Hyperparameter: Can be increased or decreased based on your dataset size\n\n# Initialize the tokenizer\n\n\n# Fit the tokenizer on the training text data\n</pre> from tensorflow.keras.preprocessing.text import Tokenizer  # Define vocabulary size (maximum number of words to keep) vocab_size = 5000  # \ud83d\udd27 Hyperparameter: Can be increased or decreased based on your dataset size  # Initialize the tokenizer   # Fit the tokenizer on the training text data  In\u00a0[\u00a0]: Copied! <pre># Convert training and validation texts to sequences\nsequences_train = \nsequences_valid = \n</pre> # Convert training and validation texts to sequences sequences_train =  sequences_valid =  In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Set maximum input length\nmax_length = 128\n\n# Pad or truncate the sequences to fixed length\npadded_sequences_train = \npadded_sequences_valid = \n</pre> from tensorflow.keras.preprocessing.sequence import pad_sequences  # Set maximum input length max_length = 128  # Pad or truncate the sequences to fixed length padded_sequences_train =  padded_sequences_valid =  In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.utils import to_categorical\n\n# Determine number of unique output classes\nnum_classes = len(set(train_labels))\n\n# One-hot encode the labels\ntrain_labels_lstm = \nvalid_labels_lstm = \n</pre> from tensorflow.keras.utils import to_categorical  # Determine number of unique output classes num_classes = len(set(train_labels))  # One-hot encode the labels train_labels_lstm =  valid_labels_lstm =   In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n</pre> from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense In\u00a0[\u00a0]: Copied! <pre>model = Sequential([\n    Embedding\n    Bidirectional\n    Dense\n])\n</pre> model = Sequential([     Embedding     Bidirectional     Dense ])  In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.metrics import Precision, Recall\n\nmodel.compile(\n    optimizer=\n    loss=\n    metrics=\n)\n</pre> from tensorflow.keras.metrics import Precision, Recall  model.compile(     optimizer=     loss=     metrics= )  In\u00a0[\u00a0]: Copied! <pre>model.summary()\n</pre> model.summary() In\u00a0[\u00a0]: Copied! <pre>history = \n</pre> history =  In\u00a0[\u00a0]: Copied! <pre>predictions = model.predict(padded_sequences_valid)\nvalid_preds = np.argmax(predictions, axis=1)\nmetrics_val.run(valid_labels, valid_preds, \"BiLSTM\")\n</pre> predictions = model.predict(padded_sequences_valid) valid_preds = np.argmax(predictions, axis=1) metrics_val.run(valid_labels, valid_preds, \"BiLSTM\") In\u00a0[\u00a0]: Copied! <pre>metrics_val.plot()\n</pre> metrics_val.plot() In\u00a0[15]: Copied! <pre>from staticvectors import StaticVectors\n\nword2vec_model = StaticVectors(\"neuml/word2vec\")\n</pre> from staticvectors import StaticVectors  word2vec_model = StaticVectors(\"neuml/word2vec\") In\u00a0[\u00a0]: Copied! <pre>embedding_matrix = \n</pre> embedding_matrix =  In\u00a0[\u00a0]: Copied! <pre>for word, i in tqdm(tokenizer.word_index.items()):\n</pre> for word, i in tqdm(tokenizer.word_index.items()): In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom tqdm import tqdm\n\nmodel = Sequential([\n    Embedding\n    Bidirectional\n    Dense\n])\n\nmodel.compile\nmodel.summary()\n</pre> import numpy as np from tqdm import tqdm  model = Sequential([     Embedding     Bidirectional     Dense ])  model.compile model.summary() In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.callbacks import EarlyStopping\n\n# Setup early stopping to stop training when validation loss stops improving\nearly_stopping = EarlyStopping(\n    \n)\n\n\nhistory = model.fit(\n    padded_sequences_train,\n    train_labels_lstm,\n    epochs=10,\n    batch_size=128,\n    validation_data=(padded_sequences_valid, valid_labels_lstm),\n    callbacks=[early_stopping]\n)\n</pre> from tensorflow.keras.callbacks import EarlyStopping  # Setup early stopping to stop training when validation loss stops improving early_stopping = EarlyStopping(      )   history = model.fit(     padded_sequences_train,     train_labels_lstm,     epochs=10,     batch_size=128,     validation_data=(padded_sequences_valid, valid_labels_lstm),     callbacks=[early_stopping] )  In\u00a0[\u00a0]: Copied! <pre>predictions = model.predict(padded_sequences_valid)\nvalid_preds = np.argmax(predictions, axis=1)\nmetrics_val.run(valid_labels, valid_preds, \"BiLSTM + W2V\")\nmetrics_val.plot()\n</pre> predictions = model.predict(padded_sequences_valid) valid_preds = np.argmax(predictions, axis=1) metrics_val.run(valid_labels, valid_preds, \"BiLSTM + W2V\") metrics_val.plot() In\u00a0[\u00a0]: Copied! <pre>from lime.lime_text import LimeTextExplainer\nimport shap\n\n# Fixed version of the TF-IDF LIME explainer function\ndef explain_tfidf_prediction(text_instance, pipeline, class_names):\n    # Create a LIME text explainer\n    explainer = LimeTextExplainer(class_names=class_names)\n    \n    # Get explanation for the prediction\n    exp = explainer.explain_instance(\n        text_instance, \n        pipeline.predict_proba, \n        num_features=10,\n        top_labels=len(class_names)  # Explain all classes\n    )\n    \n    # Display basic information\n    print(f\"Text: {text_instance}\")\n    pred_class = pipeline.predict([text_instance])[0]\n    print(f\"Predicted class: {class_names[pred_class]}\")\n    \n    # Get probabilities for all classes\n    probs = pipeline.predict_proba([text_instance])[0]\n    print(\"\\nClass probabilities:\")\n    for i, class_name in enumerate(class_names):\n        print(f\"{class_name}: {probs[i]:.4f}\")\n    \n    # Create visualization for each class\n    plt.figure(figsize=(20, 15))\n    \n    # Get the labels that LIME actually explained\n    top_labels = exp.available_labels()\n    \n    for i, label_id in enumerate(top_labels):\n        plt.subplot(2, 2, i+1)\n        \n        # Get the explanation for this class\n        exp_list = exp.as_list(label=label_id)\n        \n        # Extract words and weights\n        words = [x[0] for x in exp_list]\n        weights = [x[1] for x in exp_list]\n        \n        # Sort for better visualization\n        pairs = sorted(zip(words, weights), key=lambda x: x[1])\n        words = [x[0] for x in pairs]\n        weights = [x[1] for x in pairs]\n        \n        # Create bar chart\n        colors = ['red' if w &lt; 0 else 'green' for w in weights]\n        y_pos = np.arange(len(words))\n        \n        plt.barh(y_pos, weights, color=colors)\n        plt.yticks(y_pos, words)\n        plt.title(f\"Explanation for class: {class_names[label_id]}\")\n        plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print top contributing words for each class\n    for label_id in top_labels:\n        print(f\"\\nTop features for class: {class_names[label_id]}\")\n        exp_list = exp.as_list(label=label_id)\n        for word, weight in exp_list:\n            print(f\"{word}: {weight:.4f}\")\n    \n    return exp\n\n# Example text from test set\nexample_text = test_data[0]\nclass_names = ['World', 'Sports', 'Business', 'Sci/Tech']\nlime_exp_tfidf = explain_tfidf_prediction(example_text, grid_search.best_estimator_, class_names)\n</pre> from lime.lime_text import LimeTextExplainer import shap  # Fixed version of the TF-IDF LIME explainer function def explain_tfidf_prediction(text_instance, pipeline, class_names):     # Create a LIME text explainer     explainer = LimeTextExplainer(class_names=class_names)          # Get explanation for the prediction     exp = explainer.explain_instance(         text_instance,          pipeline.predict_proba,          num_features=10,         top_labels=len(class_names)  # Explain all classes     )          # Display basic information     print(f\"Text: {text_instance}\")     pred_class = pipeline.predict([text_instance])[0]     print(f\"Predicted class: {class_names[pred_class]}\")          # Get probabilities for all classes     probs = pipeline.predict_proba([text_instance])[0]     print(\"\\nClass probabilities:\")     for i, class_name in enumerate(class_names):         print(f\"{class_name}: {probs[i]:.4f}\")          # Create visualization for each class     plt.figure(figsize=(20, 15))          # Get the labels that LIME actually explained     top_labels = exp.available_labels()          for i, label_id in enumerate(top_labels):         plt.subplot(2, 2, i+1)                  # Get the explanation for this class         exp_list = exp.as_list(label=label_id)                  # Extract words and weights         words = [x[0] for x in exp_list]         weights = [x[1] for x in exp_list]                  # Sort for better visualization         pairs = sorted(zip(words, weights), key=lambda x: x[1])         words = [x[0] for x in pairs]         weights = [x[1] for x in pairs]                  # Create bar chart         colors = ['red' if w &lt; 0 else 'green' for w in weights]         y_pos = np.arange(len(words))                  plt.barh(y_pos, weights, color=colors)         plt.yticks(y_pos, words)         plt.title(f\"Explanation for class: {class_names[label_id]}\")         plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)          plt.tight_layout()     plt.show()          # Print top contributing words for each class     for label_id in top_labels:         print(f\"\\nTop features for class: {class_names[label_id]}\")         exp_list = exp.as_list(label=label_id)         for word, weight in exp_list:             print(f\"{word}: {weight:.4f}\")          return exp  # Example text from test set example_text = test_data[0] class_names = ['World', 'Sports', 'Business', 'Sci/Tech'] lime_exp_tfidf = explain_tfidf_prediction(example_text, grid_search.best_estimator_, class_names) In\u00a0[\u00a0]: Copied! <pre>def prepare_text_for_lstm(text, tokenizer, max_length):\n    \"\"\"Prepare text input for LSTM model\"\"\"\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    sequences = tokenizer.texts_to_sequences([text])\n    padded_seq = pad_sequences(sequences, maxlen=max_length)\n    return padded_seq\n\ndef lstm_predict_proba(texts):\n    \"\"\"Prediction function for LIME to use with LSTM model\"\"\"\n    result = np.zeros((len(texts), len(class_names)))\n    for i, text in enumerate(texts):\n        padded = prepare_text_for_lstm(text, tokenizer, max_length)\n        pred = model.predict(padded, verbose=0)\n        result[i] = pred[0]\n    return result\n\ndef explain_lstm_prediction(text_instance, class_names):\n    # Create a LIME text explainer\n    explainer = LimeTextExplainer(class_names=class_names)\n    \n    # Get explanation for the prediction\n    exp = explainer.explain_instance(\n        text_instance, \n        lstm_predict_proba, \n        num_features=10,\n        top_labels=len(class_names)  # Explain all classes\n    )\n    \n    # Display basic information\n    print(f\"Text: {text_instance}\")\n    padded = prepare_text_for_lstm(text_instance, tokenizer, max_length)\n    prediction = model.predict(padded, verbose=0)\n    predicted_class = np.argmax(prediction[0])\n    print(f\"Predicted class: {class_names[predicted_class]}\")\n    \n    # Get probabilities for all classes\n    probs = prediction[0]\n    print(\"\\nClass probabilities:\")\n    for i, class_name in enumerate(class_names):\n        print(f\"{class_name}: {probs[i]:.4f}\")\n    \n    # Create visualization for each class\n    plt.figure(figsize=(20, 15))\n    \n    # Get the labels that LIME actually explained\n    top_labels = exp.available_labels()\n    \n    for i, label_id in enumerate(top_labels):\n        plt.subplot(2, 2, i+1)\n        \n        # Get the explanation for this class\n        exp_list = exp.as_list(label=label_id)\n        \n        # Extract words and weights\n        words = [x[0] for x in exp_list]\n        weights = [x[1] for x in exp_list]\n        \n        # Sort for better visualization\n        pairs = sorted(zip(words, weights), key=lambda x: x[1])\n        words = [x[0] for x in pairs]\n        weights = [x[1] for x in pairs]\n        \n        # Create bar chart\n        colors = ['red' if w &lt; 0 else 'green' for w in weights]\n        y_pos = np.arange(len(words))\n        \n        plt.barh(y_pos, weights, color=colors)\n        plt.yticks(y_pos, words)\n        plt.title(f\"Explanation for class: {class_names[label_id]}\")\n        plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print top contributing words for each class\n    for label_id in top_labels:\n        print(f\"\\nTop features for class: {class_names[label_id]}\")\n        exp_list = exp.as_list(label=label_id)\n        for word, weight in exp_list:\n            print(f\"{word}: {weight:.4f}\")\n    \n    return exp\n\nexample_text = test_data[0]\nmodel_exp_lime = explain_lstm_prediction(example_text, class_names)\n</pre> def prepare_text_for_lstm(text, tokenizer, max_length):     \"\"\"Prepare text input for LSTM model\"\"\"     from tensorflow.keras.preprocessing.sequence import pad_sequences     sequences = tokenizer.texts_to_sequences([text])     padded_seq = pad_sequences(sequences, maxlen=max_length)     return padded_seq  def lstm_predict_proba(texts):     \"\"\"Prediction function for LIME to use with LSTM model\"\"\"     result = np.zeros((len(texts), len(class_names)))     for i, text in enumerate(texts):         padded = prepare_text_for_lstm(text, tokenizer, max_length)         pred = model.predict(padded, verbose=0)         result[i] = pred[0]     return result  def explain_lstm_prediction(text_instance, class_names):     # Create a LIME text explainer     explainer = LimeTextExplainer(class_names=class_names)          # Get explanation for the prediction     exp = explainer.explain_instance(         text_instance,          lstm_predict_proba,          num_features=10,         top_labels=len(class_names)  # Explain all classes     )          # Display basic information     print(f\"Text: {text_instance}\")     padded = prepare_text_for_lstm(text_instance, tokenizer, max_length)     prediction = model.predict(padded, verbose=0)     predicted_class = np.argmax(prediction[0])     print(f\"Predicted class: {class_names[predicted_class]}\")          # Get probabilities for all classes     probs = prediction[0]     print(\"\\nClass probabilities:\")     for i, class_name in enumerate(class_names):         print(f\"{class_name}: {probs[i]:.4f}\")          # Create visualization for each class     plt.figure(figsize=(20, 15))          # Get the labels that LIME actually explained     top_labels = exp.available_labels()          for i, label_id in enumerate(top_labels):         plt.subplot(2, 2, i+1)                  # Get the explanation for this class         exp_list = exp.as_list(label=label_id)                  # Extract words and weights         words = [x[0] for x in exp_list]         weights = [x[1] for x in exp_list]                  # Sort for better visualization         pairs = sorted(zip(words, weights), key=lambda x: x[1])         words = [x[0] for x in pairs]         weights = [x[1] for x in pairs]                  # Create bar chart         colors = ['red' if w &lt; 0 else 'green' for w in weights]         y_pos = np.arange(len(words))                  plt.barh(y_pos, weights, color=colors)         plt.yticks(y_pos, words)         plt.title(f\"Explanation for class: {class_names[label_id]}\")         plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)          plt.tight_layout()     plt.show()          # Print top contributing words for each class     for label_id in top_labels:         print(f\"\\nTop features for class: {class_names[label_id]}\")         exp_list = exp.as_list(label=label_id)         for word, weight in exp_list:             print(f\"{word}: {weight:.4f}\")          return exp  example_text = test_data[0] model_exp_lime = explain_lstm_prediction(example_text, class_names) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter4/Session_4/#advanced-methods-in-natural-language-processing-session-4","title":"Advanced Methods in Natural Language Processing - Session 4\u00b6","text":""},{"location":"chapter4/Session_4/#text-classification-with-ag-news-corpus","title":"Text Classification with AG News Corpus\u00b6","text":"<p>This notebook will guide you through different approaches to text classification using the AG News corpus. We will start with a simple baseline model and gradually move towards more complex and sophisticated models.</p>"},{"location":"chapter4/Session_4/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li><p>Part 1: Baseline Pipeline with TF-IDF and Linear Model</p> <ul> <li>1.1. Loading and Exploring Data</li> <li>1.2. Feature Extraction with TF-IDF</li> <li>1.3. Training a Linear Model</li> <li>1.4. Model Evaluation</li> </ul> </li> <li><p>Part 2: LSTM Pipeline with One-Hot Encoding</p> <ul> <li>2.1. Preprocessing for LSTM</li> <li>2.2. Building a Bidirectional LSTM Model</li> <li>2.3. Training the LSTM Model</li> <li>2.4. Model Evaluation</li> </ul> </li> <li><p>Part 3: Word Embedding Add-Ons with Word2Vec</p> <ul> <li>3.1. Loading Pre-trained Word2Vec Embeddings</li> <li>3.2. Integrating Word2Vec into LSTM Model</li> <li>3.3. Training and Evaluating the Model</li> </ul> </li> <li><p>Part 4: Model Explainability (LIME / SHAP)</p> <ul> <li>4.1. Why Explainability Matters</li> <li>4.2. Applying LIME to the TF-IDF Model</li> <li>4.3. Comparing Explanation for LSTM with Word2Vec model</li> </ul> </li> </ol>"},{"location":"chapter4/Session_4/#part-0-metrics-functions-to-consider","title":"Part 0: Metrics Functions to Consider\u00b6","text":"<p>Before diving into the model building and training, it's crucial to establish the metrics we'll use to evaluate our models. In this part, we will define and discuss the different metrics functions that are commonly used in NLP tasks, particularly for text classification:</p> <ol> <li><p>Accuracy: Measures the proportion of correct predictions among the total number of cases examined. It's a straightforward metric but can be misleading if the classes are imbalanced.</p> </li> <li><p>Precision and Recall: Precision measures the proportion of positive identifications that were actually correct, while recall measures the proportion of actual positives that were identified correctly. These metrics are especially important when dealing with imbalanced datasets.</p> </li> <li><p>F1 Score: The harmonic mean of precision and recall. It's a good way to show that a classifer has a good balance between precision and recall.</p> </li> <li><p>Confusion Matrix: A table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.</p> </li> <li><p>ROC and AUC: The receiver operating characteristic curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system. The area under the curve (AUC) represents measure of separability.</p> </li> </ol> <p>We will implement these metrics functions using libraries such as scikit-learn, and they will be used to assess and compare the performance of our different models throughout this exercise.</p>"},{"location":"chapter4/Session_4/#part-1-baseline-pipeline-with-tf-idf-and-linear-model","title":"Part 1: Baseline Pipeline with TF-IDF and Linear Model\u00b6","text":"<p>In this part, we will create a baseline model for text classification. This involves:</p>"},{"location":"chapter4/Session_4/#1-loading-and-exploring-data","title":"1. Loading and Exploring Data:\u00b6","text":"<p>We will load the AG News corpus and perform necessary preprocessing steps like exploring the dataset.</p>"},{"location":"chapter4/Session_4/#2-feature-extraction-with-tf-idf","title":"2. Feature Extraction with TF-IDF:\u00b6","text":"<p>We will convert the text data into numerical form using the TF-IDF vectorization technique. We will use the <code>Pipeline</code> class from scikit-learn which is really practical.</p>"},{"location":"chapter4/Session_4/#3-training-with-cross-validation","title":"3. Training with Cross Validation:\u00b6","text":"<p>We will train a linear classifier (such as Logistic Regression) using the extracted features, <code>Pipeline</code> module and cross validation with <code>GridSearchCV</code>.</p>"},{"location":"chapter4/Session_4/#4-model-evaluation","title":"4. Model Evaluation:\u00b6","text":"<p>We will evaluate the performance of our model on a separate test set using various metrics.</p>"},{"location":"chapter4/Session_4/#part-2-lstm-pipeline-with-one-hot-encoding","title":"Part 2: LSTM Pipeline with One-Hot Encoding\u00b6","text":"<p>In this part, we'll explore a more complex model using LSTM:</p>"},{"location":"chapter4/Session_4/#1-preprocessing-for-lstm","title":"1. Preprocessing for LSTM:\u00b6","text":"<p>We'll prepare the text data for LSTM, which involves tokenization and converting words to one-hot encoded vectors.</p>"},{"location":"chapter4/Session_4/#tokenization-vocabulary","title":"\ud83d\udd20 Tokenization &amp; Vocabulary\u00b6","text":"<p>We use Keras's <code>Tokenizer</code> to convert raw text into sequences of integers.</p> <ul> <li><code>num_words=5000</code> limits the vocabulary to the most frequent 5000 words.</li> <li>The tokenizer is fit only on the training data to avoid data leakage.</li> </ul>"},{"location":"chapter4/Session_4/#text-to-sequence","title":"\ud83e\uddee Text to Sequence\u00b6","text":"<p>Now that the tokenizer has learned the vocabulary:</p> <ul> <li>It transforms each sentence into a list of word indices.</li> <li>Words not in the top <code>vocab_size</code> are ignored.</li> </ul> <p>Example: <code>\"the cat sat\"</code> \u2192 <code>[1, 45, 213]</code></p>"},{"location":"chapter4/Session_4/#padding-sequences","title":"\ud83d\udcd0 Padding Sequences\u00b6","text":"<p>Neural networks require fixed-length input, so we:</p> <ul> <li>Pad shorter sequences with zeros.</li> <li>Truncate longer ones to the <code>max_length</code>.</li> </ul> <p><code>padding='post'</code> adds padding after the sequence (e.g., <code>[45, 213, 0, 0, 0]</code>).</p>"},{"location":"chapter4/Session_4/#one-hot-encode-labels","title":"\ud83c\udfaf One-Hot Encode Labels\u00b6","text":"<p>For multi-class classification with neural networks, we need to:</p> <ul> <li>Convert integer class labels to one-hot encoded vectors.<ul> <li>Class <code>2</code> in 4-class problem \u2192 <code>[0, 0, 1, 0]</code></li> </ul> </li> <li>This format is required by the softmax output layer in our model.</li> </ul>"},{"location":"chapter4/Session_4/#2-building-a-bidirectional-lstm-model","title":"2. Building a Bidirectional LSTM Model:\u00b6","text":"<p>We'll design a neural network with a Bidirectional LSTM layer to capture context from both directions in the text.</p>"},{"location":"chapter4/Session_4/#building-the-neural-network","title":"\ud83e\uddf1 Building the Neural Network\u00b6","text":"<p>We use Keras's <code>Sequential</code> model for simplicity. We'll define our LSTM-based architecture step-by-step next.</p>"},{"location":"chapter4/Session_4/#model-layers","title":"\ud83e\udde9 Model Layers\u00b6","text":"<ul> <li><code>Embedding</code>: Transforms integer word indices into dense vectors.</li> <li><code>Bidirectional LSTM</code>: Processes the sequence both forward and backward for richer context.</li> <li><code>Dense</code>: Final layer with softmax activation for multi-class classification.</li> </ul> <p>Each sentence becomes a sequence of vectors, processed to output a probability over each class.</p>"},{"location":"chapter4/Session_4/#model-compilation","title":"\u2699\ufe0f Model Compilation\u00b6","text":"<p>We compile the model with:</p> <ul> <li><code>adam</code> optimizer (adaptive learning rate)</li> <li><code>categorical_crossentropy</code> for multi-class output</li> <li>Additional metrics: <code>accuracy</code>, <code>precision</code>, and <code>recall</code> for deeper evaluation</li> </ul>"},{"location":"chapter4/Session_4/#model-summary","title":"\ud83e\uddfe Model Summary\u00b6","text":"<p>Let\u2019s take a quick look at the number of parameters, layer types, and output shapes in our model.</p> <p>\u2705 This helps validate that the model is structured as intended.</p>"},{"location":"chapter4/Session_4/#3-training-the-lstm-model","title":"3. Training the LSTM Model:\u00b6","text":"<p>We'll train our LSTM model on the preprocessed text data.</p>"},{"location":"chapter4/Session_4/#4-model-evaluation","title":"4. Model Evaluation:\u00b6","text":"<p>Similar to Part 1, we will evaluate our model's performance using appropriate metrics.</p>"},{"location":"chapter4/Session_4/#part-3-word-embedding-add-ons-with-word2vec","title":"Part 3: Word Embedding Add-Ons with Word2Vec\u00b6","text":"<p>This part focuses on integrating pre-trained word embeddings into our model.</p>"},{"location":"chapter4/Session_4/#1-loading-pre-trained-word2vec-embeddings","title":"1. Loading Pre-trained Word2Vec Embeddings:\u00b6","text":"<p>We'll load Word2Vec embeddings pre-trained on a large corpus.</p>"},{"location":"chapter4/Session_4/#2-integrating-word2vec-into-lstm-model","title":"2. Integrating Word2Vec into LSTM Model:\u00b6","text":"<p>We'll use these embeddings as inputs to our LSTM model, potentially enhancing its ability to understand context and semantics.</p>"},{"location":"chapter4/Session_4/#preparing-the-word2vec-embedding-matrix","title":"\ud83d\udce6 Preparing the Word2Vec Embedding Matrix\u00b6","text":"<p>We will now create an embedding matrix that maps each word in our tokenizer vocabulary to its pre-trained Word2Vec vector.</p> <ul> <li>We initialize a matrix of zeros with shape <code>(vocab_size, 300)</code></li> <li>300 is the dimensionality of Google's pre-trained Word2Vec vectors</li> </ul>"},{"location":"chapter4/Session_4/#embedding-matrix-initialization","title":"\ud83e\uddee Embedding Matrix Initialization\u00b6","text":"<p>Every row <code>i</code> in this matrix will store the 300-dimensional Word2Vec vector for the <code>i-th</code> word in our vocabulary.</p>"},{"location":"chapter4/Session_4/#filling-the-embedding-matrix","title":"\ud83d\udd0d Filling the Embedding Matrix\u00b6","text":"<p>For each word in our tokenizer's vocabulary:</p> <ul> <li>We look up its corresponding vector from the pre-trained Word2Vec model</li> <li>If it's not found (OOV word), we leave the row as zeros</li> <li>We use <code>tqdm</code> to track progress, which is helpful when processing large vocabularies</li> </ul>"},{"location":"chapter4/Session_4/#model-with-word2vec-embeddings","title":"\ud83e\uddf1 Model with Word2Vec Embeddings\u00b6","text":"<ul> <li>The <code>Embedding</code> layer now uses our pre-trained Word2Vec weights</li> <li><code>trainable=False</code> means we freeze the embeddings (no updates during training)</li> <li>Same <code>Bidirectional LSTM</code> and <code>Dense</code> classifier as before</li> </ul>"},{"location":"chapter4/Session_4/#compile-the-model","title":"\u2699\ufe0f Compile the Model\u00b6","text":"<p>We compile with:</p> <ul> <li><code>adam</code> optimizer</li> <li><code>categorical_crossentropy</code> loss for multi-class classification</li> <li>Accuracy as our evaluation metric</li> </ul> <p>Then we print the model architecture to review.</p>"},{"location":"chapter4/Session_4/#3-training-and-evaluating-the-model","title":"3. Training and Evaluating the Model:\u00b6","text":"<p>We'll train our model with these new embeddings and evaluate to see if there's an improvement in performance.</p>"},{"location":"chapter4/Session_4/#4-model-evaluation","title":"4. Model Evaluation:\u00b6","text":"<p>Similar to previous parts, we will evaluate our model's performance using appropriate metrics.</p>"},{"location":"chapter4/Session_4/#part-4-model-explainability-lime-shap","title":"Part 4: Model Explainability (LIME / SHAP)\u00b6","text":"<p>As machine learning models become more powerful, they also become more complex and opaque \u2014 especially deep learning models like LSTMs or transformer-based architectures. This complexity makes it harder to understand why a model makes a specific prediction.</p> <p>In this part, we will explore model explainability, a crucial step in building trustworthy and transparent machine learning systems.</p>"},{"location":"chapter4/Session_4/#41-why-explainability-matters","title":"4.1 Why Explainability Matters\u00b6","text":"<p>Imagine you\u2019ve built a model that predicts whether a customer is likely to churn, or whether a loan should be approved. Even if your model is 90% accurate, you might be asked:</p> <p>\ud83e\udd14 \"But why did the model make that decision?\"</p> <p>This is where explainability becomes essential.</p>"},{"location":"chapter4/Session_4/#why-its-important","title":"Why it's important:\u00b6","text":"<ul> <li>Trust: Users are more likely to trust predictions they can understand.</li> <li>Debugging: Helps identify spurious correlations or biases in the model.</li> <li>Fairness &amp; Ethics: Ensures decisions are not based on sensitive or discriminatory attributes.</li> <li>Regulatory Compliance: In some domains (like finance or healthcare), explainability is required by law.</li> </ul>"},{"location":"chapter4/Session_4/#two-main-categories-of-explainability","title":"Two Main Categories of Explainability:\u00b6","text":"<ol> <li><p>Global Explanations: Understanding the overall model behavior Example: Which words generally influence sentiment predictions the most?</p> </li> <li><p>Local Explanations: Understanding individual predictions Example: Why was *this* review classified as negative?</p> </li> </ol> <p>We'll focus primarily on local explanations using:</p> <ul> <li><code>LIME</code> (for simpler models like TF-IDF + Logistic Regression)</li> </ul> <p>Let\u2019s begin by applying LIME to our TF-IDF baseline model.</p>"},{"location":"chapter4/Session_4/#42-applying-lime-to-the-tf-idf-model","title":"4.2 \ud83e\uddea Applying LIME to the TF-IDF Model\u00b6","text":"<p>Now that we understand why explainability matters, let's start by applying LIME (Local Interpretable Model-agnostic Explanations) to our first model: \u27a1\ufe0f A TF-IDF + Logistic Regression pipeline.</p> <p>LIME works by slightly perturbing the input text and seeing how the model prediction changes. From this, it builds a local, interpretable surrogate model (like a linear regression) to approximate the complex model's behavior near that input.</p> <p>We'll explain:</p> <ul> <li>A single prediction for a text sample</li> <li>Which words had the most impact (positive or negative) on the predicted label</li> </ul>"},{"location":"chapter4/Session_4/#interpretation-of-lime-explanation","title":"\ud83e\udde0 Interpretation of LIME Explanation\u00b6","text":"<p>Let's break down what LIME revealed about the model's reasoning for this particular prediction.</p>"},{"location":"chapter4/Session_4/#predicted-class-business","title":"\u2705 Predicted Class: Business\u00b6","text":"<p>LIME shows us the top 10 words that contributed positively or negatively to each possible class (Business, World, Sci/Tech, Sports).</p>"},{"location":"chapter4/Session_4/#for-class-business","title":"\ud83d\udcac For Class: Business\u00b6","text":"<ul> <li>Words like \"Federal\", \"firm\", and \"pension\" have positive weights, meaning they support the Business prediction.</li> <li>The word \"talks\" actually detracts from the Business prediction (negative weight), suggesting it's a bit ambiguous.</li> </ul>"},{"location":"chapter4/Session_4/#for-class-world","title":"\ud83c\udf0d For Class: World\u00b6","text":"<ul> <li>Interestingly, \"talks\" strongly supports the World class here.</li> <li>Other Business-related terms (e.g., \"Federal\", \"firm\") detract from a World prediction.</li> </ul>"},{"location":"chapter4/Session_4/#insights","title":"\ud83d\udca1 Insights:\u00b6","text":"<ul> <li>Words like \"workers\", \"Unions\", and \"say\" appear across multiple classes with small influence, showing they\u2019re more generic.</li> <li>\"talks\" is context-dependent \u2013 LIME helps us disentangle how the same word can shift meaning depending on the rest of the sentence.</li> </ul> <p>\ud83e\udded Takeaway: LIME helps us peek inside the model's black box and see which features are driving predictions. It also shows that certain words may support multiple classes, but with different intensities.</p> <p>Ready to move on? Let\u2019s now explore:</p>"},{"location":"chapter4/Session_4/#43-comparing-explanations-for-lstm-and-word2vec-models","title":"4.3 \ud83d\udcca Comparing Explanations for LSTM and Word2Vec Models\u00b6","text":"<p>\u27a1\ufe0f Here, we'll try to interpret more complex models (like LSTM and Word2Vec-based models) using LIME and compare how their reasoning differs from the simpler TF-IDF model.</p>"},{"location":"chapter4/Session_4/#lime-explanation-for-lstm-model","title":"\ud83d\udd0d LIME Explanation for LSTM Model\u00b6","text":""},{"location":"chapter4/Session_4/#sentence-analyzed","title":"\ud83e\udde0 Sentence Analyzed:\u00b6","text":"<p>\"Fears for T N pension after talks Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"</p>"},{"location":"chapter4/Session_4/#predicted-class-scitech","title":"\u2705 Predicted Class: Sci/Tech\u00b6","text":"<p>With a very high probability (78.35%), the LSTM model predicted this text belongs to Sci/Tech.</p>"},{"location":"chapter4/Session_4/#class-probabilities","title":"\ud83d\udcca Class Probabilities:\u00b6","text":"<ul> <li>Sci/Tech: 0.7835</li> <li>World: 0.0995</li> <li>Business: 0.0839</li> <li>Sports: 0.0331</li> </ul>"},{"location":"chapter4/Session_4/#what-lime-reveals","title":"\ud83d\udca1 What LIME Reveals\u00b6","text":""},{"location":"chapter4/Session_4/#top-features-supporting-scitech","title":"Top Features Supporting Sci/Tech:\u00b6","text":"<ul> <li>Common connectors like \"they\", \"after\", \"are\", \"with\", and \"at\" surprisingly contribute positively to Sci/Tech.</li> <li>Words like \"firm\" and \"talks\", which might intuitively relate to Business or World, actually reduce the Sci/Tech probability here.</li> </ul>"},{"location":"chapter4/Session_4/#across-other-classes","title":"Across Other Classes:\u00b6","text":"<ul> <li>Most of the same function words (\"they\", \"after\", \"with\") appear as negative contributors to World, Business, and Sports classes.</li> <li>Words like \"firm\" slightly boost the Business class but are downplayed for Sci/Tech.</li> <li>In the Sports class, all top words negatively influence the prediction, confirming it\u2019s a poor match.</li> </ul>"},{"location":"chapter4/Session_4/#interpretation","title":"\ud83e\udde0 Interpretation:\u00b6","text":"<p>Unlike the TF-IDF model, which focused on specific content words, the LSTM model seems to be relying heavily on syntactic or structural features (like function words and word order). This could be due to:</p> <ul> <li>The sequential nature of LSTM, which captures contextual dependencies</li> <li>A possible lack of domain-specific keywords driving this prediction</li> </ul>"},{"location":"chapter4/Session_4_correction/","title":"Advanced Methods in Natural Language Processing - Session 4","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nclass Metrics:\n    def __init__(self):\n        self.results = {}\n\n    def run(self, y_true, y_pred, method_name, average='macro'):\n        # Calculate metrics\n        accuracy = accuracy_score(y_true, y_pred)\n        precision = precision_score(y_true, y_pred, average=average)\n        recall = recall_score(y_true, y_pred, average=average)\n        f1 = f1_score(y_true, y_pred, average=average)\n\n        # Store results\n        self.results[method_name] = {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n        }\n\n    def plot(self):\n        # Create subplots\n        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n\n        # Plot each metric\n        for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):\n            ax = axs[i//2, i%2]\n            values = [res[metric] * 100 for res in self.results.values()]\n            ax.bar(self.results.keys(), values)\n            ax.set_title(metric)\n            ax.set_ylim(0, 100)\n\n            # Add values on the bars\n            for j, v in enumerate(values):\n                ax.text(j, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')\n\n        plt.tight_layout()\n        plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  class Metrics:     def __init__(self):         self.results = {}      def run(self, y_true, y_pred, method_name, average='macro'):         # Calculate metrics         accuracy = accuracy_score(y_true, y_pred)         precision = precision_score(y_true, y_pred, average=average)         recall = recall_score(y_true, y_pred, average=average)         f1 = f1_score(y_true, y_pred, average=average)          # Store results         self.results[method_name] = {             'accuracy': accuracy,             'precision': precision,             'recall': recall,             'f1': f1,         }      def plot(self):         # Create subplots         fig, axs = plt.subplots(2, 2, figsize=(15, 10))          # Plot each metric         for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1']):             ax = axs[i//2, i%2]             values = [res[metric] * 100 for res in self.results.values()]             ax.bar(self.results.keys(), values)             ax.set_title(metric)             ax.set_ylim(0, 100)              # Add values on the bars             for j, v in enumerate(values):                 ax.text(j, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')          plt.tight_layout()         plt.show() In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\n# Load the 'ag_news' dataset\ndataset = load_dataset(\"ag_news\")\n\n# Explore the structure of the dataset\nprint(dataset)\n</pre> from datasets import load_dataset  # Load the 'ag_news' dataset dataset = load_dataset(\"ag_news\")  # Explore the structure of the dataset print(dataset) <p>Let's create stratified samples for training and validation sets ensuring that each class is represented in proportion to its frequency. It will go faster with just a sample, and we will be able to make tests on validation test before trying to work on the testing set.</p> In\u00a0[3]: Copied! <pre>from sklearn.model_selection import train_test_split\n\ndata = dataset['train']['text']\nlabels = dataset['train']['label']\n\ntest_data = dataset['test']['text']\ntest_labels = dataset['test']['label']\n\n# Stratified split to create a smaller training and validation set\ntrain_data, valid_data, train_labels, valid_labels = train_test_split(\n    data, labels, stratify=labels, test_size=0.2, random_state=42\n)\n\n# Further split to get 10k and 2k samples respectively\ntrain_data, _, train_labels, _ = train_test_split(\n    train_data, train_labels, stratify=train_labels, train_size=10000, random_state=42\n)\nvalid_data, _, valid_labels, _ = train_test_split(\n    valid_data, valid_labels, stratify=valid_labels, train_size=2000, random_state=42\n)\n</pre> from sklearn.model_selection import train_test_split  data = dataset['train']['text'] labels = dataset['train']['label']  test_data = dataset['test']['text'] test_labels = dataset['test']['label']  # Stratified split to create a smaller training and validation set train_data, valid_data, train_labels, valid_labels = train_test_split(     data, labels, stratify=labels, test_size=0.2, random_state=42 )  # Further split to get 10k and 2k samples respectively train_data, _, train_labels, _ = train_test_split(     train_data, train_labels, stratify=train_labels, train_size=10000, random_state=42 ) valid_data, _, valid_labels, _ = train_test_split(     valid_data, valid_labels, stratify=valid_labels, train_size=2000, random_state=42 ) In\u00a0[\u00a0]: Copied! <pre>from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\nlabels = {0: 'World', 1: 'Sports',\n          2: 'Business', 3: 'Sci/Tech'}\n\n# Prepare data for wordclouds\nlabel_data = defaultdict(lambda: '')\n\nfor text, label in zip(train_data, train_labels):\n    label_data[label] += text\n\n# Generate and plot wordclouds for each label\nfig, axs = plt.subplots(2, 2, figsize=(10, 6))  # Create 2x2 subplots\naxs = axs.flatten()  # Flatten the axis array\n\nfor ax, (label, text) in zip(axs, label_data.items()):\n    wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(text)\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.set_title('WordCloud for Label {}'.format(labels.get(label)))\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> from wordcloud import WordCloud import matplotlib.pyplot as plt from collections import defaultdict import nltk from nltk.corpus import stopwords  nltk.download('stopwords') stop_words = set(stopwords.words('english'))  labels = {0: 'World', 1: 'Sports',           2: 'Business', 3: 'Sci/Tech'}  # Prepare data for wordclouds label_data = defaultdict(lambda: '')  for text, label in zip(train_data, train_labels):     label_data[label] += text  # Generate and plot wordclouds for each label fig, axs = plt.subplots(2, 2, figsize=(10, 6))  # Create 2x2 subplots axs = axs.flatten()  # Flatten the axis array  for ax, (label, text) in zip(axs, label_data.items()):     wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(text)     ax.imshow(wordcloud, interpolation='bilinear')     ax.set_title('WordCloud for Label {}'.format(labels.get(label)))     ax.axis('off')  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>from collections import Counter\nimport matplotlib.pyplot as plt\n\n# Count the frequency of each label\nlabel_counts = Counter(train_labels)\n\n# Data to plot\n_labels = [labels.get(lab) for lab in label_counts.keys()]\nsizes = label_counts.values()\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\n\n# Plotting the pie chart\nplt.pie(sizes, labels=_labels, colors=colors, autopct='%1.1f%%', startangle=140)\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Proportion of Each Label')\nplt.show()\n</pre> from collections import Counter import matplotlib.pyplot as plt  # Count the frequency of each label label_counts = Counter(train_labels)  # Data to plot _labels = [labels.get(lab) for lab in label_counts.keys()] sizes = label_counts.values() colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']  # Plotting the pie chart plt.pie(sizes, labels=_labels, colors=colors, autopct='%1.1f%%', startangle=140) plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle. plt.title('Proportion of Each Label') plt.show()  In\u00a0[\u00a0]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n# Create a pipeline with TF-IDF and Logistic Regression\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(ngram_range=(1, 2),\n                              min_df=5,\n                              stop_words='english')),\n    ('clf', LogisticRegression(solver='liblinear')),\n])\n\n# Fit the pipeline on the training data\npipeline.fit(train_data, train_labels)\n\nvalid_preds = pipeline.predict(valid_data)\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split  # Create a pipeline with TF-IDF and Logistic Regression pipeline = Pipeline([     ('tfidf', TfidfVectorizer(ngram_range=(1, 2),                               min_df=5,                               stop_words='english')),     ('clf', LogisticRegression(solver='liblinear')), ])  # Fit the pipeline on the training data pipeline.fit(train_data, train_labels)  valid_preds = pipeline.predict(valid_data) In\u00a0[7]: Copied! <pre>metrics_val= Metrics()\nmetrics_val.run(valid_labels, valid_preds, \"basic TF-IDF\")\n</pre> metrics_val= Metrics() metrics_val.run(valid_labels, valid_preds, \"basic TF-IDF\") In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'tfidf__min_df': [1, 2, 5],  # Example values, you can choose others\n    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2)]  # Unigrams, bigrams or both\n}\n\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n\n# Fit the grid search to the data\ngrid_search.fit(train_data, train_labels)\n\n# Best parameters found by grid search\nprint(f'Best Parameters: {grid_search.best_params_}')\n\n# Evaluate on the validation set\nvalid_preds = grid_search.predict(valid_data)\nmetrics_val.run(valid_labels, valid_preds, \"CV-ed TF-IDF\")\n</pre> from sklearn.model_selection import GridSearchCV  # Define the parameter grid param_grid = {     'tfidf__min_df': [1, 2, 5],  # Example values, you can choose others     'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2)]  # Unigrams, bigrams or both }  # Create a GridSearchCV object grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)  # Fit the grid search to the data grid_search.fit(train_data, train_labels)  # Best parameters found by grid search print(f'Best Parameters: {grid_search.best_params_}')  # Evaluate on the validation set valid_preds = grid_search.predict(valid_data) metrics_val.run(valid_labels, valid_preds, \"CV-ed TF-IDF\") In\u00a0[\u00a0]: Copied! <pre>metrics_val.plot()\n</pre> metrics_val.plot() In\u00a0[10]: Copied! <pre>from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\n# Parameters\nvocab_size = 5000  # This is a hyperparameter, adjust as needed\nmax_length = 128    # This is another hyperparameter\n\n# Initialize and fit the tokenizer\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(train_data)\n\n# Convert texts to sequences of integers\nsequences_train = tokenizer.texts_to_sequences(train_data)\nsequences_valid = tokenizer.texts_to_sequences(valid_data)\n\n# Pad sequences to the same length\npadded_sequences_train = pad_sequences(sequences_train, maxlen=max_length,\n                                 padding='post', truncating='post')\npadded_sequences_valid = pad_sequences(sequences_valid, maxlen=max_length,\n                                 padding='post', truncating='post')\n\n\n# Assuming train_labels are integer labels\nnum_classes = len(set(train_labels))  # Determine the number of unique classes\n\n# Convert labels to one-hot vectors\ntrain_labels_lstm = to_categorical(train_labels, num_classes=num_classes)\nvalid_labels_lstm = to_categorical(valid_labels, num_classes=num_classes)\n</pre> from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.utils import to_categorical  # Parameters vocab_size = 5000  # This is a hyperparameter, adjust as needed max_length = 128    # This is another hyperparameter  # Initialize and fit the tokenizer tokenizer = Tokenizer(num_words=vocab_size) tokenizer.fit_on_texts(train_data)  # Convert texts to sequences of integers sequences_train = tokenizer.texts_to_sequences(train_data) sequences_valid = tokenizer.texts_to_sequences(valid_data)  # Pad sequences to the same length padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length,                                  padding='post', truncating='post') padded_sequences_valid = pad_sequences(sequences_valid, maxlen=max_length,                                  padding='post', truncating='post')   # Assuming train_labels are integer labels num_classes = len(set(train_labels))  # Determine the number of unique classes  # Convert labels to one-hot vectors train_labels_lstm = to_categorical(train_labels, num_classes=num_classes) valid_labels_lstm = to_categorical(valid_labels, num_classes=num_classes) In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\nfrom tensorflow.keras.metrics import Precision, Recall\n\nmodel = Sequential([\n    Embedding(vocab_size, output_dim=64, input_length=max_length),\n    Bidirectional(LSTM(64)),  # First bidirectional LSTM layer\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',\n              metrics=['accuracy', Precision(), Recall()])\n\nmodel.summary()\n</pre> from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense from tensorflow.keras.metrics import Precision, Recall  model = Sequential([     Embedding(vocab_size, output_dim=64, input_length=max_length),     Bidirectional(LSTM(64)),  # First bidirectional LSTM layer     Dense(num_classes, activation='softmax') ])  model.compile(optimizer='adam', loss='categorical_crossentropy',               metrics=['accuracy', Precision(), Recall()])  model.summary() In\u00a0[\u00a0]: Copied! <pre>history = model.fit(\n    padded_sequences_train,\n    train_labels_lstm,\n    epochs=10,\n    batch_size=128,\n    validation_data=(padded_sequences_valid, valid_labels_lstm)\n)\n</pre> history = model.fit(     padded_sequences_train,     train_labels_lstm,     epochs=10,     batch_size=128,     validation_data=(padded_sequences_valid, valid_labels_lstm) ) In\u00a0[\u00a0]: Copied! <pre>predictions = model.predict(padded_sequences_valid)\nvalid_preds = np.argmax(predictions, axis=1)\nmetrics_val.run(valid_labels, valid_preds, \"BiLSTM\")\n</pre> predictions = model.predict(padded_sequences_valid) valid_preds = np.argmax(predictions, axis=1) metrics_val.run(valid_labels, valid_preds, \"BiLSTM\") In\u00a0[\u00a0]: Copied! <pre>metrics_val.plot()\n</pre> metrics_val.plot() In\u00a0[15]: Copied! <pre>from staticvectors import StaticVectors\n\nword2vec_model = StaticVectors(\"neuml/word2vec\")\n</pre> from staticvectors import StaticVectors  word2vec_model = StaticVectors(\"neuml/word2vec\") In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom tqdm import tqdm\n\n# Initialize the embedding matrix\nembedding_matrix = np.zeros((vocab_size, 300))  # 300 is the dimensionality of GoogleNews vectors\n\nfor word, i in tqdm(tokenizer.word_index.items()):\n    if i &lt; vocab_size:\n        try:\n            embedding_vector = word2vec_model.embeddings([word])\n            embedding_matrix[i] = embedding_vector\n        except KeyError:\n            # Word not found in the model, leave as zeros\n            continue\n# Define the model\n\nmodel = Sequential([\n    Embedding(vocab_size, 300, weights=[embedding_matrix],\n              input_length=max_length, trainable=False),  # Set trainable to False\n    Bidirectional(LSTM(64)),  # First bidirectional LSTM layer\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n</pre> import numpy as np from tqdm import tqdm  # Initialize the embedding matrix embedding_matrix = np.zeros((vocab_size, 300))  # 300 is the dimensionality of GoogleNews vectors  for word, i in tqdm(tokenizer.word_index.items()):     if i &lt; vocab_size:         try:             embedding_vector = word2vec_model.embeddings([word])             embedding_matrix[i] = embedding_vector         except KeyError:             # Word not found in the model, leave as zeros             continue # Define the model  model = Sequential([     Embedding(vocab_size, 300, weights=[embedding_matrix],               input_length=max_length, trainable=False),  # Set trainable to False     Bidirectional(LSTM(64)),  # First bidirectional LSTM layer     Dense(num_classes, activation='softmax') ])  model.compile(optimizer='adam', loss='categorical_crossentropy',               metrics=['accuracy']) model.summary() In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.callbacks import EarlyStopping\n\n# Setup early stopping to stop training when validation loss stops improving\nearly_stopping = EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=5,  # How many epochs to wait after min has been hit\n    verbose=1,  # Verbosity level\n    mode='min',  # Mode for the monitored quantity (minimizing loss)\n    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n)\n\n\nhistory = model.fit(\n    padded_sequences_train,\n    train_labels_lstm,\n    epochs=10,\n    batch_size=128,\n    validation_data=(padded_sequences_valid, valid_labels_lstm),\n    callbacks=[early_stopping]\n)\n</pre> from tensorflow.keras.callbacks import EarlyStopping  # Setup early stopping to stop training when validation loss stops improving early_stopping = EarlyStopping(     monitor='val_loss',  # Monitor validation loss     patience=5,  # How many epochs to wait after min has been hit     verbose=1,  # Verbosity level     mode='min',  # Mode for the monitored quantity (minimizing loss)     restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity )   history = model.fit(     padded_sequences_train,     train_labels_lstm,     epochs=10,     batch_size=128,     validation_data=(padded_sequences_valid, valid_labels_lstm),     callbacks=[early_stopping] )  In\u00a0[\u00a0]: Copied! <pre>predictions = model.predict(padded_sequences_valid)\nvalid_preds = np.argmax(predictions, axis=1)\nmetrics_val.run(valid_labels, valid_preds, \"BiLSTM + W2V\")\nmetrics_val.plot()\n</pre> predictions = model.predict(padded_sequences_valid) valid_preds = np.argmax(predictions, axis=1) metrics_val.run(valid_labels, valid_preds, \"BiLSTM + W2V\") metrics_val.plot() In\u00a0[\u00a0]: Copied! <pre>grid_search.best_estimator_.steps[1][1].coef_\n</pre> grid_search.best_estimator_.steps[1][1].coef_ In\u00a0[\u00a0]: Copied! <pre>from lime.lime_text import LimeTextExplainer\n\n# Fixed version of the TF-IDF LIME explainer function\ndef explain_tfidf_prediction(text_instance, pipeline, class_names):\n    # Create a LIME text explainer\n    explainer = LimeTextExplainer(class_names=class_names)\n    \n    # Get explanation for the prediction\n    exp = explainer.explain_instance(\n        text_instance, \n        pipeline.predict_proba, \n        num_features=10,\n        top_labels=len(class_names)  # Explain all classes\n    )\n    \n    # Display basic information\n    print(f\"Text: {text_instance}\")\n    pred_class = pipeline.predict([text_instance])[0]\n    print(f\"Predicted class: {class_names[pred_class]}\")\n    \n    # Get probabilities for all classes\n    probs = pipeline.predict_proba([text_instance])[0]\n    print(\"\\nClass probabilities:\")\n    for i, class_name in enumerate(class_names):\n        print(f\"{class_name}: {probs[i]:.4f}\")\n    \n    # Create visualization for each class\n    plt.figure(figsize=(20, 15))\n    \n    # Get the labels that LIME actually explained\n    top_labels = exp.available_labels()\n    \n    for i, label_id in enumerate(top_labels):\n        plt.subplot(2, 2, i+1)\n        \n        # Get the explanation for this class\n        exp_list = exp.as_list(label=label_id)\n        \n        # Extract words and weights\n        words = [x[0] for x in exp_list]\n        weights = [x[1] for x in exp_list]\n        \n        # Sort for better visualization\n        pairs = sorted(zip(words, weights), key=lambda x: x[1])\n        words = [x[0] for x in pairs]\n        weights = [x[1] for x in pairs]\n        \n        # Create bar chart\n        colors = ['red' if w &lt; 0 else 'green' for w in weights]\n        y_pos = np.arange(len(words))\n        \n        plt.barh(y_pos, weights, color=colors)\n        plt.yticks(y_pos, words)\n        plt.title(f\"Explanation for class: {class_names[label_id]}\")\n        plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print top contributing words for each class\n    for label_id in top_labels:\n        print(f\"\\nTop features for class: {class_names[label_id]}\")\n        exp_list = exp.as_list(label=label_id)\n        for word, weight in exp_list:\n            print(f\"{word}: {weight:.4f}\")\n    \n    return exp\n\n# Example text from test set\nexample_text = test_data[0]\nclass_names = ['World', 'Sports', 'Business', 'Sci/Tech']\nlime_exp_tfidf = explain_tfidf_prediction(example_text, grid_search.best_estimator_, class_names)\n</pre> from lime.lime_text import LimeTextExplainer  # Fixed version of the TF-IDF LIME explainer function def explain_tfidf_prediction(text_instance, pipeline, class_names):     # Create a LIME text explainer     explainer = LimeTextExplainer(class_names=class_names)          # Get explanation for the prediction     exp = explainer.explain_instance(         text_instance,          pipeline.predict_proba,          num_features=10,         top_labels=len(class_names)  # Explain all classes     )          # Display basic information     print(f\"Text: {text_instance}\")     pred_class = pipeline.predict([text_instance])[0]     print(f\"Predicted class: {class_names[pred_class]}\")          # Get probabilities for all classes     probs = pipeline.predict_proba([text_instance])[0]     print(\"\\nClass probabilities:\")     for i, class_name in enumerate(class_names):         print(f\"{class_name}: {probs[i]:.4f}\")          # Create visualization for each class     plt.figure(figsize=(20, 15))          # Get the labels that LIME actually explained     top_labels = exp.available_labels()          for i, label_id in enumerate(top_labels):         plt.subplot(2, 2, i+1)                  # Get the explanation for this class         exp_list = exp.as_list(label=label_id)                  # Extract words and weights         words = [x[0] for x in exp_list]         weights = [x[1] for x in exp_list]                  # Sort for better visualization         pairs = sorted(zip(words, weights), key=lambda x: x[1])         words = [x[0] for x in pairs]         weights = [x[1] for x in pairs]                  # Create bar chart         colors = ['red' if w &lt; 0 else 'green' for w in weights]         y_pos = np.arange(len(words))                  plt.barh(y_pos, weights, color=colors)         plt.yticks(y_pos, words)         plt.title(f\"Explanation for class: {class_names[label_id]}\")         plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)          plt.tight_layout()     plt.show()          # Print top contributing words for each class     for label_id in top_labels:         print(f\"\\nTop features for class: {class_names[label_id]}\")         exp_list = exp.as_list(label=label_id)         for word, weight in exp_list:             print(f\"{word}: {weight:.4f}\")          return exp  # Example text from test set example_text = test_data[0] class_names = ['World', 'Sports', 'Business', 'Sci/Tech'] lime_exp_tfidf = explain_tfidf_prediction(example_text, grid_search.best_estimator_, class_names) In\u00a0[\u00a0]: Copied! <pre>def prepare_text_for_lstm(text, tokenizer, max_length):\n    \"\"\"Prepare text input for LSTM model\"\"\"\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    sequences = tokenizer.texts_to_sequences([text])\n    padded_seq = pad_sequences(sequences, maxlen=max_length)\n    return padded_seq\n\ndef lstm_predict_proba(texts):\n    \"\"\"Prediction function for LIME to use with LSTM model\"\"\"\n    result = np.zeros((len(texts), len(class_names)))\n    for i, text in enumerate(texts):\n        padded = prepare_text_for_lstm(text, tokenizer, max_length)\n        pred = model.predict(padded, verbose=0)\n        result[i] = pred[0]\n    return result\n\ndef explain_lstm_prediction(text_instance, class_names):\n    # Create a LIME text explainer\n    explainer = LimeTextExplainer(class_names=class_names)\n    \n    # Get explanation for the prediction\n    exp = explainer.explain_instance(\n        text_instance, \n        lstm_predict_proba, \n        num_features=10,\n        top_labels=len(class_names)  # Explain all classes\n    )\n    \n    # Display basic information\n    print(f\"Text: {text_instance}\")\n    padded = prepare_text_for_lstm(text_instance, tokenizer, max_length)\n    prediction = model.predict(padded, verbose=0)\n    predicted_class = np.argmax(prediction[0])\n    print(f\"Predicted class: {class_names[predicted_class]}\")\n    \n    # Get probabilities for all classes\n    probs = prediction[0]\n    print(\"\\nClass probabilities:\")\n    for i, class_name in enumerate(class_names):\n        print(f\"{class_name}: {probs[i]:.4f}\")\n    \n    # Create visualization for each class\n    plt.figure(figsize=(20, 15))\n    \n    # Get the labels that LIME actually explained\n    top_labels = exp.available_labels()\n    \n    for i, label_id in enumerate(top_labels):\n        plt.subplot(2, 2, i+1)\n        \n        # Get the explanation for this class\n        exp_list = exp.as_list(label=label_id)\n        \n        # Extract words and weights\n        words = [x[0] for x in exp_list]\n        weights = [x[1] for x in exp_list]\n        \n        # Sort for better visualization\n        pairs = sorted(zip(words, weights), key=lambda x: x[1])\n        words = [x[0] for x in pairs]\n        weights = [x[1] for x in pairs]\n        \n        # Create bar chart\n        colors = ['red' if w &lt; 0 else 'green' for w in weights]\n        y_pos = np.arange(len(words))\n        \n        plt.barh(y_pos, weights, color=colors)\n        plt.yticks(y_pos, words)\n        plt.title(f\"Explanation for class: {class_names[label_id]}\")\n        plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print top contributing words for each class\n    for label_id in top_labels:\n        print(f\"\\nTop features for class: {class_names[label_id]}\")\n        exp_list = exp.as_list(label=label_id)\n        for word, weight in exp_list:\n            print(f\"{word}: {weight:.4f}\")\n    \n    return exp\n\nexample_text = test_data[0]\nmodel_exp_lime = explain_lstm_prediction(example_text, class_names)\n</pre> def prepare_text_for_lstm(text, tokenizer, max_length):     \"\"\"Prepare text input for LSTM model\"\"\"     from tensorflow.keras.preprocessing.sequence import pad_sequences     sequences = tokenizer.texts_to_sequences([text])     padded_seq = pad_sequences(sequences, maxlen=max_length)     return padded_seq  def lstm_predict_proba(texts):     \"\"\"Prediction function for LIME to use with LSTM model\"\"\"     result = np.zeros((len(texts), len(class_names)))     for i, text in enumerate(texts):         padded = prepare_text_for_lstm(text, tokenizer, max_length)         pred = model.predict(padded, verbose=0)         result[i] = pred[0]     return result  def explain_lstm_prediction(text_instance, class_names):     # Create a LIME text explainer     explainer = LimeTextExplainer(class_names=class_names)          # Get explanation for the prediction     exp = explainer.explain_instance(         text_instance,          lstm_predict_proba,          num_features=10,         top_labels=len(class_names)  # Explain all classes     )          # Display basic information     print(f\"Text: {text_instance}\")     padded = prepare_text_for_lstm(text_instance, tokenizer, max_length)     prediction = model.predict(padded, verbose=0)     predicted_class = np.argmax(prediction[0])     print(f\"Predicted class: {class_names[predicted_class]}\")          # Get probabilities for all classes     probs = prediction[0]     print(\"\\nClass probabilities:\")     for i, class_name in enumerate(class_names):         print(f\"{class_name}: {probs[i]:.4f}\")          # Create visualization for each class     plt.figure(figsize=(20, 15))          # Get the labels that LIME actually explained     top_labels = exp.available_labels()          for i, label_id in enumerate(top_labels):         plt.subplot(2, 2, i+1)                  # Get the explanation for this class         exp_list = exp.as_list(label=label_id)                  # Extract words and weights         words = [x[0] for x in exp_list]         weights = [x[1] for x in exp_list]                  # Sort for better visualization         pairs = sorted(zip(words, weights), key=lambda x: x[1])         words = [x[0] for x in pairs]         weights = [x[1] for x in pairs]                  # Create bar chart         colors = ['red' if w &lt; 0 else 'green' for w in weights]         y_pos = np.arange(len(words))                  plt.barh(y_pos, weights, color=colors)         plt.yticks(y_pos, words)         plt.title(f\"Explanation for class: {class_names[label_id]}\")         plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)          plt.tight_layout()     plt.show()          # Print top contributing words for each class     for label_id in top_labels:         print(f\"\\nTop features for class: {class_names[label_id]}\")         exp_list = exp.as_list(label=label_id)         for word, weight in exp_list:             print(f\"{word}: {weight:.4f}\")          return exp  example_text = test_data[0] model_exp_lime = explain_lstm_prediction(example_text, class_names) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter4/Session_4_correction/#advanced-methods-in-natural-language-processing-session-4","title":"Advanced Methods in Natural Language Processing - Session 4\u00b6","text":""},{"location":"chapter4/Session_4_correction/#text-classification-with-ag-news-corpus","title":"Text Classification with AG News Corpus\u00b6","text":"<p>This notebook will guide you through different approaches to text classification using the AG News corpus. We will start with a simple baseline model and gradually move towards more complex and sophisticated models.</p>"},{"location":"chapter4/Session_4_correction/#table-of-contents","title":"Table of Contents\u00b6","text":"<ol> <li><p>Part 1: Baseline Pipeline with TF-IDF and Linear Model</p> <ul> <li>1.1. Loading and Exploring Data</li> <li>1.2. Feature Extraction with TF-IDF</li> <li>1.3. Training a Linear Model</li> <li>1.4. Model Evaluation</li> </ul> </li> <li><p>Part 2: LSTM Pipeline with One-Hot Encoding</p> <ul> <li>2.1. Preprocessing for LSTM</li> <li>2.2. Building a Bidirectional LSTM Model</li> <li>2.3. Training the LSTM Model</li> <li>2.4. Model Evaluation</li> </ul> </li> <li><p>Part 3: Word Embedding Add-Ons with Word2Vec</p> <ul> <li>3.1. Loading Pre-trained Word2Vec Embeddings</li> <li>3.2. Integrating Word2Vec into LSTM Model</li> <li>3.3. Training and Evaluating the Model</li> </ul> </li> <li><p>Part 4: Model Explainability (LIME / SHAP)</p> <ul> <li>4.1. Why Explainability Matters</li> <li>4.2. Applying LIME to the TF-IDF Model</li> <li>4.3. Comparing Explanation for LSTM with Word2Vec model</li> </ul> </li> </ol>"},{"location":"chapter4/Session_4_correction/#part-0-metrics-functions-to-consider","title":"Part 0: Metrics Functions to Consider\u00b6","text":"<p>Before diving into the model building and training, it's crucial to establish the metrics we'll use to evaluate our models. In this part, we will define and discuss the different metrics functions that are commonly used in NLP tasks, particularly for text classification:</p> <ol> <li><p>Accuracy: Measures the proportion of correct predictions among the total number of cases examined. It's a straightforward metric but can be misleading if the classes are imbalanced.</p> </li> <li><p>Precision and Recall: Precision measures the proportion of positive identifications that were actually correct, while recall measures the proportion of actual positives that were identified correctly. These metrics are especially important when dealing with imbalanced datasets.</p> </li> <li><p>F1 Score: The harmonic mean of precision and recall. It's a good way to show that a classifer has a good balance between precision and recall.</p> </li> <li><p>Confusion Matrix: A table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.</p> </li> <li><p>ROC and AUC: The receiver operating characteristic curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system. The area under the curve (AUC) represents measure of separability.</p> </li> </ol> <p>We will implement these metrics functions using libraries such as scikit-learn, and they will be used to assess and compare the performance of our different models throughout this exercise.</p>"},{"location":"chapter4/Session_4_correction/#part-1-baseline-pipeline-with-tf-idf-and-linear-model","title":"Part 1: Baseline Pipeline with TF-IDF and Linear Model\u00b6","text":"<p>In this part, we will create a baseline model for text classification. This involves:</p>"},{"location":"chapter4/Session_4_correction/#1-loading-and-exploring-data","title":"1. Loading and Exploring Data:\u00b6","text":"<p>We will load the AG News corpus and perform necessary preprocessing steps like exploring the dataset.</p>"},{"location":"chapter4/Session_4_correction/#2-feature-extraction-with-tf-idf","title":"2. Feature Extraction with TF-IDF:\u00b6","text":"<p>We will convert the text data into numerical form using the TF-IDF vectorization technique. We will use the <code>Pipeline</code> class from scikit-learn which is really practical.</p>"},{"location":"chapter4/Session_4_correction/#3-training-with-cross-validation","title":"3. Training with Cross Validation:\u00b6","text":"<p>We will train a linear classifier (such as Logistic Regression) using the extracted features, <code>Pipeline</code> module and cross validation with <code>GridSearchCV</code>.</p>"},{"location":"chapter4/Session_4_correction/#4-model-evaluation","title":"4. Model Evaluation:\u00b6","text":"<p>We will evaluate the performance of our model on a separate test set using various metrics.</p>"},{"location":"chapter4/Session_4_correction/#part-2-lstm-pipeline-with-one-hot-encoding","title":"Part 2: LSTM Pipeline with One-Hot Encoding\u00b6","text":"<p>In this part, we'll explore a more complex model using LSTM:</p>"},{"location":"chapter4/Session_4_correction/#1-preprocessing-for-lstm","title":"1. Preprocessing for LSTM:\u00b6","text":"<p>We'll prepare the text data for LSTM, which involves tokenization and converting words to one-hot encoded vectors.</p>"},{"location":"chapter4/Session_4_correction/#2-building-a-bidirectional-lstm-model","title":"2. Building a Bidirectional LSTM Model:\u00b6","text":"<p>We'll design a neural network with a Bidirectional LSTM layer to capture context from both directions in the text.</p>"},{"location":"chapter4/Session_4_correction/#3-training-the-lstm-model","title":"3. Training the LSTM Model:\u00b6","text":"<p>We'll train our LSTM model on the preprocessed text data.</p>"},{"location":"chapter4/Session_4_correction/#4-model-evaluation","title":"4. Model Evaluation:\u00b6","text":"<p>Similar to Part 1, we will evaluate our model's performance using appropriate metrics.</p>"},{"location":"chapter4/Session_4_correction/#part-3-word-embedding-add-ons-with-word2vec","title":"Part 3: Word Embedding Add-Ons with Word2Vec\u00b6","text":"<p>This part focuses on integrating pre-trained word embeddings into our model.</p>"},{"location":"chapter4/Session_4_correction/#1-loading-pre-trained-word2vec-embeddings","title":"1. Loading Pre-trained Word2Vec Embeddings:\u00b6","text":"<p>We'll load Word2Vec embeddings pre-trained on a large corpus.</p>"},{"location":"chapter4/Session_4_correction/#2-integrating-word2vec-into-lstm-model","title":"2. Integrating Word2Vec into LSTM Model:\u00b6","text":"<p>We'll use these embeddings as inputs to our LSTM model, potentially enhancing its ability to understand context and semantics.</p>"},{"location":"chapter4/Session_4_correction/#3-training-and-evaluating-the-model","title":"3. Training and Evaluating the Model:\u00b6","text":"<p>We'll train our model with these new embeddings and evaluate to see if there's an improvement in performance.</p>"},{"location":"chapter4/Session_4_correction/#4-model-evaluation","title":"4. Model Evaluation:\u00b6","text":"<p>Similar to previous parts, we will evaluate our model's performance using appropriate metrics.</p>"},{"location":"chapter4/Session_4_correction/#part-4-model-explainability-lime-shap","title":"Part 4: Model Explainability (LIME / SHAP)\u00b6","text":"<p>As machine learning models become more powerful, they also become more complex and opaque \u2014 especially deep learning models like LSTMs or transformer-based architectures. This complexity makes it harder to understand why a model makes a specific prediction.</p> <p>In this part, we will explore model explainability, a crucial step in building trustworthy and transparent machine learning systems.</p>"},{"location":"chapter4/Session_4_correction/#41-why-explainability-matters","title":"4.1 Why Explainability Matters\u00b6","text":"<p>Imagine you\u2019ve built a model that predicts whether a customer is likely to churn, or whether a loan should be approved. Even if your model is 90% accurate, you might be asked:</p> <p>\ud83e\udd14 \"But why did the model make that decision?\"</p> <p>This is where explainability becomes essential.</p>"},{"location":"chapter4/Session_4_correction/#why-its-important","title":"Why it's important:\u00b6","text":"<ul> <li>Trust: Users are more likely to trust predictions they can understand.</li> <li>Debugging: Helps identify spurious correlations or biases in the model.</li> <li>Fairness &amp; Ethics: Ensures decisions are not based on sensitive or discriminatory attributes.</li> <li>Regulatory Compliance: In some domains (like finance or healthcare), explainability is required by law.</li> </ul>"},{"location":"chapter4/Session_4_correction/#two-main-categories-of-explainability","title":"Two Main Categories of Explainability:\u00b6","text":"<ol> <li><p>Global Explanations: Understanding the overall model behavior Example: Which words generally influence sentiment predictions the most?</p> </li> <li><p>Local Explanations: Understanding individual predictions Example: Why was *this* review classified as negative?</p> </li> </ol> <p>We'll focus primarily on local explanations using:</p> <ul> <li><code>LIME</code> (for simpler models like TF-IDF + Logistic Regression)</li> </ul> <p>Let\u2019s begin by applying LIME to our TF-IDF baseline model.</p>"},{"location":"chapter4/Session_4_correction/#42-applying-lime-to-the-tf-idf-model","title":"4.2 \ud83e\uddea Applying LIME to the TF-IDF Model\u00b6","text":"<p>Now that we understand why explainability matters, let's start by applying LIME (Local Interpretable Model-agnostic Explanations) to our first model: \u27a1\ufe0f A TF-IDF + Logistic Regression pipeline.</p> <p>LIME works by slightly perturbing the input text and seeing how the model prediction changes. From this, it builds a local, interpretable surrogate model (like a linear regression) to approximate the complex model's behavior near that input.</p> <p>We'll explain:</p> <ul> <li>A single prediction for a text sample</li> <li>Which words had the most impact (positive or negative) on the predicted label</li> </ul>"},{"location":"chapter4/Session_4_correction/#interpretation-of-lime-explanation","title":"\ud83e\udde0 Interpretation of LIME Explanation\u00b6","text":"<p>Let's break down what LIME revealed about the model's reasoning for this particular prediction.</p>"},{"location":"chapter4/Session_4_correction/#predicted-class-business","title":"\u2705 Predicted Class: Business\u00b6","text":"<p>LIME shows us the top 10 words that contributed positively or negatively to each possible class (Business, World, Sci/Tech, Sports).</p>"},{"location":"chapter4/Session_4_correction/#for-class-business","title":"\ud83d\udcac For Class: Business\u00b6","text":"<ul> <li>Words like \"Federal\", \"firm\", and \"pension\" have positive weights, meaning they support the Business prediction.</li> <li>The word \"talks\" actually detracts from the Business prediction (negative weight), suggesting it's a bit ambiguous.</li> </ul>"},{"location":"chapter4/Session_4_correction/#for-class-world","title":"\ud83c\udf0d For Class: World\u00b6","text":"<ul> <li>Interestingly, \"talks\" strongly supports the World class here.</li> <li>Other Business-related terms (e.g., \"Federal\", \"firm\") detract from a World prediction.</li> </ul>"},{"location":"chapter4/Session_4_correction/#insights","title":"\ud83d\udca1 Insights:\u00b6","text":"<ul> <li>Words like \"workers\", \"Unions\", and \"say\" appear across multiple classes with small influence, showing they\u2019re more generic.</li> <li>\"talks\" is context-dependent \u2013 LIME helps us disentangle how the same word can shift meaning depending on the rest of the sentence.</li> </ul> <p>\ud83e\udded Takeaway: LIME helps us peek inside the model's black box and see which features are driving predictions. It also shows that certain words may support multiple classes, but with different intensities.</p> <p>Ready to move on? Let\u2019s now explore:</p>"},{"location":"chapter4/Session_4_correction/#43-comparing-explanations-for-lstm-and-word2vec-models","title":"4.3 \ud83d\udcca Comparing Explanations for LSTM and Word2Vec Models\u00b6","text":"<p>\u27a1\ufe0f Here, we'll try to interpret more complex models (like LSTM and Word2Vec-based models) using LIME and compare how their reasoning differs from the simpler TF-IDF model.</p>"},{"location":"chapter4/Session_4_correction/#lime-explanation-for-lstm-model","title":"\ud83d\udd0d LIME Explanation for LSTM Model\u00b6","text":""},{"location":"chapter4/Session_4_correction/#sentence-analyzed","title":"\ud83e\udde0 Sentence Analyzed:\u00b6","text":"<p>\"Fears for T N pension after talks Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\"</p>"},{"location":"chapter4/Session_4_correction/#predicted-class-scitech","title":"\u2705 Predicted Class: Sci/Tech\u00b6","text":"<p>With a very high probability (78.35%), the LSTM model predicted this text belongs to Sci/Tech.</p>"},{"location":"chapter4/Session_4_correction/#class-probabilities","title":"\ud83d\udcca Class Probabilities:\u00b6","text":"<ul> <li>Sci/Tech: 0.7835</li> <li>World: 0.0995</li> <li>Business: 0.0839</li> <li>Sports: 0.0331</li> </ul>"},{"location":"chapter4/Session_4_correction/#what-lime-reveals","title":"\ud83d\udca1 What LIME Reveals\u00b6","text":""},{"location":"chapter4/Session_4_correction/#top-features-supporting-scitech","title":"Top Features Supporting Sci/Tech:\u00b6","text":"<ul> <li>Common connectors like \"they\", \"after\", \"are\", \"with\", and \"at\" surprisingly contribute positively to Sci/Tech.</li> <li>Words like \"firm\" and \"talks\", which might intuitively relate to Business or World, actually reduce the Sci/Tech probability here.</li> </ul>"},{"location":"chapter4/Session_4_correction/#across-other-classes","title":"Across Other Classes:\u00b6","text":"<ul> <li>Most of the same function words (\"they\", \"after\", \"with\") appear as negative contributors to World, Business, and Sports classes.</li> <li>Words like \"firm\" slightly boost the Business class but are downplayed for Sci/Tech.</li> <li>In the Sports class, all top words negatively influence the prediction, confirming it\u2019s a poor match.</li> </ul>"},{"location":"chapter4/Session_4_correction/#interpretation","title":"\ud83e\udde0 Interpretation:\u00b6","text":"<p>Unlike the TF-IDF model, which focused on specific content words, the LSTM model seems to be relying heavily on syntactic or structural features (like function words and word order). This could be due to:</p> <ul> <li>The sequential nature of LSTM, which captures contextual dependencies</li> <li>A possible lack of domain-specific keywords driving this prediction</li> </ul>"},{"location":"chapter5/","title":"Session 5: Transformers & BERT","text":""},{"location":"chapter5/#course-materials","title":"\ud83c\udf93 Course Materials","text":""},{"location":"chapter5/#slides","title":"\ud83d\udcd1 Slides","text":"<p>Download Session 5 Slides (PDF)</p>"},{"location":"chapter5/#notebooks","title":"\ud83d\udcd3 Notebooks","text":"<ul> <li>Implementing BERT with Hugging Face Transformers</li> <li>Visualizing Attention Mechanisms</li> <li>Comparing LSTM vs. BERT vs. TinyBERT vs. ModernBERT</li> </ul>"},{"location":"chapter5/#session-5-attention-transformers-and-bert","title":"\ud83d\ude80 Session 5: Attention, Transformers, and BERT","text":"<p>In this fifth session, we move from traditional sequence models to the architecture that revolutionized NLP: the Transformer. We analyze how attention mechanisms solved the context-length limitations of RNNs, and how BERT, built on top of Transformers, became the new backbone of language understanding.</p> <p>We also explore fine-tuning BERT for downstream tasks, and examine several variants (e.g., SciBERT, XLM-T, ModernBERT) tailored for specific domains or efficiency needs.</p>"},{"location":"chapter5/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ol> <li>Identify the limitations of RNNs and understand why attention mechanisms were introduced.</li> <li>Understand the full Transformer architecture including self-attention and feed-forward components.</li> <li>Grasp the innovations of BERT: bidirectionality, MLM, and NSP.</li> <li>Learn to fine-tune BERT for real tasks (NER, classification, QA).</li> <li>Explore extensions and variants like DistilBERT, SciBERT, XtremeDistil, and ModernBERT.</li> </ol>"},{"location":"chapter5/#topics-covered","title":"\ud83d\udcda Topics Covered","text":""},{"location":"chapter5/#attention-transformers","title":"Attention &amp; Transformers","text":"<ul> <li>Limitations of RNNs (sequential processing, long-distance dependencies).</li> <li>Attention Mechanism: Query-Key-Value, dynamic focus, soft memory.</li> <li>Self-Attention: Core of the Transformer \u2014 all tokens attend to all others.</li> <li>Multi-Head Attention: Capture different representation subspaces.</li> <li>Transformer Architecture: Encoder-decoder stack, position encoding, full parallelization.</li> </ul>"},{"location":"chapter5/#bert-bidirectional-encoder-representations-from-transformers","title":"BERT: Bidirectional Encoder Representations from Transformers","text":"<ul> <li>BERT architecture: 12\u201324 layers, multi-head attention, 110M+ parameters.</li> <li>Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).</li> <li>Tokenization strategies (WordPiece, special tokens).</li> <li>Fine-tuning BERT for:</li> <li>Classification</li> <li>Token-level tasks (e.g., NER, QA)</li> <li>Performance on benchmarks (GLUE, SQuAD).</li> </ul>"},{"location":"chapter5/#bert-variants-and-extensions","title":"BERT Variants and Extensions","text":"<ul> <li>SciBERT for scientific text understanding.</li> <li>EconBERTa for named entity recognition in economic research.</li> <li>XLM-T for multilingual social media analysis.</li> <li>XtremeDistilTransformer: BERT distilled for efficiency.</li> <li>ModernBERT (2024): Faster, longer-context, flash attention, rotary embeddings.</li> </ul>"},{"location":"chapter5/#key-takeaways","title":"\ud83e\udde0 Key Takeaways","text":"Architecture Sequential? Long-Context Friendly Fine-Tunable Efficient Inference LSTM \u2705 \u274c \u2705 \u26a0\ufe0f Transformer \u274c \u2705 \u2705 \u2705 BERT \u274c \u2705 (but limited tokens) \u2705 \u26a0\ufe0f ModernBERT \u274c \u2705 (8k tokens) \u2705 \u2705"},{"location":"chapter5/#bibliography-recommended-reading","title":"\ud83d\udcd6 Bibliography &amp; Recommended Reading","text":"<ul> <li> <p>Vaswani et al. (2017): Attention Is All You Need \u2013 Paper   The foundation of the Transformer model.</p> </li> <li> <p>Alammar (2018): The Illustrated Transformer \u2013 Blog Post   Highly visual explanation of attention and Transformer layers.</p> </li> <li> <p>Devlin et al. (2019): BERT: Pre-training of Deep Bidirectional Transformers \u2013 Paper   Original BERT paper introducing MLM and NSP.</p> </li> <li> <p>Warner et al. (2024): ModernBERT \u2013 Paper   A modern rethinking of BERT optimized for efficiency and long-context modeling.</p> </li> <li> <p>Rogers et al. (2020): A Primer in BERTology \u2013 Paper   Analysis and interpretability of BERT\u2019s internal behavior.</p> </li> </ul>"},{"location":"chapter5/#practical-components","title":"\ud83d\udcbb Practical Components","text":"<ul> <li>Hugging Face BERT: Load, fine-tune, and evaluate BERT on classification or QA tasks.</li> <li>Attention Visualization: See how attention heads behave using heatmaps and interpret interactions between tokens.</li> <li>Model Benchmarking: Compare inference time, memory use, and accuracy of LSTM, BERT, TinyBERT, and ModernBERT.</li> </ul>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/","title":"BERT Implementation with HF vs LSTM Text Classification vs. TF-IDF + Logistic Regression","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" # This is needed for the LSTM model on GPU\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Use TensorFlow backend\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n                          DataCollatorWithPadding, TrainingArguments, Trainer)\nimport evaluate, torch, os, random, json\n\nnp.random.seed(42)\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from datasets import load_dataset from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.metrics import confusion_matrix, classification_report  import os os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" # This is needed for the LSTM model on GPU os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Use TensorFlow backend  import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers  from transformers import (AutoTokenizer, AutoModelForSequenceClassification,                           DataCollatorWithPadding, TrainingArguments, Trainer) import evaluate, torch, os, random, json  np.random.seed(42) <pre>2025-05-05 11:11:03.608750: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-05 11:11:03.643424: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-05-05 11:11:03.643448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-05-05 11:11:03.644375: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-05 11:11:03.650611: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-05 11:11:04.384736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n</pre> In\u00a0[2]: Copied! <pre># Load the imdb\ndataset = load_dataset('imdb')\nprint(dataset)\n\n\n# Convert to pandas DataFrame for easier manipulation\ntrain_df = dataset['train'].shuffle(seed=42).to_pandas()\ntest_df = dataset['test'].shuffle(seed=42).to_pandas()\n# Clean the memory\ndel dataset\n</pre> # Load the imdb dataset = load_dataset('imdb') print(dataset)   # Convert to pandas DataFrame for easier manipulation train_df = dataset['train'].shuffle(seed=42).to_pandas() test_df = dataset['test'].shuffle(seed=42).to_pandas() # Clean the memory del dataset <pre>DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n</pre> In\u00a0[3]: Copied! <pre># 3.1 Vectorize text with TF-IDF\ntfidf_vec = TfidfVectorizer(stop_words='english', min_df=5, max_features=20000)\n\nX_train_tfidf = tfidf_vec.fit_transform(train_df['text'])\ny_train = train_df['label'].values\n\nX_test_tfidf = tfidf_vec.transform(test_df['text'])\ny_test = test_df['label'].values\n\nprint(\"X_train shape:\", X_train_tfidf.shape, \", y_train shape:\", y_train.shape)\n</pre> # 3.1 Vectorize text with TF-IDF tfidf_vec = TfidfVectorizer(stop_words='english', min_df=5, max_features=20000)  X_train_tfidf = tfidf_vec.fit_transform(train_df['text']) y_train = train_df['label'].values  X_test_tfidf = tfidf_vec.transform(test_df['text']) y_test = test_df['label'].values  print(\"X_train shape:\", X_train_tfidf.shape, \", y_train shape:\", y_train.shape) <pre>X_train shape: (25000, 20000) , y_train shape: (25000,)\n</pre> In\u00a0[4]: Copied! <pre># 3.2 Train a Logistic Regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train_tfidf, y_train)\n\n# Evaluate on test\ny_pred_logreg = logreg.predict(X_test_tfidf)\n\nprint(\"LogisticRegression Test Classification Report:\")\nprint(classification_report(y_test, y_pred_logreg))\ncm_logreg = confusion_matrix(y_test, y_pred_logreg)\nprint(\"Confusion Matrix:\", cm_logreg)\n</pre> # 3.2 Train a Logistic Regression model logreg = LogisticRegression() logreg.fit(X_train_tfidf, y_train)  # Evaluate on test y_pred_logreg = logreg.predict(X_test_tfidf)  print(\"LogisticRegression Test Classification Report:\") print(classification_report(y_test, y_pred_logreg)) cm_logreg = confusion_matrix(y_test, y_pred_logreg) print(\"Confusion Matrix:\", cm_logreg) <pre>LogisticRegression Test Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.88      0.88     12500\n           1       0.88      0.88      0.88     12500\n\n    accuracy                           0.88     25000\n   macro avg       0.88      0.88      0.88     25000\nweighted avg       0.88      0.88      0.88     25000\n\nConfusion Matrix: [[10985  1515]\n [ 1479 11021]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(4,4))\nsns.heatmap(cm_logreg, annot=True, cmap='Blues', fmt='d', cbar=False)\nplt.title(\"LogReg TF-IDF - Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()\n</pre> plt.figure(figsize=(4,4)) sns.heatmap(cm_logreg, annot=True, cmap='Blues', fmt='d', cbar=False) plt.title(\"LogReg TF-IDF - Confusion Matrix\") plt.xlabel(\"Predicted\") plt.ylabel(\"True\") plt.show() In\u00a0[6]: Copied! <pre>from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n#Create the tokenizer with the same vocabulary as the TF-IDF vectorizer\ncustom_vocab = tfidf_vec.vocabulary_\nvocab_size = len(custom_vocab) + 1 # +1 for the OOV token\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=\"&lt;OOV&gt;\")\n\n# Manually assign word index\ntokenizer.word_index = {word: i for i, word in enumerate(custom_vocab)}\ntokenizer.word_index[tokenizer.oov_token] = len(custom_vocab)\nprint(len(tokenizer.word_index))\n\n##Let's increase the max length of the sequences to 128\nmax_len = 128\n\n# Let's create a function to convert the text to a sequence of token IDs and pad them to the same length: 128\n\ndef text_to_seq(df_col, max_len=128):\n    seqs = tokenizer.texts_to_sequences(df_col)\n    # pad\n    seqs_padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n    return seqs_padded\n\nX_train_seq = text_to_seq(train_df['text'].map(lambda x: x.lower()), max_len=max_len)\nX_test_seq = text_to_seq(test_df['text'].map(lambda x: x.lower()), max_len=max_len)\n\ny_train_lstm = train_df['label'].values\ny_test_lstm  = test_df['label'].values\n\nX_train_seq.shape, y_train_lstm.shape\n</pre> from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences  #Create the tokenizer with the same vocabulary as the TF-IDF vectorizer custom_vocab = tfidf_vec.vocabulary_ vocab_size = len(custom_vocab) + 1 # +1 for the OOV token tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"\")  # Manually assign word index tokenizer.word_index = {word: i for i, word in enumerate(custom_vocab)} tokenizer.word_index[tokenizer.oov_token] = len(custom_vocab) print(len(tokenizer.word_index))  ##Let's increase the max length of the sequences to 128 max_len = 128  # Let's create a function to convert the text to a sequence of token IDs and pad them to the same length: 128  def text_to_seq(df_col, max_len=128):     seqs = tokenizer.texts_to_sequences(df_col)     # pad     seqs_padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')     return seqs_padded  X_train_seq = text_to_seq(train_df['text'].map(lambda x: x.lower()), max_len=max_len) X_test_seq = text_to_seq(test_df['text'].map(lambda x: x.lower()), max_len=max_len)  y_train_lstm = train_df['label'].values y_test_lstm  = test_df['label'].values  X_train_seq.shape, y_train_lstm.shape <pre>20001\n</pre> Out[6]: <pre>((25000, 128), (25000,))</pre> In\u00a0[7]: Copied! <pre>import keras\nfrom keras import layers, regularizers\n\nembedding_dim = 32\ninputs = keras.Input(shape=(max_len,), dtype=\"int32\")\nx = layers.Embedding(vocab_size, embedding_dim)(inputs)\n\n# LSTM with regularizer\nx = layers.LSTM(32,\n               return_sequences=True,\n               kernel_regularizer=regularizers.L2(0.01))(x)\n\nx = layers.Flatten()(x)\nx = layers.Dropout(0.15)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.summary()\n</pre> import keras from keras import layers, regularizers  embedding_dim = 32 inputs = keras.Input(shape=(max_len,), dtype=\"int32\") x = layers.Embedding(vocab_size, embedding_dim)(inputs)  # LSTM with regularizer x = layers.LSTM(32,                return_sequences=True,                kernel_regularizer=regularizers.L2(0.01))(x)  x = layers.Flatten()(x) x = layers.Dropout(0.15)(x) outputs = layers.Dense(1, activation=\"sigmoid\")(x) model = keras.Model(inputs, outputs) model.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 128)]             0         \n                                                                 \n embedding (Embedding)       (None, 128, 32)           640032    \n                                                                 \n lstm (LSTM)                 (None, 128, 32)           8320      \n                                                                 \n</pre> <pre>2025-05-05 11:11:25.038066: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n2025-05-05 11:11:25.042701: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n</pre> <pre> flatten (Flatten)           (None, 4096)              0         \n                                                                 \n dropout (Dropout)           (None, 4096)              0         \n                                                                 \n dense (Dense)               (None, 1)                 4097      \n                                                                 \n=================================================================\nTotal params: 652449 (2.49 MB)\nTrainable params: 652449 (2.49 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> In\u00a0[8]: Copied! <pre>from tensorflow.keras.metrics import Recall, Precision\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\n\n# Then compile with the correct metric objects\nmodel.compile(\n    optimizer=optimizer,\n    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=['accuracy', Recall(), Precision()]\n)\n</pre> from tensorflow.keras.metrics import Recall, Precision  optimizer = keras.optimizers.Adam(learning_rate=0.001)  # Then compile with the correct metric objects model.compile(     optimizer=optimizer,     loss=keras.losses.BinaryCrossentropy(from_logits=False),     metrics=['accuracy', Recall(), Precision()] ) In\u00a0[9]: Copied! <pre>from tensorflow.keras.callbacks import EarlyStopping\n\nhistory = model.fit(\n    X_train_seq, y_train_lstm,\n    validation_split=0.2,\n    epochs=20,\n    batch_size=128,\n    verbose=1,\n    callbacks=[EarlyStopping(monitor='val_loss', patience=3)]\n)\n</pre> from tensorflow.keras.callbacks import EarlyStopping  history = model.fit(     X_train_seq, y_train_lstm,     validation_split=0.2,     epochs=20,     batch_size=128,     verbose=1,     callbacks=[EarlyStopping(monitor='val_loss', patience=3)] ) <pre>Epoch 1/20\n</pre> <pre>157/157 [==============================] - 10s 39ms/step - loss: 0.7218 - accuracy: 0.6925 - recall: 0.6582 - precision: 0.5813 - val_loss: 0.4788 - val_accuracy: 0.8064 - val_recall: 0.6960 - val_precision: 0.7081\nEpoch 2/20\n157/157 [==============================] - 6s 36ms/step - loss: 0.2989 - accuracy: 0.8913 - recall: 0.7491 - precision: 0.7688 - val_loss: 0.3881 - val_accuracy: 0.8410 - val_recall: 0.7881 - val_precision: 0.8005\nEpoch 3/20\n157/157 [==============================] - 6s 36ms/step - loss: 0.1876 - accuracy: 0.9392 - recall: 0.8165 - precision: 0.8247 - val_loss: 0.4466 - val_accuracy: 0.8336 - val_recall: 0.8351 - val_precision: 0.8405\nEpoch 4/20\n157/157 [==============================] - 6s 36ms/step - loss: 0.1205 - accuracy: 0.9647 - recall: 0.8514 - precision: 0.8543 - val_loss: 0.6309 - val_accuracy: 0.8064 - val_recall: 0.8590 - val_precision: 0.8662\nEpoch 5/20\n157/157 [==============================] - 6s 36ms/step - loss: 0.0881 - accuracy: 0.9752 - recall: 0.8662 - precision: 0.8768 - val_loss: 0.6237 - val_accuracy: 0.8214 - val_recall: 0.8742 - val_precision: 0.8835\n</pre> In\u00a0[12]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title('LSTM Loss')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.legend()\nplt.title('LSTM Accuracy')\nplt.show()\n</pre> plt.figure(figsize=(12,4)) plt.subplot(1,2,1) plt.plot(history.history['loss'], label='Train Loss') plt.plot(history.history['val_loss'], label='Val Loss') plt.legend() plt.title('LSTM Loss')  plt.subplot(1,2,2) plt.plot(history.history['accuracy'], label='Train Acc') plt.plot(history.history['val_accuracy'], label='Val Acc') plt.legend() plt.title('LSTM Accuracy') plt.show() In\u00a0[11]: Copied! <pre>y_pred_lstm_prob = model.predict(X_test_seq)\ny_pred_lstm = (y_pred_lstm_prob&gt;0.5).astype(int)\nprint(\"LSTM Classification Report:\")\nprint(classification_report(y_test_lstm, y_pred_lstm))\n</pre> y_pred_lstm_prob = model.predict(X_test_seq) y_pred_lstm = (y_pred_lstm_prob&gt;0.5).astype(int) print(\"LSTM Classification Report:\") print(classification_report(y_test_lstm, y_pred_lstm)) <pre>782/782 [==============================] - 5s 6ms/step\nLSTM Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.78      0.78      0.78     12500\n           1       0.78      0.78      0.78     12500\n\n    accuracy                           0.78     25000\n   macro avg       0.78      0.78      0.78     25000\nweighted avg       0.78      0.78      0.78     25000\n\n</pre> In\u00a0[13]: Copied! <pre>from datasets import load_dataset, DatasetDict\nfrom transformers import (AutoTokenizer,\n                          AutoModelForSequenceClassification,\n                          TrainingArguments, Trainer,\n                          EarlyStoppingCallback)\nimport evaluate, numpy as np, matplotlib.pyplot as plt\nfrom pathlib import Path\nimport torch, random, os\n\nmodel_ckpt   = \"bert-base-uncased\"   # \u2714\ufe0f swap for \"microsoft/MiniLM-L6...\" if you need something lighter\nnum_labels   = 2                     # binary classification\nmax_length   = 128                   # truncate / pad length\nbatch_size   = 64\nseed         = 42\n\ndef set_seed(seed):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed(seed)\n</pre> from datasets import load_dataset, DatasetDict from transformers import (AutoTokenizer,                           AutoModelForSequenceClassification,                           TrainingArguments, Trainer,                           EarlyStoppingCallback) import evaluate, numpy as np, matplotlib.pyplot as plt from pathlib import Path import torch, random, os  model_ckpt   = \"bert-base-uncased\"   # \u2714\ufe0f swap for \"microsoft/MiniLM-L6...\" if you need something lighter num_labels   = 2                     # binary classification max_length   = 128                   # truncate / pad length batch_size   = 64 seed         = 42  def set_seed(seed):     random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed) set_seed(seed)  In\u00a0[14]: Copied! <pre># Load your dataset -----------------------------------------------------------\nraw = load_dataset(\"imdb\", split={\"train\":\"train\", \"test\":\"test\"})\ndataset = DatasetDict(train=raw[\"train\"], test=raw[\"test\"])\n\n# Tokeniser -------------------------------------------------------------------\ntok = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tok(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n\ndataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\ndataset = dataset.rename_column(\"label\", \"labels\")\ndataset.set_format(\"torch\")\n</pre> # Load your dataset ----------------------------------------------------------- raw = load_dataset(\"imdb\", split={\"train\":\"train\", \"test\":\"test\"}) dataset = DatasetDict(train=raw[\"train\"], test=raw[\"test\"])  # Tokeniser ------------------------------------------------------------------- tok = AutoTokenizer.from_pretrained(model_ckpt)  def tokenize(batch):     return tok(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)  dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"]) dataset = dataset.rename_column(\"label\", \"labels\") dataset.set_format(\"torch\") In\u00a0[15]: Copied! <pre>model = AutoModelForSequenceClassification.from_pretrained(\n    model_ckpt,\n    num_labels = num_labels,\n    problem_type = \"single_label_classification\"\n)\n</pre> model = AutoModelForSequenceClassification.from_pretrained(     model_ckpt,     num_labels = num_labels,     problem_type = \"single_label_classification\" ) <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[16]: Copied! <pre>metric_accuracy  = evaluate.load(\"accuracy\")\nmetric_precision = evaluate.load(\"precision\")\nmetric_recall    = evaluate.load(\"recall\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n\n    acc  = metric_accuracy.compute(predictions=preds, references=labels)\n    prec = metric_precision.compute(predictions=preds, references=labels, average=\"binary\")\n    rec  = metric_recall.compute(predictions=preds, references=labels, average=\"binary\")\n\n    # merge into a single dict that Trainer can log\n    return {\n        \"accuracy\":  acc[\"accuracy\"],\n        \"precision\": prec[\"precision\"],\n        \"recall\":    rec[\"recall\"],\n    }\n\n\nargs = TrainingArguments(\n    output_dir          = \"./bert_cls\",\n    eval_strategy       = \"epoch\",\n    save_strategy       = \"epoch\",\n    logging_strategy    = \"steps\",\n    logging_steps       = 50,\n    learning_rate       = 2e-5,\n    per_device_train_batch_size = batch_size,\n    per_device_eval_batch_size  = batch_size,\n    num_train_epochs    = 10,\n    weight_decay        = 0.01,\n    load_best_model_at_end = True,\n    metric_for_best_model = \"eval_loss\",\n    save_total_limit    = 2,\n    seed                = seed,\n    report_to           = \"none\",  # Disable wandb\n    fp16                = True, # Enable mixed precision training\n    gradient_accumulation_steps = 2\n)\n\ntrainer = Trainer(\n    model           = model,\n    args            = args,\n    train_dataset   = dataset[\"train\"],\n    eval_dataset    = dataset[\"test\"].shuffle(seed=seed),  # small held-out set for early-stop\n    compute_metrics = compute_metrics,\n    callbacks       = [EarlyStoppingCallback(early_stopping_patience=3)]\n)\n</pre> metric_accuracy  = evaluate.load(\"accuracy\") metric_precision = evaluate.load(\"precision\") metric_recall    = evaluate.load(\"recall\")  def compute_metrics(eval_pred):     logits, labels = eval_pred     preds = np.argmax(logits, axis=-1)      acc  = metric_accuracy.compute(predictions=preds, references=labels)     prec = metric_precision.compute(predictions=preds, references=labels, average=\"binary\")     rec  = metric_recall.compute(predictions=preds, references=labels, average=\"binary\")      # merge into a single dict that Trainer can log     return {         \"accuracy\":  acc[\"accuracy\"],         \"precision\": prec[\"precision\"],         \"recall\":    rec[\"recall\"],     }   args = TrainingArguments(     output_dir          = \"./bert_cls\",     eval_strategy       = \"epoch\",     save_strategy       = \"epoch\",     logging_strategy    = \"steps\",     logging_steps       = 50,     learning_rate       = 2e-5,     per_device_train_batch_size = batch_size,     per_device_eval_batch_size  = batch_size,     num_train_epochs    = 10,     weight_decay        = 0.01,     load_best_model_at_end = True,     metric_for_best_model = \"eval_loss\",     save_total_limit    = 2,     seed                = seed,     report_to           = \"none\",  # Disable wandb     fp16                = True, # Enable mixed precision training     gradient_accumulation_steps = 2 )  trainer = Trainer(     model           = model,     args            = args,     train_dataset   = dataset[\"train\"],     eval_dataset    = dataset[\"test\"].shuffle(seed=seed),  # small held-out set for early-stop     compute_metrics = compute_metrics,     callbacks       = [EarlyStoppingCallback(early_stopping_patience=3)] ) In\u00a0[17]: Copied! <pre>train_output = trainer.train()\n</pre> train_output = trainer.train()        [ 980/1950 06:19 &lt; 06:16, 2.58 it/s, Epoch 5/10]      Epoch Training Loss Validation Loss Accuracy Precision Recall 1 0.318100 0.288745 0.877800 0.850776 0.916320 2 0.227900 0.273267 0.887760 0.898209 0.874640 3 0.159500 0.303185 0.889200 0.878128 0.903840 4 0.093700 0.365608 0.882120 0.907238 0.851280 5 0.064300 0.425973 0.886440 0.869728 0.909040 In\u00a0[18]: Copied! <pre># Collect logs ---------------------------------------------------------------\nlogs = trainer.state.log_history\ntrain_loss, eval_loss, eval_acc, steps_eval, steps_train = [], [], [], [], []\n\nfor e in logs:\n    if 'loss' in e and e.get(\"epoch\") is not None:\n        train_loss.append(e['loss'])\n        steps_train.append(e['epoch'])\n    if 'eval_loss' in e:\n        eval_loss.append(e['eval_loss'])\n        eval_acc.append(e['eval_accuracy'])\n        steps_eval.append(e['epoch'])\n\n# Plot -----------------------------------------------------------------------\nplt.figure(figsize=(6,4))\nplt.plot(steps_train, train_loss, label=\"train-loss\")\nplt.plot(steps_eval, eval_loss, label=\"eval-loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"BERT Loss Curves\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.plot(steps_eval, eval_acc, label=\"eval-accuracy\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"BERT Accuracy Curve\")\nplt.show()\n</pre> # Collect logs --------------------------------------------------------------- logs = trainer.state.log_history train_loss, eval_loss, eval_acc, steps_eval, steps_train = [], [], [], [], []  for e in logs:     if 'loss' in e and e.get(\"epoch\") is not None:         train_loss.append(e['loss'])         steps_train.append(e['epoch'])     if 'eval_loss' in e:         eval_loss.append(e['eval_loss'])         eval_acc.append(e['eval_accuracy'])         steps_eval.append(e['epoch'])  # Plot ----------------------------------------------------------------------- plt.figure(figsize=(6,4)) plt.plot(steps_train, train_loss, label=\"train-loss\") plt.plot(steps_eval, eval_loss, label=\"eval-loss\") plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"BERT Loss Curves\") plt.show()  plt.figure(figsize=(6,4)) plt.plot(steps_eval, eval_acc, label=\"eval-accuracy\") plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"BERT Accuracy Curve\") plt.show()  In\u00a0[19]: Copied! <pre>test_metrics = trainer.evaluate(dataset[\"test\"])\nprint(test_metrics)\n</pre> test_metrics = trainer.evaluate(dataset[\"test\"]) print(test_metrics)        [  4/391 00:00 &lt; 00:17, 22.73 it/s]      <pre>{'eval_loss': 0.27326661348342896, 'eval_accuracy': 0.88776, 'eval_precision': 0.8982090042720999, 'eval_recall': 0.87464, 'eval_runtime': 17.5668, 'eval_samples_per_second': 1423.138, 'eval_steps_per_second': 22.258, 'epoch': 5.0}\n</pre> In\u00a0[20]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\n# Calculate metrics for LogReg (TF-IDF)\nlogreg_accuracy = accuracy_score(y_test, y_pred_logreg)\nlogreg_precision = precision_score(y_test, y_pred_logreg, average='weighted')\nlogreg_recall = recall_score(y_test, y_pred_logreg, average='weighted')\nlogreg_f1 = f1_score(y_test, y_pred_logreg, average='weighted')\n\n# Calculate metrics for LSTM\nlstm_accuracy = accuracy_score(y_test_lstm, y_pred_lstm)\nlstm_precision = precision_score(y_test_lstm, y_pred_lstm, average='weighted')\nlstm_recall = recall_score(y_test_lstm, y_pred_lstm, average='weighted')\nlstm_f1 = f1_score(y_test_lstm, y_pred_lstm, average='weighted')\n\n# Extract BERT metrics from test_metrics (assuming this structure)\n# If your test_metrics has a different structure, adjust accordingly\nbert_accuracy = test_metrics.get('eval_accuracy', 0)\nbert_precision = test_metrics.get('eval_precision', 0)\nbert_recall = test_metrics.get('eval_recall', 0)\nbert_f1 = test_metrics.get('eval_f1', 2 * bert_recall * bert_precision / (bert_recall + bert_precision))\n\n# Create a DataFrame for easy plotting\nmodels = ['TF-IDF+LogReg', 'LSTM', 'BERT']\nmetrics_df = pd.DataFrame({\n    'Accuracy': [logreg_accuracy, lstm_accuracy, bert_accuracy],\n    'Precision': [logreg_precision, lstm_precision, bert_precision],\n    'Recall': [logreg_recall, lstm_recall, bert_recall],\n    'F1-Score': [logreg_f1, lstm_f1, bert_f1]\n}, index=models)\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 6), sharey=True)\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n\n# Colors for each model\ncolors = ['#3498db', '#2ecc71', '#e74c3c']  # Blue, Green, Red\n\n# Plot each metric in a separate subplot\nfor i, metric in enumerate(metrics):\n    ax = axes[i]\n    bars = ax.bar(models, metrics_df[metric], color=colors, alpha=0.8)\n    \n    # Add value labels on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n    \n    # Set title and customize\n    ax.set_title(metric, fontsize=14)\n    ax.set_ylim(0, 1.05)\n    ax.grid(axis='y', alpha=0.3)\n    \n    # Only show y-axis label on the first subplot\n    if i == 0:\n        ax.set_ylabel('Score', fontsize=12)\n    \n    # Rotate x-axis labels to avoid overlap\n    plt.setp(ax.get_xticklabels(), rotation=30, ha='right')\n\n# Add a common legend\nfig.legend(\n    [plt.Rectangle((0,0),1,1, color=color) for color in colors],\n    models,\n    loc='upper center', \n    bbox_to_anchor=(0.5, 0.05),\n    ncol=3,\n    fontsize=12\n)\n\nplt.suptitle('Performance Comparison of Text Classification Models', fontsize=16)\nplt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjust layout to accommodate the legend\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score   # Calculate metrics for LogReg (TF-IDF) logreg_accuracy = accuracy_score(y_test, y_pred_logreg) logreg_precision = precision_score(y_test, y_pred_logreg, average='weighted') logreg_recall = recall_score(y_test, y_pred_logreg, average='weighted') logreg_f1 = f1_score(y_test, y_pred_logreg, average='weighted')  # Calculate metrics for LSTM lstm_accuracy = accuracy_score(y_test_lstm, y_pred_lstm) lstm_precision = precision_score(y_test_lstm, y_pred_lstm, average='weighted') lstm_recall = recall_score(y_test_lstm, y_pred_lstm, average='weighted') lstm_f1 = f1_score(y_test_lstm, y_pred_lstm, average='weighted')  # Extract BERT metrics from test_metrics (assuming this structure) # If your test_metrics has a different structure, adjust accordingly bert_accuracy = test_metrics.get('eval_accuracy', 0) bert_precision = test_metrics.get('eval_precision', 0) bert_recall = test_metrics.get('eval_recall', 0) bert_f1 = test_metrics.get('eval_f1', 2 * bert_recall * bert_precision / (bert_recall + bert_precision))  # Create a DataFrame for easy plotting models = ['TF-IDF+LogReg', 'LSTM', 'BERT'] metrics_df = pd.DataFrame({     'Accuracy': [logreg_accuracy, lstm_accuracy, bert_accuracy],     'Precision': [logreg_precision, lstm_precision, bert_precision],     'Recall': [logreg_recall, lstm_recall, bert_recall],     'F1-Score': [logreg_f1, lstm_f1, bert_f1] }, index=models)  fig, axes = plt.subplots(1, 4, figsize=(16, 6), sharey=True) metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']  # Colors for each model colors = ['#3498db', '#2ecc71', '#e74c3c']  # Blue, Green, Red  # Plot each metric in a separate subplot for i, metric in enumerate(metrics):     ax = axes[i]     bars = ax.bar(models, metrics_df[metric], color=colors, alpha=0.8)          # Add value labels on top of bars     for bar in bars:         height = bar.get_height()         ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,                 f'{height:.3f}', ha='center', va='bottom', fontsize=9)          # Set title and customize     ax.set_title(metric, fontsize=14)     ax.set_ylim(0, 1.05)     ax.grid(axis='y', alpha=0.3)          # Only show y-axis label on the first subplot     if i == 0:         ax.set_ylabel('Score', fontsize=12)          # Rotate x-axis labels to avoid overlap     plt.setp(ax.get_xticklabels(), rotation=30, ha='right')  # Add a common legend fig.legend(     [plt.Rectangle((0,0),1,1, color=color) for color in colors],     models,     loc='upper center',      bbox_to_anchor=(0.5, 0.05),     ncol=3,     fontsize=12 )  plt.suptitle('Performance Comparison of Text Classification Models', fontsize=16) plt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjust layout to accommodate the legend plt.show() In\u00a0[21]: Copied! <pre>from transformers import AutoTokenizer, TFAutoModel\n\nseed              = 42\nmodel_name        = \"bert-base-uncased\"      # \u270f\ufe0f switch to any BERT-like ckpt\nmax_len           = 128\nbatch_size        = 64\nepochs            = 5\nlearning_rate     = 2e-5\n\ntf.keras.utils.set_random_seed(seed)\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbase_model = TFAutoModel.from_pretrained(model_name)\nbase_model.layers[0].trainable = False\n\nbase_model.summary()\n</pre> from transformers import AutoTokenizer, TFAutoModel  seed              = 42 model_name        = \"bert-base-uncased\"      # \u270f\ufe0f switch to any BERT-like ckpt max_len           = 128 batch_size        = 64 epochs            = 5 learning_rate     = 2e-5  tf.keras.utils.set_random_seed(seed)  # Load pre-trained BERT model and tokenizer tokenizer = AutoTokenizer.from_pretrained(model_name) base_model = TFAutoModel.from_pretrained(model_name) base_model.layers[0].trainable = False  base_model.summary() <pre>Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n</pre> <pre>Model: \"tf_bert_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bert (TFBertMainLayer)      multiple                  109482240 \n                                                                 \n=================================================================\nTotal params: 109482240 (417.64 MB)\nTrainable params: 0 (0.00 Byte)\nNon-trainable params: 109482240 (417.64 MB)\n_________________________________________________________________\n</pre> In\u00a0[22]: Copied! <pre># Simplest approach - use global pooling\ninput_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n\n# Get BERT outputs\nbert_outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\nsequence_output = bert_outputs.last_hidden_state\n# Classification head\nx = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(64, return_sequences=False)\n)(sequence_output)\nx = tf.keras.layers.Dropout(0.3)(x)\noutput = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel = tf.keras.Model(\n    inputs=[input_ids, attention_mask], \n    outputs=output\n)\n\n# Same compilation as before\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\nmodel.compile(\n    optimizer=optimizer, \n    loss=loss, \n    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\nmodel.summary()\n</pre> # Simplest approach - use global pooling input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids') attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')  # Get BERT outputs bert_outputs = base_model(input_ids=input_ids, attention_mask=attention_mask) sequence_output = bert_outputs.last_hidden_state # Classification head x = tf.keras.layers.Bidirectional(     tf.keras.layers.LSTM(64, return_sequences=False) )(sequence_output) x = tf.keras.layers.Dropout(0.3)(x) output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)  model = tf.keras.Model(     inputs=[input_ids, attention_mask],      outputs=output )  # Same compilation as before optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate) loss = tf.keras.losses.BinaryCrossentropy(from_logits=False) model.compile(     optimizer=optimizer,      loss=loss,      metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()] )  model.summary() <pre>Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_ids (InputLayer)      [(None, 128)]                0         []                            \n                                                                                                  \n attention_mask (InputLayer  [(None, 128)]                0         []                            \n )                                                                                                \n                                                                                                  \n tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n )                           ngAndCrossAttentions(last_   40         'attention_mask[0][0]']      \n                             hidden_state=(None, 128, 7                                           \n                             68),                                                                 \n                              pooler_output=(None, 768)                                           \n                             , past_key_values=None, hi                                           \n                             dden_states=None, attentio                                           \n                             ns=None, cross_attentions=                                           \n                             None)                                                                \n                                                                                                  \n bidirectional (Bidirection  (None, 128)                  426496    ['tf_bert_model[0][0]']       \n al)                                                                                              \n                                                                                                  \n dropout_37 (Dropout)        (None, 128)                  0         ['bidirectional[0][0]']       \n                                                                                                  \n dense (Dense)               (None, 1)                    129       ['dropout_37[0][0]']          \n                                                                                                  \n==================================================================================================\nTotal params: 109908865 (419.27 MB)\nTrainable params: 426625 (1.63 MB)\nNon-trainable params: 109482240 (417.64 MB)\n__________________________________________________________________________________________________\n</pre> In\u00a0[23]: Copied! <pre># Prepare data for Keras\nfrom datasets import load_dataset\nfrom transformers import DefaultDataCollator\n\n# Load the dataset - this gives you a DatasetDict with 'train' and 'test' splits\ndataset = load_dataset(\"imdb\", split={\"train\":\"train\", \"test\":\"test\"})\n\n# Create the data collator\ndata_collator = DefaultDataCollator(return_tensors=\"tf\")\n\n# First, tokenize the data\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"], \n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_len\n    )\n\n# Apply tokenization to both train and test sets\ntokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\ntokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)\n\n# Update to use the new recommended format\ntf_train_dataset = tokenized_train.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=128,\n)\n\ntf_test_dataset = tokenized_test.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=128,\n)\n</pre> # Prepare data for Keras from datasets import load_dataset from transformers import DefaultDataCollator  # Load the dataset - this gives you a DatasetDict with 'train' and 'test' splits dataset = load_dataset(\"imdb\", split={\"train\":\"train\", \"test\":\"test\"})  # Create the data collator data_collator = DefaultDataCollator(return_tensors=\"tf\")  # First, tokenize the data def tokenize_function(examples):     return tokenizer(         examples[\"text\"],          padding=\"max_length\",         truncation=True,         max_length=max_len     )  # Apply tokenization to both train and test sets tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True) tokenized_test = dataset[\"test\"].map(tokenize_function, batched=True)  # Update to use the new recommended format tf_train_dataset = tokenized_train.to_tf_dataset(     columns=[\"attention_mask\", \"input_ids\"],     label_cols=[\"labels\"],     shuffle=True,     collate_fn=data_collator,     batch_size=128, )  tf_test_dataset = tokenized_test.to_tf_dataset(     columns=[\"attention_mask\", \"input_ids\"],     label_cols=[\"labels\"],     shuffle=False,     collate_fn=data_collator,     batch_size=128, ) <pre>/home/ubuntu/.cache/pypoetry/virtualenvs/bse-nlp-V5dzOmtQ-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\nOld behaviour: columns=['a'], labels=['labels'] -&gt; (tf.Tensor, tf.Tensor)  \n             : columns='a', labels='labels' -&gt; (tf.Tensor, tf.Tensor)  \nNew behaviour: columns=['a'],labels=['labels'] -&gt; ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n             : columns='a', labels='labels' -&gt; (tf.Tensor, tf.Tensor) \n  warnings.warn(\n</pre> In\u00a0[24]: Copied! <pre># Custom callback for early stopping using tf.keras\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    tf_train_dataset,\n    epochs=10,\n    validation_data=tf_test_dataset,\n    callbacks=[early_stopping],\n    verbose=1\n)\n</pre> # Custom callback for early stopping using tf.keras early_stopping = tf.keras.callbacks.EarlyStopping(     monitor='val_loss',     patience=3,     restore_best_weights=True )  # Train the model history = model.fit(     tf_train_dataset,     epochs=10,     validation_data=tf_test_dataset,     callbacks=[early_stopping],     verbose=1 ) <pre>Epoch 1/10\n</pre> <pre>196/196 [==============================] - 753s 4s/step - loss: 0.6647 - accuracy: 0.5958 - precision_1: 0.6026 - recall_1: 0.5626 - val_loss: 0.6131 - val_accuracy: 0.6968 - val_precision_1: 0.6920 - val_recall_1: 0.7092\nEpoch 2/10\n196/196 [==============================] - 768s 4s/step - loss: 0.5829 - accuracy: 0.7106 - precision_1: 0.7159 - recall_1: 0.6984 - val_loss: 0.5196 - val_accuracy: 0.7547 - val_precision_1: 0.7349 - val_recall_1: 0.7967\nEpoch 3/10\n196/196 [==============================] - 719s 4s/step - loss: 0.4876 - accuracy: 0.7721 - precision_1: 0.7732 - recall_1: 0.7701 - val_loss: 0.4531 - val_accuracy: 0.7902 - val_precision_1: 0.7666 - val_recall_1: 0.8346\nEpoch 4/10\n196/196 [==============================] - 718s 4s/step - loss: 0.4498 - accuracy: 0.7943 - precision_1: 0.7948 - recall_1: 0.7935 - val_loss: 0.4340 - val_accuracy: 0.8008 - val_precision_1: 0.7756 - val_recall_1: 0.8465\nEpoch 5/10\n196/196 [==============================] - 717s 4s/step - loss: 0.4300 - accuracy: 0.8044 - precision_1: 0.8024 - recall_1: 0.8076 - val_loss: 0.4241 - val_accuracy: 0.8061 - val_precision_1: 0.7808 - val_recall_1: 0.8511\nEpoch 6/10\n196/196 [==============================] - 714s 4s/step - loss: 0.4223 - accuracy: 0.8100 - precision_1: 0.8085 - recall_1: 0.8126 - val_loss: 0.4140 - val_accuracy: 0.8107 - val_precision_1: 0.7918 - val_recall_1: 0.8430\nEpoch 7/10\n196/196 [==============================] - 719s 4s/step - loss: 0.4151 - accuracy: 0.8123 - precision_1: 0.8083 - recall_1: 0.8187 - val_loss: 0.4075 - val_accuracy: 0.8153 - val_precision_1: 0.7981 - val_recall_1: 0.8441\nEpoch 8/10\n196/196 [==============================] - 717s 4s/step - loss: 0.4076 - accuracy: 0.8137 - precision_1: 0.8117 - recall_1: 0.8169 - val_loss: 0.4020 - val_accuracy: 0.8180 - val_precision_1: 0.8042 - val_recall_1: 0.8406\nEpoch 9/10\n196/196 [==============================] - 714s 4s/step - loss: 0.4011 - accuracy: 0.8216 - precision_1: 0.8196 - recall_1: 0.8249 - val_loss: 0.3981 - val_accuracy: 0.8194 - val_precision_1: 0.8045 - val_recall_1: 0.8438\nEpoch 10/10\n196/196 [==============================] - 713s 4s/step - loss: 0.3933 - accuracy: 0.8246 - precision_1: 0.8226 - recall_1: 0.8278 - val_loss: 0.3945 - val_accuracy: 0.8208 - val_precision_1: 0.8037 - val_recall_1: 0.8490\n</pre> In\u00a0[26]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title('BERT Loss')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.legend()\nplt.title('BERT Accuracy')\nplt.show()\n</pre> plt.figure(figsize=(12,4)) plt.subplot(1,2,1) plt.plot(history.history['loss'], label='Train Loss') plt.plot(history.history['val_loss'], label='Val Loss') plt.legend() plt.title('BERT Loss')  plt.subplot(1,2,2) plt.plot(history.history['accuracy'], label='Train Acc') plt.plot(history.history['val_accuracy'], label='Val Acc') plt.legend() plt.title('BERT Accuracy') plt.show() In\u00a0[30]: Copied! <pre># Evaluate the model\ndef compute_metrics_keras(y_true, y_pred):\n    # Convert predictions to class labels if they're probabilities\n    if len(y_pred.shape) &gt; 1 and y_pred.shape[1] &gt; 1:\n        y_pred_classes = np.argmax(y_pred, axis=-1)\n    else:\n        # For binary classification with single output\n        y_pred_classes = (y_pred &gt; 0.5).astype(int).flatten()\n    \n    # Convert y_true to a regular numpy array if it's an iterator\n    if not isinstance(y_true, np.ndarray):\n        y_true = np.array(list(y_true))\n    \n    # Make sure y_true is flattened for binary classification\n    y_true = np.array(y_true).flatten()\n    \n    # Calculate metrics using sklearn instead of HF evaluate\n    accuracy = accuracy_score(y_true, y_pred_classes)\n    precision = precision_score(y_true, y_pred_classes, average=\"binary\", zero_division=0)\n    recall = recall_score(y_true, y_pred_classes, average=\"binary\", zero_division=0)\n    f1 = f1_score(y_true, y_pred_classes, average=\"binary\", zero_division=0)\n    \n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\ny_pred = model.predict(tf_test_dataset)\ny_true_array = np.array([y for _, y in tf_test_dataset.unbatch()])\n\nbert_rnn_metrics = compute_metrics_keras(y_true_array, y_pred)\nbert_rnn_accuracy = bert_rnn_metrics[\"accuracy\"]\nbert_rnn_precision = bert_rnn_metrics[\"precision\"]\nbert_rnn_recall = bert_rnn_metrics[\"recall\"]\nbert_rnn_f1 = bert_rnn_metrics[\"f1\"]\n</pre> # Evaluate the model def compute_metrics_keras(y_true, y_pred):     # Convert predictions to class labels if they're probabilities     if len(y_pred.shape) &gt; 1 and y_pred.shape[1] &gt; 1:         y_pred_classes = np.argmax(y_pred, axis=-1)     else:         # For binary classification with single output         y_pred_classes = (y_pred &gt; 0.5).astype(int).flatten()          # Convert y_true to a regular numpy array if it's an iterator     if not isinstance(y_true, np.ndarray):         y_true = np.array(list(y_true))          # Make sure y_true is flattened for binary classification     y_true = np.array(y_true).flatten()          # Calculate metrics using sklearn instead of HF evaluate     accuracy = accuracy_score(y_true, y_pred_classes)     precision = precision_score(y_true, y_pred_classes, average=\"binary\", zero_division=0)     recall = recall_score(y_true, y_pred_classes, average=\"binary\", zero_division=0)     f1 = f1_score(y_true, y_pred_classes, average=\"binary\", zero_division=0)          return {         \"accuracy\": accuracy,         \"precision\": precision,         \"recall\": recall,         \"f1\": f1     }  y_pred = model.predict(tf_test_dataset) y_true_array = np.array([y for _, y in tf_test_dataset.unbatch()])  bert_rnn_metrics = compute_metrics_keras(y_true_array, y_pred) bert_rnn_accuracy = bert_rnn_metrics[\"accuracy\"] bert_rnn_precision = bert_rnn_metrics[\"precision\"] bert_rnn_recall = bert_rnn_metrics[\"recall\"] bert_rnn_f1 = bert_rnn_metrics[\"f1\"] <pre>196/196 [==============================] - 331s 2s/step\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create a DataFrame for easy plotting - now with 4 models\nmodels = ['TF-IDF+LogReg', 'LSTM', 'BERT', 'BERT (frozen)+RNN']\nmetrics_df = pd.DataFrame({\n    'Accuracy': [logreg_accuracy, lstm_accuracy, bert_accuracy, bert_rnn_accuracy],\n    'Precision': [logreg_precision, lstm_precision, bert_precision, bert_rnn_precision],\n    'Recall': [logreg_recall, lstm_recall, bert_recall, bert_rnn_recall],\n    'F1-Score': [logreg_f1, lstm_f1, bert_f1, bert_rnn_f1]\n}, index=models)\n\n# Create visualization\nfig, axes = plt.subplots(1, 4, figsize=(18, 6), sharey=True)\nmetrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n\n# Colors for each model - add a fourth color\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']  # Blue, Green, Red, Purple\n\n# Plot each metric in a separate subplot\nfor i, metric in enumerate(metrics_list):\n    ax = axes[i]\n    bars = ax.bar(models, metrics_df[metric], color=colors, alpha=0.8)\n    \n    # Add value labels on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n    \n    # Set title and customize\n    ax.set_title(metric, fontsize=14)\n    ax.set_ylim(0, 1.05)\n    ax.grid(axis='y', alpha=0.3)\n    \n    # Only show y-axis label on the first subplot\n    if i == 0:\n        ax.set_ylabel('Score', fontsize=12)\n    \n    # Rotate x-axis labels to avoid overlap\n    plt.setp(ax.get_xticklabels(), rotation=30, ha='right')\n\n# Add a common legend\nfig.legend(\n    [plt.Rectangle((0,0),1,1, color=color) for color in colors],\n    models,\n    loc='upper center', \n    bbox_to_anchor=(0.5, 0.05),\n    ncol=4,\n    fontsize=12\n)\n\nplt.suptitle('Performance Comparison of Text Classification Models', fontsize=16)\nplt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjust layout to accommodate the legend\nplt.show()\n\n# Print the metrics table for reference\nprint(\"Performance Metrics Comparison:\")\nprint(metrics_df.round(4))\n</pre> # Create a DataFrame for easy plotting - now with 4 models models = ['TF-IDF+LogReg', 'LSTM', 'BERT', 'BERT (frozen)+RNN'] metrics_df = pd.DataFrame({     'Accuracy': [logreg_accuracy, lstm_accuracy, bert_accuracy, bert_rnn_accuracy],     'Precision': [logreg_precision, lstm_precision, bert_precision, bert_rnn_precision],     'Recall': [logreg_recall, lstm_recall, bert_recall, bert_rnn_recall],     'F1-Score': [logreg_f1, lstm_f1, bert_f1, bert_rnn_f1] }, index=models)  # Create visualization fig, axes = plt.subplots(1, 4, figsize=(18, 6), sharey=True) metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score']  # Colors for each model - add a fourth color colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']  # Blue, Green, Red, Purple  # Plot each metric in a separate subplot for i, metric in enumerate(metrics_list):     ax = axes[i]     bars = ax.bar(models, metrics_df[metric], color=colors, alpha=0.8)          # Add value labels on top of bars     for bar in bars:         height = bar.get_height()         ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,                 f'{height:.3f}', ha='center', va='bottom', fontsize=9)          # Set title and customize     ax.set_title(metric, fontsize=14)     ax.set_ylim(0, 1.05)     ax.grid(axis='y', alpha=0.3)          # Only show y-axis label on the first subplot     if i == 0:         ax.set_ylabel('Score', fontsize=12)          # Rotate x-axis labels to avoid overlap     plt.setp(ax.get_xticklabels(), rotation=30, ha='right')  # Add a common legend fig.legend(     [plt.Rectangle((0,0),1,1, color=color) for color in colors],     models,     loc='upper center',      bbox_to_anchor=(0.5, 0.05),     ncol=4,     fontsize=12 )  plt.suptitle('Performance Comparison of Text Classification Models', fontsize=16) plt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjust layout to accommodate the legend plt.show()  # Print the metrics table for reference print(\"Performance Metrics Comparison:\") print(metrics_df.round(4)) <pre>Performance Metrics Comparison:\n               Accuracy  Precision  Recall  F1-Score\nTF-IDF+LogReg    0.8802     0.8802  0.8802    0.8802\nLSTM             0.7789     0.7789  0.7789    0.7789\nBERT             0.8878     0.8982  0.8746    0.8863\nBERT+RNN         0.8208     0.8037  0.8490    0.8257\n</pre> <p>Now let's see the results when we resume finetuning with the whole weights</p> In\u00a0[\u00a0]: Copied! <pre>for layer in model.layers:\n    layer.trainable = True\n\n# Custom callback for early stopping using tf.keras\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    tf_train_dataset,\n    epochs=10,\n    validation_data=tf_test_dataset,\n    callbacks=[early_stopping],\n    verbose=1\n)\n</pre> for layer in model.layers:     layer.trainable = True  # Custom callback for early stopping using tf.keras early_stopping = tf.keras.callbacks.EarlyStopping(     monitor='val_loss',     patience=3,     restore_best_weights=True )  # Train the model history = model.fit(     tf_train_dataset,     epochs=10,     validation_data=tf_test_dataset,     callbacks=[early_stopping],     verbose=1 ) <pre>Epoch 1/10\n</pre> <pre>196/196 [==============================] - ETA: 0s - loss: 0.3911 - accuracy: 0.8228 - precision_1: 0.8226 - recall_1: 0.8230</pre> In\u00a0[1]: Copied! <pre>plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.legend()\nplt.title('BERT unfrozen+RNN Loss')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.legend()\nplt.title('BERT unfrozen+RNN Accuracy')\nplt.show()\n</pre> plt.figure(figsize=(12,4)) plt.subplot(1,2,1) plt.plot(history.history['loss'], label='Train Loss') plt.plot(history.history['val_loss'], label='Val Loss') plt.legend() plt.title('BERT unfrozen+RNN Loss')  plt.subplot(1,2,2) plt.plot(history.history['accuracy'], label='Train Acc') plt.plot(history.history['val_accuracy'], label='Val Acc') plt.legend() plt.title('BERT unfrozen+RNN Accuracy') plt.show() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 plt.figure(figsize=(12,4))\n      2 plt.subplot(1,2,1)\n      3 plt.plot(history.history['loss'], label='Train Loss')\n\nNameError: name 'plt' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>y_pred = model.predict(tf_test_dataset)\ny_true_array = np.array([y for _, y in tf_test_dataset.unbatch()])\n\nbert_unfrozen_rnn_metrics = compute_metrics_keras(y_true_array, y_pred)\nbert_unfrozen_rnn_accuracy = bert_unfrozen_rnn_metrics[\"accuracy\"]\nbert_unfrozen_rnn_precision = bert_unfrozen_rnn_metrics[\"precision\"]\nbert_unfrozen_rnn_recall = bert_unfrozen_rnn_metrics[\"recall\"]\nbert_unfrozen_rnn_f1 = bert_unfrozen_rnn_metrics[\"f1\"]\n</pre> y_pred = model.predict(tf_test_dataset) y_true_array = np.array([y for _, y in tf_test_dataset.unbatch()])  bert_unfrozen_rnn_metrics = compute_metrics_keras(y_true_array, y_pred) bert_unfrozen_rnn_accuracy = bert_unfrozen_rnn_metrics[\"accuracy\"] bert_unfrozen_rnn_precision = bert_unfrozen_rnn_metrics[\"precision\"] bert_unfrozen_rnn_recall = bert_unfrozen_rnn_metrics[\"recall\"] bert_unfrozen_rnn_f1 = bert_unfrozen_rnn_metrics[\"f1\"] In\u00a0[\u00a0]: Copied! <pre># Create a DataFrame for easy plotting - now with 4 models\nmodels = ['TF-IDF+LogReg', 'LSTM', 'BERT', 'BERT (frozen)+RNN', 'BERT (unfrozen)+RNN']\nmetrics_df = pd.DataFrame({\n    'Accuracy': [logreg_accuracy, lstm_accuracy, bert_accuracy, bert_rnn_accuracy, bert_unfrozen_rnn_accuracy],\n    'Precision': [logreg_precision, lstm_precision, bert_precision, bert_rnn_precision, bert_unfrozen_rnn_precision],\n    'Recall': [logreg_recall, lstm_recall, bert_recall, bert_rnn_recall, bert_unfrozen_rnn_recall],\n    'F1-Score': [logreg_f1, lstm_f1, bert_f1, bert_rnn_f1, bert_unfrozen_rnn_f1]\n}, index=models)\n\n# Create visualization\nfig, axes = plt.subplots(1, 4, figsize=(18, 6), sharey=True)\nmetrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n\n# Colors for each model - add a fourth color\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f1c40f']  # Blue, Green, Red, Purple, Yellow\n\n# Plot each metric in a separate subplot\nfor i, metric in enumerate(metrics_list):\n    ax = axes[i]\n    bars = ax.bar(models, metrics_df[metric], color=colors, alpha=0.8)\n    \n    # Add value labels on top of bars\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n    \n    # Set title and customize\n    ax.set_title(metric, fontsize=14)\n    ax.set_ylim(0, 1.05)\n    ax.grid(axis='y', alpha=0.3)\n    \n    # Only show y-axis label on the first subplot\n    if i == 0:\n        ax.set_ylabel('Score', fontsize=12)\n    \n    # Rotate x-axis labels to avoid overlap\n    plt.setp(ax.get_xticklabels(), rotation=30, ha='right')\n\n# Add a common legend\nfig.legend(\n    [plt.Rectangle((0,0),1,1, color=color) for color in colors],\n    models,\n    loc='upper center', \n    bbox_to_anchor=(0.5, 0.05),\n    ncol=4,\n    fontsize=12\n)\n\nplt.suptitle('Performance Comparison of Text Classification Models', fontsize=16)\nplt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjust layout to accommodate the legend\nplt.show()\n\n# Print the metrics table for reference\nprint(\"Performance Metrics Comparison:\")\nprint(metrics_df.round(4))\n</pre> # Create a DataFrame for easy plotting - now with 4 models models = ['TF-IDF+LogReg', 'LSTM', 'BERT', 'BERT (frozen)+RNN', 'BERT (unfrozen)+RNN'] metrics_df = pd.DataFrame({     'Accuracy': [logreg_accuracy, lstm_accuracy, bert_accuracy, bert_rnn_accuracy, bert_unfrozen_rnn_accuracy],     'Precision': [logreg_precision, lstm_precision, bert_precision, bert_rnn_precision, bert_unfrozen_rnn_precision],     'Recall': [logreg_recall, lstm_recall, bert_recall, bert_rnn_recall, bert_unfrozen_rnn_recall],     'F1-Score': [logreg_f1, lstm_f1, bert_f1, bert_rnn_f1, bert_unfrozen_rnn_f1] }, index=models)  # Create visualization fig, axes = plt.subplots(1, 4, figsize=(18, 6), sharey=True) metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score']  # Colors for each model - add a fourth color colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f1c40f']  # Blue, Green, Red, Purple, Yellow  # Plot each metric in a separate subplot for i, metric in enumerate(metrics_list):     ax = axes[i]     bars = ax.bar(models, metrics_df[metric], color=colors, alpha=0.8)          # Add value labels on top of bars     for bar in bars:         height = bar.get_height()         ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,                 f'{height:.3f}', ha='center', va='bottom', fontsize=9)          # Set title and customize     ax.set_title(metric, fontsize=14)     ax.set_ylim(0, 1.05)     ax.grid(axis='y', alpha=0.3)          # Only show y-axis label on the first subplot     if i == 0:         ax.set_ylabel('Score', fontsize=12)          # Rotate x-axis labels to avoid overlap     plt.setp(ax.get_xticklabels(), rotation=30, ha='right')  # Add a common legend fig.legend(     [plt.Rectangle((0,0),1,1, color=color) for color in colors],     models,     loc='upper center',      bbox_to_anchor=(0.5, 0.05),     ncol=4,     fontsize=12 )  plt.suptitle('Performance Comparison of Text Classification Models', fontsize=16) plt.tight_layout(rect=[0, 0.1, 1, 0.95])  # Adjust layout to accommodate the legend plt.show()  # Print the metrics table for reference print(\"Performance Metrics Comparison:\") print(metrics_df.round(4)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#bert-implementation-with-hf-vs-lstm-text-classification-vs-tf-idf-logistic-regression","title":"BERT Implementation with HF vs LSTM Text Classification vs. TF-IDF + Logistic Regression\u00b6","text":"<p>In this notebook, we compare three different approaches to text classification:</p> <ol> <li>Logistic Regression on top of TF-IDF features</li> <li>LSTM (using Keras) trained end-to-end</li> <li>BERT (using Hugging Face) trained end-to-end</li> </ol> <p>We'll explore:</p> <ul> <li>The architectures for each approach</li> <li>Their respective training processes</li> <li>Visualization of results and metrics</li> <li>How each model interprets words in the final decision</li> <li>Potential improvements for the BERT model</li> </ul>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#1-environment-setup-imports","title":"1. Environment Setup &amp; Imports\u00b6","text":"<p>We'll install any dependencies (if needed) and import the necessary libraries.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#2-data-loading-preprocessing","title":"2. Data Loading &amp; Preprocessing\u00b6","text":"<p>For demonstration, we'll assume we have a text classification dataset named <code>df</code>. It contains two columns:</p> <ul> <li><code>text</code> : The text field</li> <li><code>label</code>: The target label (0 or 1, for binary classification, or more classes)</li> </ul> <p>We'll split the data into train and test sets, then further create a small dev set if needed.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#3-tf-idf-logistic-regression","title":"3. TF-IDF + Logistic Regression\u00b6","text":"<p>We'll:</p> <ol> <li>Vectorize our text using TF-IDF.</li> <li>Train a Logistic Regression model.</li> <li>Evaluate on the test set.</li> <li>Visualize results.</li> </ol>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#visualization-of-confusion-matrix","title":"Visualization of Confusion Matrix\u00b6","text":""},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#4-lstm-classification","title":"4: LSTM Classification\u00b6","text":"<p>For the whole explanation on the LSTM classification, please refer to the Session 4.</p> <p>Here we will directly train the model.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#41-preprocessing-the-text-for-lstm","title":"4.1 Preprocessing the Text for LSTM\u00b6","text":""},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#42-build-the-lstm-model","title":"4.2 Build the LSTM Model\u00b6","text":"<p>Now that we have our sequences ready and padded, we can build an LSTM-based neural network using Keras.</p> <p>We\u2019ll use a simple architecture:</p> <ul> <li>An Embedding layer to learn word representations.</li> <li>A single LSTM layer to process the sequence.</li> <li>A Dense layer with a sigmoid activation to output binary predictions.</li> </ul> <p>The optimizer is Adam with a learning rate of 0.01.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#43-train-the-lstm-model","title":"4.3 Train the LSTM Model\u00b6","text":"<p>Now we train the model using our preprocessed sequences and binary labels.</p> <p>We use:</p> <ul> <li><code>validation_split=0.2</code>: to hold out 20% of training data for validation.</li> <li><code>epochs=10</code>: to iterate 10 times over the dataset.</li> <li><code>batch_size=128</code>: A high batch size to speed up training and also to reduce overfitting.</li> </ul> <p>Note that LSTMs are more computationally expensive than logistic regression, so training can take longer \u2014 especially with small batches.</p> <p>To control the overfitting we can use the callback <code>EarlyStopping</code> to stop the training if the validation loss does not improve for 3 epochs.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#44-training-curves","title":"4.4 Training Curves\u00b6","text":"<p>Let's visualize the training and validation accuracy/loss.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#45-evaluate-on-test-data","title":"4.5 Evaluate on Test Data\u00b6","text":""},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#5-bert-text-classification","title":"5 : BERT (Text Classification)\u00b6","text":"<p>In this section we'll fine-tune BERT (Base-uncased) \u2013 a bidirectional transformer that already knows an enormous amount about language from pre-training on Wikipedia + Books.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#why-move-from-lstm-to-bert","title":"\ud83e\udd14 Why Move From LSTM to BERT?\u00b6","text":"LSTM + Embeddings BERT (Transformer) Context Window Limited, sequential Global (self-attention) Pre-training Usually random embeddings 110 M parameters pre-trained on ~3 B words Handles Long Ranges? Struggles Much better (all-to-all attention) Training Data Needed More (from scratch) Less (only fine-tune) Compute / Memory Lower Higher <p>Key idea: BERT already understands syntax, semantics, and even some world-knowledg. Fine-tuning adapts that knowledge to your downstream task (e.g. sentiment, toxicity).</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#51-pre-processing-tokenizer","title":"5.1 Pre-Processing (Tokenizer)\u00b6","text":"<p>BERT can't ingest raw text \u2013 we first:</p> <ol> <li>Tokenize into WordPieces (e.g. \u201c<code>playing</code>\u201d \u2192 \u201c<code>play</code>\u201d, \u201c<code>##ing</code>\u201d).</li> <li>Map to integer IDs from BERT\u2019s 30 522-wordpiece vocabulary.</li> <li>Pad/Truncate every sequence to <code>max_length</code>.</li> </ol> <p>We\u2019ll wrap this in a small function so \ud83e\udd17 <code>Dataset.map</code> can apply it in parallel.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#52-build-the-bert-model","title":"5.2 Build the BERT Model\u00b6","text":"<p>We\u2019ll fine-tune <code>BertForSequenceClassification</code>:</p> <ul> <li>An embedding + 12 Transformer layers share weights with the pre-training check-point.</li> <li>A fresh classification head (dense layer) is added on top.</li> </ul>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#53-training-set-up-trainer-early-stopping","title":"5.3 Training (set up <code>Trainer</code> + Early Stopping)\u00b6","text":"<p>We use \ud83e\udd17 <code>Trainer</code> which handles:</p> <ul> <li>Optimiser (AdamW), LR-schedule, mixed-precision, etc.</li> <li>Metrics logging every eval-steps.</li> <li>Callbacks \u2013 we\u2019ll plug in <code>EarlyStoppingCallback</code> to halt training if <code>eval_loss</code> hasn\u2019t improved for 3 checkpoints.</li> </ul> <p><code>TrainingArguments</code> govern the run; tweak learning-rate, epochs, batch-size, etc.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#54-train-the-model","title":"5.4 Train the Model \ud83d\ude80\u00b6","text":"<p>Just one line of code !</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#55-training-curves","title":"5.5 Training Curves \ud83d\udcc8\u00b6","text":"<p>Let\u2019s plot loss &amp; accuracy logged in <code>trainer.state.log_history</code>. We\u2019ll visualise train vs eval so you can spot over-fitting at a glance.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#56-final-evaluation-on-the-test-set","title":"5.6 Final Evaluation (on the Test Set)\u00b6","text":"<p>We now evaluate on the full test split that was never touched during training.</p>"},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#57-compare-with-other-models","title":"5.7 Compare with other models\u00b6","text":""},{"location":"chapter5/Session_5_1_BERT_HF_Implementation/#bert-vs-others-quick-recap","title":"\u2705 BERT vs Others \u2014 Quick Recap\u00b6","text":"Model Accuracy (test) Training Time Params (trainable) Notes TF-IDF + Logistic Reg. ~0.88 \u23f1\ufe0f seconds 20 K Strong baseline LSTM + Embeddings ~0.78 \u23f1\ufe0f minutes 600k Needs careful tuning Fine-tuned BERT ~0.93 \u23f1\ufe0f ~5-10 min ~110 M (but only a few % heavily updated) State-of-the-art context understanding <p>Takeaway: BERT\u2019s pre-training gives it a massive head-start, letting you reach higher accuracy with (often) less labelled data and fewer epochs than an LSTM trained from scratch. Early-stopping keeps us from over-fitting, while plotting loss/accuracy curves helps diagnose training health \ud83e\ude7a.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/","title":"Exploring BERT Attention: From Interpretation to Fine-Tuning","text":"In\u00a0[1]: Copied! <pre># Step 1: Load BERT model and tokenizer (xtremedistil for speed)\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch\n\n# Load tokenizer and model\nmodel_name = \"microsoft/xtremedistil-l6-h256-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nconfig = AutoConfig.from_pretrained(model_name, output_attentions=True)\nmodel = AutoModel.from_pretrained(model_name, config=config)\n\nmodel.eval()\n</pre> # Step 1: Load BERT model and tokenizer (xtremedistil for speed)  from transformers import AutoTokenizer, AutoModel, AutoConfig import torch  # Load tokenizer and model model_name = \"microsoft/xtremedistil-l6-h256-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_name) config = AutoConfig.from_pretrained(model_name, output_attentions=True) model = AutoModel.from_pretrained(model_name, config=config)  model.eval() Out[1]: <pre>BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 256, padding_idx=0)\n    (position_embeddings): Embedding(512, 256)\n    (token_type_embeddings): Embedding(2, 256)\n    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-5): 6 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=256, out_features=256, bias=True)\n    (activation): Tanh()\n  )\n)</pre> In\u00a0[2]: Copied! <pre>sentence = \"The movie was surprisingly good, I really enjoyed it!\"\ninputs = tokenizer(sentence, return_tensors=\"pt\")\n\n# Forward pass with attentions\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extract attention weights\n# shape: (num_layers, batch, num_heads, seq_len, seq_len)\nattentions = torch.stack(outputs.attentions)  # 6 layers\nattentions.shape\n</pre> sentence = \"The movie was surprisingly good, I really enjoyed it!\" inputs = tokenizer(sentence, return_tensors=\"pt\")  # Forward pass with attentions with torch.no_grad():     outputs = model(**inputs)  # Extract attention weights # shape: (num_layers, batch, num_heads, seq_len, seq_len) attentions = torch.stack(outputs.attentions)  # 6 layers attentions.shape <pre>BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n</pre> Out[2]: <pre>torch.Size([6, 1, 8, 13, 13])</pre> In\u00a0[3]: Copied! <pre># Unpack attention tensor dimensions\nnum_layers, batch_size, num_heads, seq_len, _ = attentions.shape\nprint(f\"Layers: {num_layers}, Heads: {num_heads}, Tokens: {seq_len}\")\n\n# Decode tokens\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\nprint(\"\\nBERT tokens:\")\nprint(tokens)\n</pre> # Unpack attention tensor dimensions num_layers, batch_size, num_heads, seq_len, _ = attentions.shape print(f\"Layers: {num_layers}, Heads: {num_heads}, Tokens: {seq_len}\")  # Decode tokens tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) print(\"\\nBERT tokens:\") print(tokens) <pre>Layers: 6, Heads: 8, Tokens: 13\n\nBERT tokens:\n['[CLS]', 'the', 'movie', 'was', 'surprisingly', 'good', ',', 'i', 'really', 'enjoyed', 'it', '!', '[SEP]']\n</pre> In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport numpy as np\nfrom ipywidgets import interact, IntSlider\n\ndef extract_attention_by_layers(attention_matrix):\n    \"\"\"Extract attention into a dictionary by layers.\"\"\"\n    num_layers = attention_matrix.shape[0]\n    attention_by_layer = {}\n    \n    for layer in range(num_layers):\n        # Store all heads for this layer\n        attention_by_layer[f\"layer_{layer}\"] = attention_matrix[layer].detach().clone()\n    \n    return attention_by_layer\n\ndef plot_all_heads_in_layer(tokens, attention_matrix, layer=0, num_heads=None):\n    if num_heads is None:\n        num_heads = attention_matrix.shape[2]  # Get number of heads\n    \n    fig, axes = plt.subplots(nrows=int(np.ceil(num_heads/4)), ncols=min(4, num_heads), \n                           figsize=(16, 3*int(np.ceil(num_heads/4))))\n    axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n    \n    for head_idx in range(num_heads):\n        attn = attention_matrix[layer, 0, head_idx].detach().numpy()\n        ax = axes[head_idx]\n        sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens, \n                   cmap=\"viridis\", ax=ax, cbar=False)\n        ax.set_title(f\"Head {head_idx}\")\n        ax.set_xlabel(\"\")\n        ax.set_ylabel(\"\")\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n    \n    # Add a common colorbar\n    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n    fig.colorbar(axes[0].collections[0], cax=cbar_ax)\n    \n    plt.suptitle(f\"All Attention Heads in Layer {layer}\", fontsize=16)\n    plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n    return fig\n\ndef interactive_layer_attention(tokens, attention_matrix):\n    num_layers = attention_matrix.shape[0]\n    \n    # Create dictionary with attention by layers\n    attention_by_layer = extract_attention_by_layers(attention_matrix)\n    print(f\"Created dictionary with {len(attention_by_layer)} layers\")\n    \n    @interact(layer=IntSlider(min=0, max=num_layers-1, step=1, value=0))\n    def update(layer):\n        plot_all_heads_in_layer(tokens, attention_matrix, layer)\n        \n    return attention_by_layer\n\n# Usage:\nattention_dict = interactive_layer_attention(tokens, attentions)\n</pre> import matplotlib.pyplot as plt import seaborn as sns import torch import numpy as np from ipywidgets import interact, IntSlider  def extract_attention_by_layers(attention_matrix):     \"\"\"Extract attention into a dictionary by layers.\"\"\"     num_layers = attention_matrix.shape[0]     attention_by_layer = {}          for layer in range(num_layers):         # Store all heads for this layer         attention_by_layer[f\"layer_{layer}\"] = attention_matrix[layer].detach().clone()          return attention_by_layer  def plot_all_heads_in_layer(tokens, attention_matrix, layer=0, num_heads=None):     if num_heads is None:         num_heads = attention_matrix.shape[2]  # Get number of heads          fig, axes = plt.subplots(nrows=int(np.ceil(num_heads/4)), ncols=min(4, num_heads),                             figsize=(16, 3*int(np.ceil(num_heads/4))))     axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]          for head_idx in range(num_heads):         attn = attention_matrix[layer, 0, head_idx].detach().numpy()         ax = axes[head_idx]         sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens,                     cmap=\"viridis\", ax=ax, cbar=False)         ax.set_title(f\"Head {head_idx}\")         ax.set_xlabel(\"\")         ax.set_ylabel(\"\")         ax.set_xticklabels(ax.get_xticklabels(), rotation=90)         ax.set_yticklabels(ax.get_yticklabels(), rotation=0)          # Add a common colorbar     cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])     fig.colorbar(axes[0].collections[0], cax=cbar_ax)          plt.suptitle(f\"All Attention Heads in Layer {layer}\", fontsize=16)     plt.tight_layout(rect=[0, 0, 0.9, 0.95])     return fig  def interactive_layer_attention(tokens, attention_matrix):     num_layers = attention_matrix.shape[0]          # Create dictionary with attention by layers     attention_by_layer = extract_attention_by_layers(attention_matrix)     print(f\"Created dictionary with {len(attention_by_layer)} layers\")          @interact(layer=IntSlider(min=0, max=num_layers-1, step=1, value=0))     def update(layer):         plot_all_heads_in_layer(tokens, attention_matrix, layer)              return attention_by_layer  # Usage: attention_dict = interactive_layer_attention(tokens, attentions) <pre>Created dictionary with 6 layers\n</pre> <pre>interactive(children=(IntSlider(value=0, description='layer', max=5), Output()), _dom_classes=('widget-interac\u2026</pre> In\u00a0[5]: Copied! <pre># Step 1: Load IMDB dataset\n\nfrom datasets import load_dataset\n\n# We'll just use a small portion to keep things quick and interpretable\nimdb = load_dataset(\"imdb\")\nsmall_train = imdb[\"train\"].shuffle(seed=42).select(range(2000))\nsmall_test = imdb[\"test\"].shuffle(seed=42).select(range(500))\n\n# Display a sample\nprint(small_train[0])\n</pre> # Step 1: Load IMDB dataset  from datasets import load_dataset  # We'll just use a small portion to keep things quick and interpretable imdb = load_dataset(\"imdb\") small_train = imdb[\"train\"].shuffle(seed=42).select(range(2000)) small_test = imdb[\"test\"].shuffle(seed=42).select(range(500))  # Display a sample print(small_train[0]) <pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n</pre> <pre>{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...', 'label': 1}\n</pre> In\u00a0[6]: Copied! <pre># Step 2: Tokenization &amp; Dataloader setup\n\nfrom transformers import DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\n\ndef tokenize_fn(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=64)\n\n# Apply tokenization\ntokenized_train = small_train.map(tokenize_fn, batched=True)\ntokenized_test  = small_test.map(tokenize_fn, batched=True)\n\n# Set format for PyTorch\ntokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# Dynamic padding for batches\ncollator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n\n# Dataloaders\ntrain_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True, collate_fn=collator)\ntest_loader  = DataLoader(tokenized_test, batch_size=16, shuffle=False, collate_fn=collator)\n\n# Peek at one batch\nbatch = next(iter(train_loader))\nprint({k: v.shape for k, v in batch.items()})\n</pre> # Step 2: Tokenization &amp; Dataloader setup  from transformers import DataCollatorWithPadding from torch.utils.data import DataLoader  def tokenize_fn(batch):     return tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=64)  # Apply tokenization tokenized_train = small_train.map(tokenize_fn, batched=True) tokenized_test  = small_test.map(tokenize_fn, batched=True)  # Set format for PyTorch tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]) tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])  # Dynamic padding for batches collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")  # Dataloaders train_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True, collate_fn=collator) test_loader  = DataLoader(tokenized_test, batch_size=16, shuffle=False, collate_fn=collator)  # Peek at one batch batch = next(iter(train_loader)) print({k: v.shape for k, v in batch.items()}) <pre>{'input_ids': torch.Size([16, 64]), 'attention_mask': torch.Size([16, 64]), 'labels': torch.Size([16])}\n</pre> In\u00a0[7]: Copied! <pre># Step 3: Custom model that freezes all but last attention layer\n\nfrom transformers import AutoModel\nimport torch.nn as nn\n\nclass BertWithLastLayerAttentionClassifier(nn.Module):\n    def __init__(self, model_name, num_classes=2):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained(model_name, output_attentions=True)\n        \n        # Freeze all layers except the last encoder layer\n        for name, param in self.bert.named_parameters():\n            # Only unfreeze Layer 5 and classifier\n            if \"encoder.layer.5\" not in name and \"pooler\" not in name:\n                param.requires_grad = False\n        \n        # Classification head on [CLS]\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        # Get hidden states and attention maps\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            output_attentions=True)\n        \n        last_hidden_state = outputs.last_hidden_state  # (batch, seq_len, hidden)\n        cls_embedding = last_hidden_state[:, 0, :]     # Take [CLS] token representation\n        logits = self.classifier(cls_embedding)        # Binary classification logits\n\n        return logits, outputs.attentions  # Also return attention weights\n\nmodel_name = \"microsoft/xtremedistil-l6-h256-uncased\"\nfinetune_model = BertWithLastLayerAttentionClassifier(model_name)\n</pre> # Step 3: Custom model that freezes all but last attention layer  from transformers import AutoModel import torch.nn as nn  class BertWithLastLayerAttentionClassifier(nn.Module):     def __init__(self, model_name, num_classes=2):         super().__init__()         self.bert = AutoModel.from_pretrained(model_name, output_attentions=True)                  # Freeze all layers except the last encoder layer         for name, param in self.bert.named_parameters():             # Only unfreeze Layer 5 and classifier             if \"encoder.layer.5\" not in name and \"pooler\" not in name:                 param.requires_grad = False                  # Classification head on [CLS]         self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)      def forward(self, input_ids, attention_mask):         # Get hidden states and attention maps         outputs = self.bert(input_ids=input_ids,                             attention_mask=attention_mask,                             output_attentions=True)                  last_hidden_state = outputs.last_hidden_state  # (batch, seq_len, hidden)         cls_embedding = last_hidden_state[:, 0, :]     # Take [CLS] token representation         logits = self.classifier(cls_embedding)        # Binary classification logits          return logits, outputs.attentions  # Also return attention weights  model_name = \"microsoft/xtremedistil-l6-h256-uncased\" finetune_model = BertWithLastLayerAttentionClassifier(model_name)  In\u00a0[8]: Copied! <pre># Step 4: Training setup\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom tqdm.notebook import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfinetune_model.to(device)\n\n# Loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = AdamW(filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=2e-5)\n\n# Accuracy function\ndef compute_accuracy(preds, labels):\n    return (preds.argmax(dim=1) == labels).float().mean().item()\n\n# Training loop\ndef train_model(model, train_loader, test_loader, epochs=10):\n    for epoch in range(epochs):\n        model.train()\n        total_loss, total_acc = 0, 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            \n            # Check if label key exists, otherwise check for labels\n            if \"label\" in batch:\n                labels = batch[\"label\"].to(device)\n            elif \"labels\" in batch:\n                labels = batch[\"labels\"].to(device)\n            else:\n                raise KeyError(\"No label found in batch. Make sure your dataset includes 'label' or 'labels' field.\")\n\n            optimizer.zero_grad()\n            logits, _ = model(input_ids, attention_mask)\n            loss = loss_fn(logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            total_acc += compute_accuracy(logits, labels)\n\n        avg_loss = total_loss / len(train_loader)\n        avg_acc = total_acc / len(train_loader)\n        print(f\"Train Loss: {avg_loss:.4f}, Train Accuracy: {avg_acc:.4f}\")\n\n        # Evaluation\n        model.eval()\n        with torch.no_grad():\n            val_loss, val_acc = 0, 0\n            for batch in test_loader:\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                \n                # Check if label key exists, otherwise check for labels\n                if \"label\" in batch:\n                    labels = batch[\"label\"].to(device)\n                elif \"labels\" in batch:\n                    labels = batch[\"labels\"].to(device)\n                else:\n                    raise KeyError(\"No label found in batch. Make sure your dataset includes 'label' or 'labels' field.\")\n\n                logits, _ = model(input_ids, attention_mask)\n                loss = loss_fn(logits, labels)\n\n                val_loss += loss.item()\n                val_acc += compute_accuracy(logits, labels)\n\n        avg_val_loss = val_loss / len(test_loader)\n        avg_val_acc = val_acc / len(test_loader)\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_acc:.4f}\")\n\n# Run training\ntrain_model(finetune_model, train_loader, test_loader, epochs=10)\n</pre> # Step 4: Training setup  import torch import torch.nn as nn from torch.optim import AdamW from tqdm.notebook import tqdm  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") finetune_model.to(device)  # Loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = AdamW(filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=2e-5)  # Accuracy function def compute_accuracy(preds, labels):     return (preds.argmax(dim=1) == labels).float().mean().item()  # Training loop def train_model(model, train_loader, test_loader, epochs=10):     for epoch in range(epochs):         model.train()         total_loss, total_acc = 0, 0                  for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):             input_ids = batch[\"input_ids\"].to(device)             attention_mask = batch[\"attention_mask\"].to(device)                          # Check if label key exists, otherwise check for labels             if \"label\" in batch:                 labels = batch[\"label\"].to(device)             elif \"labels\" in batch:                 labels = batch[\"labels\"].to(device)             else:                 raise KeyError(\"No label found in batch. Make sure your dataset includes 'label' or 'labels' field.\")              optimizer.zero_grad()             logits, _ = model(input_ids, attention_mask)             loss = loss_fn(logits, labels)             loss.backward()             optimizer.step()              total_loss += loss.item()             total_acc += compute_accuracy(logits, labels)          avg_loss = total_loss / len(train_loader)         avg_acc = total_acc / len(train_loader)         print(f\"Train Loss: {avg_loss:.4f}, Train Accuracy: {avg_acc:.4f}\")          # Evaluation         model.eval()         with torch.no_grad():             val_loss, val_acc = 0, 0             for batch in test_loader:                 input_ids = batch[\"input_ids\"].to(device)                 attention_mask = batch[\"attention_mask\"].to(device)                                  # Check if label key exists, otherwise check for labels                 if \"label\" in batch:                     labels = batch[\"label\"].to(device)                 elif \"labels\" in batch:                     labels = batch[\"labels\"].to(device)                 else:                     raise KeyError(\"No label found in batch. Make sure your dataset includes 'label' or 'labels' field.\")                  logits, _ = model(input_ids, attention_mask)                 loss = loss_fn(logits, labels)                  val_loss += loss.item()                 val_acc += compute_accuracy(logits, labels)          avg_val_loss = val_loss / len(test_loader)         avg_val_acc = val_acc / len(test_loader)         print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {avg_val_acc:.4f}\")  # Run training train_model(finetune_model, train_loader, test_loader, epochs=10)  <pre>Epoch 1:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.6835, Train Accuracy: 0.5485\nVal Loss: 0.6667, Val Accuracy: 0.6055\n</pre> <pre>Epoch 2:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.6689, Train Accuracy: 0.6115\nVal Loss: 0.6507, Val Accuracy: 0.6152\n</pre> <pre>Epoch 3:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.6534, Train Accuracy: 0.6160\nVal Loss: 0.6316, Val Accuracy: 0.6387\n</pre> <pre>Epoch 4:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.6319, Train Accuracy: 0.6515\nVal Loss: 0.6348, Val Accuracy: 0.6328\n</pre> <pre>Epoch 5:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.6135, Train Accuracy: 0.6730\nVal Loss: 0.5822, Val Accuracy: 0.6973\n</pre> <pre>Epoch 6:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.6016, Train Accuracy: 0.6720\nVal Loss: 0.5755, Val Accuracy: 0.6875\n</pre> <pre>Epoch 7:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.5876, Train Accuracy: 0.6900\nVal Loss: 0.5680, Val Accuracy: 0.6934\n</pre> <pre>Epoch 8:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.5739, Train Accuracy: 0.7020\nVal Loss: 0.5621, Val Accuracy: 0.6973\n</pre> <pre>Epoch 9:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.5607, Train Accuracy: 0.7125\nVal Loss: 0.5665, Val Accuracy: 0.6973\n</pre> <pre>Epoch 10:   0%|          | 0/125 [00:00&lt;?, ?it/s]</pre> <pre>Train Loss: 0.5626, Train Accuracy: 0.7045\nVal Loss: 0.5688, Val Accuracy: 0.6973\n</pre> In\u00a0[11]: Copied! <pre>def get_attention_from_finetuned(model, sentence):\n    inputs = tokenizer(sentence, return_tensors=\"pt\")\n    \n    # Remove token_type_ids if present since the model doesn't expect it\n    if 'token_type_ids' in inputs:\n        inputs.pop('token_type_ids')\n    \n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        logits, attentions = model(**inputs)\n    \n    token_list = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    return attentions[-1][0].cpu(), token_list\n\nattention_fine_tune, tokens = get_attention_from_finetuned(finetune_model, sentence)\n</pre> def get_attention_from_finetuned(model, sentence):     inputs = tokenizer(sentence, return_tensors=\"pt\")          # Remove token_type_ids if present since the model doesn't expect it     if 'token_type_ids' in inputs:         inputs.pop('token_type_ids')          inputs = {k: v.to(device) for k, v in inputs.items()}          with torch.no_grad():         logits, attentions = model(**inputs)          token_list = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])     return attentions[-1][0].cpu(), token_list  attention_fine_tune, tokens = get_attention_from_finetuned(finetune_model, sentence) <p>Let\u2019s now visualize all 8 attention heads in Layer 5 after fine-tuning. This will help us see if:</p> <ul> <li><code>[CLS]</code> has gained stronger focus (for classification)</li> <li>Heads now better lock on to sentiment cues like <code>good</code>, <code>enjoyed</code>, or <code>surprisingly</code></li> </ul> In\u00a0[19]: Copied! <pre># Reuse our visualization tool from earlier\n\ndef plot_heads_after_finetuning(tokens, attn_layer, after=True):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import numpy as np\n\n    num_heads = attn_layer.shape[0]\n    fig, axes = plt.subplots(nrows=int(np.ceil(num_heads/4)), ncols=min(4, num_heads),\n                             figsize=(16, 3*int(np.ceil(num_heads/4))))\n    axes = axes.flatten()\n\n    for i in range(num_heads):\n        sns.heatmap(attn_layer[i].numpy(), xticklabels=tokens, yticklabels=tokens,\n                    cmap=\"viridis\", ax=axes[i], cbar=False)\n        axes[i].set_title(f\"Head {i}\")\n        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=90)\n        axes[i].set_yticklabels(axes[i].get_yticklabels(), rotation=0)\n\n    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n    fig.colorbar(axes[0].collections[0], cax=cbar_ax)\n\n    if after:\n        plt.suptitle(\"Layer 5 Attention Heads (After Fine-Tuning)\", fontsize=16)\n    else:\n        plt.suptitle(\"Layer 5 Attention Heads (Before Fine-Tuning)\", fontsize=16)\n    plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n    plt.show()\n\n# Run visualization\nplot_heads_after_finetuning(tokens, attentions[-1][0], after=False)\nplot_heads_after_finetuning(tokens, attention_fine_tune, after=True)\n</pre> # Reuse our visualization tool from earlier  def plot_heads_after_finetuning(tokens, attn_layer, after=True):     import matplotlib.pyplot as plt     import seaborn as sns     import numpy as np      num_heads = attn_layer.shape[0]     fig, axes = plt.subplots(nrows=int(np.ceil(num_heads/4)), ncols=min(4, num_heads),                              figsize=(16, 3*int(np.ceil(num_heads/4))))     axes = axes.flatten()      for i in range(num_heads):         sns.heatmap(attn_layer[i].numpy(), xticklabels=tokens, yticklabels=tokens,                     cmap=\"viridis\", ax=axes[i], cbar=False)         axes[i].set_title(f\"Head {i}\")         axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=90)         axes[i].set_yticklabels(axes[i].get_yticklabels(), rotation=0)      cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])     fig.colorbar(axes[0].collections[0], cax=cbar_ax)      if after:         plt.suptitle(\"Layer 5 Attention Heads (After Fine-Tuning)\", fontsize=16)     else:         plt.suptitle(\"Layer 5 Attention Heads (Before Fine-Tuning)\", fontsize=16)     plt.tight_layout(rect=[0, 0, 0.9, 0.95])     plt.show()  # Run visualization plot_heads_after_finetuning(tokens, attentions[-1][0], after=False) plot_heads_after_finetuning(tokens, attention_fine_tune, after=True) <pre>/var/folders/2z/g737jg9d2jj206wkf56g7gyc0000gn/T/ipykernel_94246/1305395018.py:27: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n</pre> <pre>/var/folders/2z/g737jg9d2jj206wkf56g7gyc0000gn/T/ipykernel_94246/1305395018.py:27: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n</pre>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#exploring-bert-attention-from-interpretation-to-fine-tuning","title":"Exploring BERT Attention: From Interpretation to Fine-Tuning\u00b6","text":"<p>In this notebook, we will explore how BERT uses attention mechanisms to understand text, without diving straight into training. We\u2019ll:</p> <ol> <li>Visualize BERT's attention heads: What do they attend to? Are there patterns at different layers?</li> <li>Fine-tune the last attention layer and a classifier on the IMDB sentiment dataset.</li> <li>Compare attention before and after fine-tuning to observe how attention adapts during learning.</li> </ol>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#1-understanding-pretrained-attention","title":"1 \ud83d\udd0d Understanding Pretrained Attention\u00b6","text":"<p>BERT uses multi-layer, multi-head self-attention to understand relationships between tokens. Each layer in BERT contains multiple attention heads \u2014 and each head might focus on different aspects:</p> <ul> <li>Syntactic roles (e.g., subject \u2192 verb)</li> <li>Semantic similarity (e.g., synonyms)</li> <li>Special tokens ([CLS], [SEP], etc.)</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#objective","title":"\ud83d\udccc Objective\u00b6","text":"<p>In this section, we will:</p> <ul> <li>Load a lightweight, distilled BERT model (xtremedistil) for faster visualization.</li> <li>Pass a sentence through the model.</li> <li>Visualize and compare the attention maps at different layers and heads. This will help us intuitively understand how BERT processes and attends to the input text.</li> </ul> <p>Let\u2019s begin by loading the model and tokenizer!</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#step-2-tokenize-input-and-extract-attention-weights","title":"\ud83d\udd27 Step 2: Tokenize Input and Extract Attention Weights\u00b6","text":"<p>Now that we've loaded our distilled BERT model and tokenizer, let's walk through the process of running a sentence through the model and capturing its attention weights.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#line-by-line-explanation","title":"\ud83d\udd0d Line-by-line Explanation\u00b6","text":"<ol> <li>We define a test sentence. Our goal is to understand how BERT internally pays attention across these words.</li> <li>We tokenize the sentence into input IDs and attention masks, and return them as PyTorch tensors (required for BERT).</li> <li>We pass the inputs to the model inside a <code>torch.no_grad()</code> block \u2014 since we're just doing inference and don't need gradients.</li> </ol> <p>We collect the attention weights for each layer (BERT returns them as a tuple). Each element contains one layer's attention of shape:</p> <ul> <li><code>batch_size = 1</code> (since it's one sentence)</li> <li><code>num_heads = 8</code></li> <li><code>seq_len = number of tokens</code> (e.g., 13 if including special tokens)</li> </ul> <p>So we end up with: <code>torch.Size([6, 1, 8, seq_len, seq_len])</code></p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#step-3-inspect-attention-tensor-dimensions-and-tokens","title":"\ud83e\udde0 Step 3: Inspect Attention Tensor Dimensions and Tokens\u00b6","text":"<p>Now that we\u2019ve extracted the attention weights, let\u2019s understand what we\u2019re working with.</p> <p>We want to:</p> <ol> <li>Verify the shape of the attention tensor.</li> <li>List the input tokens (as BERT sees them).</li> <li>Understand what each axis of the attention tensor means.</li> </ol>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#attention-shape-breakdown","title":"Attention Shape Breakdown\u00b6","text":"<ul> <li><code>attentions.shape \u2192 (num_layers, batch, num_heads, seq_len, seq_len)</code></li> <li>Each matrix inside has shape <code>(seq_len, seq_len)</code>:<ul> <li>Rows are query positions (i.e., \"who is attending\")</li> <li>Columns are key positions (i.e., \"who is being attended to\")</li> </ul> </li> </ul> <p>We'll also print the actual BERT tokens so we can interpret the matrices meaningfully.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#step-4-visualize-attention-across-all-heads-in-a-layer","title":"\ud83c\udfa8 Step 4: Visualize Attention Across All Heads in a Layer\u00b6","text":"<p>Now that we understand the attention tensor structure and the tokens, let\u2019s visualize how each head behaves in a given layer.</p> <p>Instead of just picking one head, we\u2019ll look at all attention heads in a chosen layer using a grid of heatmaps.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#what-this-visualization-shows","title":"\ud83d\udd0d What This Visualization Shows\u00b6","text":"<ul> <li>Each subplot represents the attention matrix of one head in the selected layer.</li> <li>The matrix shows how each token attends to every other token.</li> <li>Rows = queries (the attending token), Columns = keys (the token being attended to).</li> <li>Color intensity indicates attention strength.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#interactive-tool","title":"\ud83c\udf9b Interactive Tool\u00b6","text":"<p>You can use a slider to choose the layer you want to inspect. This allows you to see how attention patterns evolve across BERT layers \u2014 some attend broadly, others sharply focus on key tokens like <code>[CLS]</code>, verbs, or punctuation.</p> <p>Let\u2019s run the interactive tool!</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#key-takeaways-layer-0-attention-patterns","title":"\ud83e\udde0 Key Takeaways \u2014 Layer 0 Attention Patterns\u00b6","text":"<p>In Layer 0 of the xtremedistil BERT model, attention heads show foundational patterns that prepare for deeper semantic understanding. Here's a summary of what each head is doing:</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#diagonal-identity-heads","title":"\ud83d\udd39 Diagonal / Identity Heads\u00b6","text":"<p>Preserve each token's representation by mostly attending to itself.</p> <ul> <li>Head 0 \u2013 strong self-attention</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#directional-sequential-heads","title":"\ud83d\udd38 Directional / Sequential Heads\u00b6","text":"<p>Focus on tokens nearby in sequence \u2014 useful for grammar and structure.</p> <ul> <li>Head 3 \u2013 strong previous-token pattern --&gt; attends to previous token</li> <li>Head 5 \u2013 strong next-token pattern --&gt; attends to next token</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#sparse-focused-heads","title":"\ud83d\udd3b Sparse / Focused Heads\u00b6","text":"<p>Attend to a few specific tokens (e.g., sentiment words or key phrases).</p> <ul> <li>Head 1 \u2013 All words attending [CLS] and [SEP]</li> <li>Head 2 \u2013 sparse attention (few strong connections and especially on movie)</li> <li>Head 7 \u2013 focuses around the diagonal linking nouns, verbs and adjectives from the same part of the sentence</li> <li>Head 4 \u2013 [SEP] and [surprisingly] attended broadly</li> <li>Head 6 \u2013 [CLS] attended broadly</li> </ul> <p>These patterns show how even the first layer of BERT begins organizing language:</p> <ul> <li>Some heads memorize position or identity,</li> <li>Others build early syntactic or phrasal relationships,</li> <li>And some prepare global summaries for later use.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#key-takeaways-layer-5-attention-patterns-final-layer","title":"\ud83e\udde0 Key Takeaways \u2014 Layer 5 Attention Patterns (Final Layer)\u00b6","text":"<p>In the last layer of xtremedistil BERT, attention heads become much more specialized and focused. They're no longer just preserving structure \u2014 they're identifying key semantic cues to prepare for downstream tasks like classification or question answering.</p> <p>Here\u2019s how the heads in Layer 5 cluster based on behavior:</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#sentence-anchor-special-token-focus","title":"\ud83d\udd39 Sentence Anchor / Special Token Focus\u00b6","text":"<p>Heads that allocate strong attention to sentence-level structural tokens like <code>[CLS]</code>, <code>[SEP]</code>, or function words.</p> <ul> <li>Head 0 \u2013 Attends to <code>[CLS]</code>, <code>[SEP]</code>, and mid-sentence context tokens like <code>movie</code>, <code>was</code>, <code>surprisingly</code>, <code>good</code>.</li> <li>Head 2 \u2013 Balanced across <code>[CLS]</code>, <code>[SEP]</code>, <code>was</code>, <code>it</code>, <code>the</code>.</li> <li>Head 6 \u2013 Strong triple focus on <code>[CLS]</code> and <code>the</code>, possibly re-centering <code>[CLS]</code>.</li> <li>Head 7 \u2013 Mostly attends to <code>was</code>, but <code>[CLS]</code> and <code>[SEP]</code> get notable attention too.</li> </ul> <p>\ud83d\udccc Interpretation: These heads likely support global summarization, or feed context to the <code>[CLS]</code> token for sentence-level prediction.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#sentiment-phrase-heads","title":"\ud83d\udd38 Sentiment Phrase Heads\u00b6","text":"<p>These heads focus on important semantic cues or emotional words in the sentence.</p> <ul> <li>Head 1 \u2013 Attends strongly to <code>surprisingly</code>, <code>good</code>, <code>really</code>, and <code>enjoyed</code>.</li> <li>Head 3 \u2013 Focuses tightly on <code>surprisingly</code> and <code>was</code>.</li> <li>Head 5 \u2013 Almost exclusively focuses on <code>was</code>.</li> </ul> <p>\ud83d\udccc Interpretation: These heads appear to lock on to sentiment-bearing phrases, which is crucial for tasks like sentiment analysis or emotion classification.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#token-level-detail-spreaders","title":"\ud83d\udd3b Token-Level Detail Spreaders\u00b6","text":"<p>Heads that distribute attention across punctuation, function words, and syntax markers \u2014 possibly for fine control.</p> <ul> <li>Head 4 \u2013 Favors tokens like <code>the</code>, <code>movie</code>, <code>i</code>, <code>it</code>, punctuation (<code>,</code>, <code>.</code>). \ud83d\udccc Interpretation: This head may be balancing token-level refinement before final decoding, especially for syntactic normalization.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#final-note","title":"\ud83c\udf93 Final Note\u00b6","text":"<p>While Layer 0 built structure and flow, Layer 5 refines meaning and consolidates relevance.</p> <ul> <li>[CLS] is now dominant, indicating readiness for classification tasks.</li> <li>Key content words like <code>good</code>, <code>enjoyed</code>, and <code>surprisingly</code> are getting priority.</li> <li>There's less uniformity \u2014 each head has its own role in encoding final semantics.</li> </ul> <p>This is exactly what we expect from the last layer of BERT \u2014 every head is now highly purpose-driven.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#2-fine-tuning-bert-on-imdb-impact-on-attention","title":"2 \ud83d\udee0 Fine-Tuning BERT on IMDB: Impact on Attention\u00b6","text":"<p>Now that we've seen how a pretrained model distributes attention, the big question is:</p> <p>How does attention change when we fine-tune the model for a specific task?</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#objective","title":"\ud83c\udfaf Objective\u00b6","text":"<p>In this section, we will:</p> <ol> <li>Fine-tune only the last attention layer (plus a small classifier head) on the IMDB sentiment classification dataset.</li> <li>Freeze all other weights \u2014 this way, any change in behavior must come from the last attention block and classifier.</li> <li>After training, we\u2019ll re-run attention visualizations for a test sentence and compare them to the original pretrained version.</li> </ol> <p>This will help us answer:</p> <ul> <li>Which heads adapt to focus more on sentiment-bearing words?</li> <li>Does <code>[CLS]</code> become more dominant or refined?</li> <li>Are certain heads re-purposed for the task?</li> </ul> <p>Let\u2019s start by loading the IMDB dataset.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#step-2-tokenization-and-batching","title":"\ud83e\uddfc Step 2: Tokenization and Batching\u00b6","text":"<p>Now that we\u2019ve loaded the IMDB dataset, let\u2019s preprocess the data for BERT input.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#key-preprocessing-choices","title":"\ud83d\udccb Key Preprocessing Choices:\u00b6","text":"<ul> <li>Tokenizer: We use the same <code>xtremedistil</code> tokenizer to keep consistency with the model.</li> <li>Truncation: Reviews are truncated to 64 tokens for speed and memory efficiency.</li> <li>Padding: Handled dynamically per batch using Hugging Face's <code>DataCollatorWithPadding</code>.</li> <li>Batched format: All data is returned as PyTorch tensors for compatibility with the model and training loop.</li> </ul> <p>We then create PyTorch DataLoaders that will serve batches to the model during training and evaluation.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#step-3-freezing-bert-adding-a-classifier-on-top","title":"\ud83e\udde0 Step 3: Freezing BERT &amp; Adding a Classifier on Top\u00b6","text":"<p>We now prepare our model for fine-tuning.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#objective","title":"\ud83c\udfaf Objective:\u00b6","text":"<p>We want to only fine-tune the last attention layer and a classifier head. Why?</p> <ul> <li>Lower BERT layers already capture general syntax and semantics.</li> <li>The last layer can adapt to the sentiment task (IMDB) by adjusting how tokens attend to each other.</li> <li>This setup lets us observe how attention patterns change due to fine-tuning.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#what-well-do","title":"\ud83c\udfd7 What We\u2019ll Do:\u00b6","text":"<ol> <li>Load the same <code>xtremedistil</code> BERT model with attention outputs.</li> <li>Freeze all layers except Layer 5 (the last one).</li> <li>Add a [CLS]-based classifier (simple linear layer).</li> <li>Build a custom model class that returns both predictions and attention weights.</li> </ol>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#step-4-training-the-model-only-last-layer-classifier","title":"\ud83c\udfcb\ufe0f Step 4: Training the Model (Only Last Layer + Classifier)\u00b6","text":"<p>Now that our model is set up to fine-tune only the last attention layer and the classifier, we\u2019ll define the training loop.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#what-well-do","title":"\ud83e\uddea What We\u2019ll Do:\u00b6","text":"<ol> <li>Use cross-entropy loss for binary classification (IMDB: positive vs. negative).</li> <li>Track accuracy during training and validation.</li> <li>Use <code>AdamW</code> optimizer \u2014 designed for transformer architectures.</li> <li>Train for a few epochs to observe how the model adapts.</li> </ol> <p>Since we are fine-tuning only a small number of parameters, training is faster and lets us isolate the effect of attention change during learning.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#step-5-visualizing-post-fine-tuning-attention-layer-5","title":"\ud83d\udd0d Step 5: Visualizing Post-Fine-Tuning Attention (Layer 5)\u00b6","text":"<p>Now comes the most insightful part of this notebook:</p> <p>How has attention changed after fine-tuning?</p> <p>We will:</p> <ol> <li>Pass a test sentence through the fine-tuned model.</li> <li>Extract attention weights from Layer 5 (the only layer we allowed to change).</li> <li>Compare this with the attention patterns we saw before fine-tuning.</li> <li>Interpret how the model\u2019s focus has shifted \u2014 especially around sentiment-bearing words.</li> </ol> <p>This will help us understand how BERT adapts attention when trained for a specific task (like sentiment classification).</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#key-takeaways-layer-5-attention-changes-after-fine-tuning","title":"\ud83e\udde0 Key Takeaways \u2014 Layer 5 Attention Changes After Fine-Tuning\u00b6","text":"<p>Here\u2019s a head-by-head comparison showing how attention patterns evolved after fine-tuning BERT on the IMDB sentiment classification task.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-0","title":"\ud83d\udd38 Head 0\u00b6","text":"<ul> <li>Before: Attended broadly to <code>[CLS]</code>, <code>[SEP]</code>, and mid-sentence tokens like <code>movie</code>, <code>was</code>, <code>surprisingly</code>, <code>good</code>.</li> <li>After: Much sharper, heavily focuses on <code>was</code>, <code>movie</code>, <code>surprisingly</code>, and <code>good</code> \u2014 clear sentiment alignment.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-1","title":"\ud83d\udd38 Head 1\u00b6","text":"<ul> <li>Before: Focused on <code>surprisingly</code>, <code>good</code>, <code>really</code>, and <code>enjoyed</code>.</li> <li>After: Refined and intensified \u2014 now strongly attends to <code>surprisingly</code>, <code>good</code>, and <code>enjoyed</code>, with minor focus on <code>movie</code> and <code>really</code>.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-2","title":"\ud83d\udd38 Head 2\u00b6","text":"<ul> <li>Before: Fairly balanced \u2014 attention spread across <code>[CLS]</code>, <code>[SEP]</code>, <code>was</code>, <code>it</code>, and <code>the</code>.</li> <li>After: Dramatically simplified \u2014 now focuses almost entirely on <code>was</code>, indicating task-specific tuning.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-3","title":"\ud83d\udd38 Head 3\u00b6","text":"<ul> <li>Before: Narrow focus on <code>surprisingly</code> and <code>was</code>.</li> <li>After: almost Unchanged \u2014 maintains sharp focus on <code>surprisingly</code> and <code>was</code>.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-4","title":"\ud83d\udd38 Head 4\u00b6","text":"<ul> <li>Before: Attended to structure words like <code>the</code>, <code>movie</code>, <code>i</code>, <code>it</code>, and punctuation.</li> <li>After: More diffuse, with slightly increased focus toward the end of the sentence.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-5","title":"\ud83d\udd38 Head 5\u00b6","text":"<ul> <li>Before: Focused nearly exclusively on <code>was</code>.</li> <li>After: Still strong on <code>was</code>, but now adds meaningful attention to <code>good</code> \u2014 sentiment refinement.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-6","title":"\ud83d\udd38 Head 6\u00b6","text":"<ul> <li>Before: Emphasis on <code>[CLS]</code> and <code>the</code>, possibly for sentence-wide summary.</li> <li>After: Shifts focus to the beginning (<code>[CLS]</code>, <code>the</code>, <code>movie</code>) and last token \u2014 suggesting stronger sentence boundary modeling.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#head-7","title":"\ud83d\udd38 Head 7\u00b6","text":"<ul> <li>Before: Primary attention on <code>was</code>, with some spread to <code>[CLS]</code> and <code>[SEP]</code>.</li> <li>After: Tightens up \u2014 now mostly targets <code>was</code> and <code>good</code>.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#summary","title":"\ud83d\udccc Summary\u00b6","text":"<ul> <li>Attention after fine-tuning is more task-specific, concentrated, and semantic-aware.</li> <li>Sentiment-rich words (<code>good</code>, <code>enjoyed</code>, <code>surprisingly</code>) receive much stronger focus.</li> <li><code>[CLS]</code> becomes more strategically attended to in certain heads (for classification).</li> <li>Some heads specialize, while others broaden slightly to reinforce sentence structure.</li> </ul> <p>These changes reflect how attention is actively re-learned to support a new task \u2014 in this case, sentiment classification.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#conclusion-what-we-learned-about-attention-in-bert","title":"\u2705 Conclusion \u2014 What We Learned About Attention in BERT\u00b6","text":"<p>Over the course of this notebook, we\u2019ve taken a deep dive into BERT\u2019s attention mechanism, starting from raw pretrained behavior all the way to its adaptation through fine-tuning.</p>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#what-we-did","title":"\ud83d\udd0d What We Did\u00b6","text":"<ol> <li><p>Visualized raw attention across all heads in Layer 0 and Layer 5 of <code>xtremedistil</code> BERT.</p> <ul> <li>Layer 0: Showed basic structural patterns like self-attention, previous/next token focus.</li> <li>Layer 5: Began to specialize \u2014 focusing on <code>[CLS]</code>, sentiment phrases like <code>good</code>, <code>enjoyed</code>.</li> </ul> </li> <li><p>Fine-tuned only the last attention layer + a classifier on IMDB sentiment data.</p> <ul> <li>All other weights were frozen.</li> <li>Training was fast and interpretable.</li> </ul> </li> <li><p>Visualized Layer 5 again after fine-tuning to inspect what changed.</p> </li> </ol>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#what-we-learned","title":"\ud83e\udde0 What We Learned\u00b6","text":"<ul> <li>\u2705 Attention is task-aware: After fine-tuning, heads shifted focus toward words like <code>good</code>, <code>enjoyed</code>, <code>surprisingly</code> \u2014 all sentiment-laden tokens.</li> <li>\u2705 [CLS] gets re-prioritized: Many heads increased attention to <code>[CLS]</code> after training \u2014 a crucial move since it's used for sentence-level prediction.</li> <li>\u2705 Head specialization improves: Some heads become tightly focused, others maintain structure \u2014 supporting both meaning and form.</li> <li>\u2705 Interpretability is possible: By visualizing and tracking attention, we can understand what the model \u201ccares about\u201d during learning.</li> </ul>"},{"location":"chapter5/Session_5_2_Attention_Visualization/#takeaway","title":"\ud83c\udf93 Takeaway\u00b6","text":"<p>BERT isn't just a black box. Its attention patterns reflect real linguistic and semantic structure \u2014 and those patterns evolve as it learns specific tasks.</p> <p>With the right tools and a bit of curiosity, we can peek inside the model and watch it think.</p>"},{"location":"chapter6/","title":"Few-Shot Learning &amp; Transfer Learning","text":""},{"location":"chapter6/#session-6-leveraging-pre-trained-knowledge","title":"Session 6: Leveraging Pre-trained Knowledge","text":"<p>This session explores how modern NLP models can be adapted to new tasks with limited labeled data through few-shot learning and transfer learning techniques.</p>"},{"location":"chapter6/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the concept of transfer learning in the context of NLP</li> <li>Learn about few-shot learning approaches for language models</li> <li>Explore techniques for fine-tuning pre-trained models on downstream tasks</li> <li>Understand prompt-based learning for generating labels</li> <li>Gain practical experience with implementing few-shot learning techniques</li> </ul>"},{"location":"chapter6/#topics-covered","title":"Topics Covered","text":""},{"location":"chapter6/#language-models-as-few-shot-learners","title":"Language Models as Few-Shot Learners","text":"<ul> <li>The concept of few-shot, one-shot, and zero-shot learning</li> <li>Fine-tuning BERT architecture for transfer learning on downstream tasks</li> <li>Strategies for adapting pre-trained models to specific domains</li> <li>Parameter-efficient fine-tuning methods (adapters, prompt tuning)</li> <li>Handling limited labeled data scenarios</li> <li>Cross-lingual transfer learning</li> </ul>"},{"location":"chapter6/#leveraging-existing-knowledge","title":"Leveraging Existing Knowledge","text":"<ul> <li>Using prompts to generate labels and guide model behavior</li> <li>In-context learning and demonstration examples</li> <li>Prompt engineering techniques and best practices</li> <li>Pattern-exploiting training methods</li> <li>Self-training and semi-supervised approaches</li> <li>Knowledge distillation for model compression</li> </ul>"},{"location":"chapter6/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>Brown et al. (2020) \"Language Models are Few-Shot Learners\"</li> <li>Gao et al. (2020) \"Making Pre-trained Language Models Better Few-shot Learners\"</li> <li>Gao, Tianyu (2021) \"Prompting: Better Ways of Using Language Models for NLP Tasks\"</li> <li>Howard &amp; Ruder (2018) \"Universal Language Model Fine-tuning for Text Classification\"</li> <li>Sun et al. (2019) \"How to Fine-Tune BERT for Text Classification?\"</li> <li>Timo Schick and Hinrich Sch\u00fctze (2021) \"Generating Datasets with Pretrained Language Models\"</li> <li>Timo Schick and Hinrich Sch\u00fctze (2021) \"Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference\"</li> </ul>"},{"location":"chapter6/#practical-components","title":"Practical Components","text":"<ul> <li>Implementing few-shot learning techniques with pre-trained models</li> <li>Designing effective prompts for various NLP tasks</li> <li>Fine-tuning BERT-based models on small datasets</li> <li>Evaluating transfer learning performance across different domains</li> <li>Comparing few-shot learning approaches with fully supervised methods</li> <li>Hands-on exercises with the Hugging Face Transformers library</li> </ul>"},{"location":"chapter7/","title":"Injustice &amp; Biases in NLP","text":""},{"location":"chapter7/#session-7-ethical-considerations-in-language-models","title":"Session 7: Ethical Considerations in Language Models","text":"<p>This session addresses the critical issues of bias, fairness, and ethical considerations in NLP systems, with a focus on identifying and mitigating harmful biases in large language models.</p>"},{"location":"chapter7/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the sources and types of biases in NLP systems</li> <li>Learn to identify biases in language models and their outputs</li> <li>Explore techniques for measuring and quantifying bias</li> <li>Develop strategies for mitigating biases in NLP applications</li> <li>Consider the broader ethical implications of deploying language technologies</li> </ul>"},{"location":"chapter7/#topics-covered","title":"Topics Covered","text":""},{"location":"chapter7/#are-large-language-models-stochastic-parrots","title":"Are Large Language Models Stochastic Parrots?","text":"<ul> <li>Critical examination of large language models and their limitations</li> <li>Understanding how models reproduce biases present in training data</li> <li>The \"stochastic parrot\" critique and its implications</li> <li>Evaluating the utility of language models across different demographics and contexts</li> <li>Accessibility and inclusivity concerns in NLP technologies</li> <li>Environmental and computational costs of large language models</li> </ul>"},{"location":"chapter7/#detecting-and-mitigating-biases","title":"Detecting and Mitigating Biases","text":"<ul> <li>Types of biases: gender, racial, cultural, socioeconomic, and others</li> <li>Methods for detecting bias in word embeddings and language models</li> <li>Quantitative metrics for measuring bias in NLP systems</li> <li>Bias mitigation strategies: pre-processing, in-processing, and post-processing</li> <li>Techniques for self-diagnosis and self-debiasing in language models</li> <li>Balancing bias mitigation with model performance</li> <li>Responsible AI development practices</li> </ul>"},{"location":"chapter7/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>Bender et al. (2021) \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c\"</li> <li>Sheng et al. (2019) \"The Woman Worked as a Babysitter: On Biases in Language Generation\"</li> <li>Kirk et al. (2021) \"How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases\"</li> <li>Timo Schick et al. (2021) \"Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP\"</li> <li>Strubell et al. (2019) \"Energy and Policy Considerations for Deep Learning in NLP\"</li> <li>Dodge et al. (2022) \"Measuring the Carbon Intensity of AI in Cloud Instances\"</li> <li>Luccioni et al. (2023) \"Power Hungry Processing: Watts Driving the Cost of AI Deployment?\"</li> </ul>"},{"location":"chapter7/#practical-components","title":"Practical Components","text":"<ul> <li>Analyzing bias in pre-trained word embeddings and language models</li> <li>Implementing bias detection metrics for NLP systems</li> <li>Applying bias mitigation techniques to existing models</li> <li>Case studies of biased outputs in real-world NLP applications</li> <li>Ethical considerations in designing and deploying NLP systems</li> <li>Group discussions on responsible AI development</li> </ul>"},{"location":"chapter8/","title":"Practical NLP - 2","text":""},{"location":"chapter8/#session-8-advanced-nlp-implementation","title":"Session 8: Advanced NLP Implementation","text":"<p>This hands-on session builds on previous knowledge to implement advanced NLP techniques, with a focus on transformer-based models, fine-tuning strategies, and bias detection.</p>"},{"location":"chapter8/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Gain practical experience fine-tuning transformer models for specific tasks</li> <li>Learn strategies for working with limited training data</li> <li>Understand techniques for adapting models to low-resource scenarios</li> <li>Develop skills in detecting and addressing biases in NLP models</li> <li>Apply best practices for model evaluation and deployment</li> </ul>"},{"location":"chapter8/#topics-covered","title":"Topics Covered","text":""},{"location":"chapter8/#fine-tuning-a-bert-model","title":"Fine-tuning a BERT Model","text":"<ul> <li>Setting up the fine-tuning pipeline</li> <li>Preparing data for transformer models</li> <li>Hyperparameter optimization for fine-tuning</li> <li>Techniques for efficient fine-tuning (gradient accumulation, mixed precision)</li> <li>Saving and loading fine-tuned models</li> <li>Deploying fine-tuned models for inference</li> </ul>"},{"location":"chapter8/#how-much-data-to-get-the-best-results","title":"How Much Data to Get the Best Results?","text":"<ul> <li>Data efficiency in transformer models</li> <li>Learning curves and diminishing returns</li> <li>Strategies for data augmentation in NLP</li> <li>Active learning approaches for efficient data annotation</li> <li>Cross-validation strategies for small datasets</li> <li>Balancing model size and data requirements</li> </ul>"},{"location":"chapter8/#low-resource-no-problem","title":"Low Resource? No Problem","text":"<ul> <li>Transfer learning for low-resource languages and domains</li> <li>Few-shot and zero-shot learning techniques</li> <li>Cross-lingual transfer methods</li> <li>Unsupervised and self-supervised approaches</li> <li>Leveraging multilingual models for low-resource scenarios</li> <li>Domain adaptation strategies</li> </ul>"},{"location":"chapter8/#detecting-biases","title":"Detecting Biases","text":"<ul> <li>Implementing bias detection metrics</li> <li>Analyzing model outputs for various demographic groups</li> <li>Counterfactual data augmentation for bias testing</li> <li>Visualizing attention patterns to identify bias sources</li> <li>Evaluating fairness across different tasks and contexts</li> <li>Documenting model limitations and potential biases</li> </ul>"},{"location":"chapter8/#practical-components","title":"Practical Components","text":"<ul> <li>Hands-on implementation of BERT fine-tuning for classification tasks</li> <li>Experiments with varying amounts of training data</li> <li>Implementing and evaluating low-resource NLP techniques</li> <li>Building a bias detection and evaluation pipeline</li> <li>Group project: developing a fair and efficient NLP system</li> </ul>"},{"location":"chapter8/#tools-and-frameworks","title":"Tools and Frameworks","text":"<ul> <li>Hugging Face Transformers library</li> <li>PyTorch and TensorFlow</li> <li>Fairness indicators and bias measurement tools</li> <li>Model analysis and visualization libraries</li> </ul>"},{"location":"chapter9/","title":"LLM Introduction","text":""},{"location":"chapter9/#session-9-large-language-models","title":"Session 9: Large Language Models","text":"<p>This session introduces the latest generation of Large Language Models (LLMs), exploring their capabilities, limitations, and practical applications through prompt engineering and fine-tuning.</p>"},{"location":"chapter9/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the architecture and capabilities of modern Large Language Models</li> <li>Learn effective prompt engineering techniques for various tasks</li> <li>Explore different approaches to fine-tuning LLMs</li> <li>Gain practical experience working with state-of-the-art language models</li> <li>Understand the limitations and challenges of current LLM technology</li> </ul>"},{"location":"chapter9/#topics-covered","title":"Topics Covered","text":""},{"location":"chapter9/#prompt-engineering-fine-tuning","title":"Prompt Engineering &amp; Fine-tuning","text":"<ul> <li>Zero-shot learning: using LLMs without task-specific examples</li> <li>Few-shot learning through in-context examples</li> <li>Chain of thought prompting for complex reasoning tasks</li> <li>Formatting outputs for structured information extraction</li> <li>Prompt design principles and best practices</li> <li>Instruction tuning and alignment techniques</li> <li>Parameter-efficient fine-tuning methods (LoRA, P-tuning, etc.)</li> <li>Full fine-tuning vs. adapter-based approaches</li> </ul>"},{"location":"chapter9/#advanced-llm-concepts","title":"Advanced LLM Concepts","text":"<ul> <li>Scaling laws and emergent abilities in LLMs</li> <li>Model architectures: decoder-only vs. encoder-decoder</li> <li>Open vs. closed models: comparing capabilities and limitations</li> <li>Multilingual capabilities and cross-lingual transfer</li> <li>Multimodal extensions to language models</li> <li>Retrieval-augmented generation</li> <li>Efficient inference techniques</li> </ul>"},{"location":"chapter9/#recommended-reading","title":"Recommended Reading","text":"<ul> <li>Brown et al. (2020) \"Language Models are Few-Shot Learners\"</li> <li>Le Scao et al. (2022) \"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model\"</li> <li>Touvron et al. (2023) \"LLaMA: Open and Efficient Foundation Language Models\"</li> <li>Suau et al. (2022) \"Self-conditioning Pre-Trained Language Models\"</li> <li>Ag\u00fcera et al. (2022) \"Do Large Language Models Understand Us?\"</li> <li>Kaswan et al. (2023) \"The (Ab)Use of Open Source Code to Train Large Language Models\"</li> <li>Chen et al. (2024) \"What is the Role of Small Models in the LLM Era: A Survey\"</li> </ul>"},{"location":"chapter9/#practical-components","title":"Practical Components","text":"<ul> <li>Hands-on prompt engineering for various NLP tasks</li> <li>Implementing chain-of-thought reasoning with LLMs</li> <li>Fine-tuning smaller language models on specific domains</li> <li>Evaluating LLM performance across different tasks</li> <li>Comparing different prompting strategies and their effectiveness</li> <li>Building applications that leverage LLM capabilities</li> </ul>"},{"location":"evaluations/evaluations/","title":"Evaluation","text":""},{"location":"evaluations/evaluations/#first-home-assignment","title":"First Home Assignment","text":""},{"location":"evaluations/evaluations/#second-home-assignment","title":"Second Home Assignment","text":""},{"location":"evaluations/evaluations/#final-project","title":"Final Project","text":""},{"location":"evaluations/final-project/","title":"Final Project","text":""},{"location":"evaluations/final-project/#final-project","title":"Final Project","text":"<p>By group of 3-4 students. You need to set-up the group as soon as possible. Once the group is validated with me I will put the group below.</p>"},{"location":"evaluations/first-home-assignment/","title":"First Home Assignment","text":""},{"location":"evaluations/first-home-assignment/#first-home-assignment","title":"First Home Assignment","text":""},{"location":"evaluations/second-home-assignment/","title":"Second Home Assignment","text":""},{"location":"evaluations/second-home-assignment/#second-home-assignment","title":"Second Home Assignment","text":""}]}
